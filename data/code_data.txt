
def split_phylogeny(p, level="s"):
    level = level+"__"
    result = p.split(level)
    return result[0]+level+result[1].split(";")[0]

def ensure_dir(d):
    if not os.path.exists(d):
        try:
            os.makedirs(d)
        except OSError as oe:
            # should not happen with os.makedirs
            # ENOENT: No such file or directory
            if os.errno == errno.ENOENT:
                return msg.format(d)
            else:
                return msg.format(d, oe.strerror)

def file_handle(fnh, mode="rU"):
    handle = None
    if isinstance(fnh, file):
        if fnh.closed:
            raise ValueError("Input file is closed.")
        handle = fnh
    elif isinstance(fnh, str):
        handle = open(fnh, mode)

    return handle

def gather_categories(imap, header, categories=None):
    # If no categories provided, return all SampleIDs
    if categories is None:

        return {"default": DataCategory(set(imap.keys()), {})}

    cat_ids = [header.index(cat)
               for cat in categories if cat in header and "=" not in cat]

    table = OrderedDict()

    conditions = defaultdict(set)
    for i, cat in enumerate(categories):
        if "=" in cat and cat.split("=")[0] in header:
            cat_name = header[header.index(cat.split("=")[0])]
            conditions[cat_name].add(cat.split("=")[1])

    # If invalid categories or conditions identified, return all SampleIDs
    if not cat_ids and not conditions:

        return {"default": DataCategory(set(imap.keys()), {})}

    #If only category column given, return column-wise SampleIDs
    if cat_ids and not conditions:
        for sid, row in imap.items():
            cat_name = "_".join([row[cid] for cid in cat_ids])
            if cat_name not in table:
                table[cat_name] = DataCategory(set(), {})
            table[cat_name].sids.add(sid)
        return table

    # Collect all condition names
    cond_ids = set()
    for k in conditions:
        try:
            cond_ids.add(header.index(k))
        except ValueError:
            continue
    idx_to_test = set(cat_ids).union(cond_ids)

    # If column name and condition given, return overlapping SampleIDs of column and
    # condition combinations
    for sid, row in imap.items():
        if all([row[header.index(c)] in conditions[c] for c in conditions]):
            key = "_".join([row[idx] for idx in idx_to_test])
            try:
                assert key in table.keys()
            except AssertionError:
                table[key] = DataCategory(set(), {})
            table[key].sids.add(sid)
    try:
        assert len(table) > 0
    except AssertionError:

        return {"default": DataCategory(set(imap.keys()), {})}
    else:
        return table

def parse_unifrac(unifracFN):
    with open(unifracFN, "rU") as uF:
        first = uF.next().split("\t")
        lines = [line.strip() for line in uF]

    unifrac = {"pcd": OrderedDict(), "eigvals": [], "varexp": []}
    if first[0] == "pc vector number":
        return parse_unifrac_v1_8(unifrac, lines)
    elif first[0] == "Eigvals":
        return parse_unifrac_v1_9(unifrac, lines)
    else:
        raise ValueError("File format not supported/recognized. Please check input "
                         "unifrac file.")

def parse_unifrac_v1_8(unifrac, file_data):
    for line in file_data:
        if line == "":
            break
        line = line.split("\t")
        unifrac["pcd"][line[0]] = [float(e) for e in line[1:]]

    unifrac["eigvals"] = [float(entry) for entry in file_data[-2].split("\t")[1:]]
    unifrac["varexp"] = [float(entry) for entry in file_data[-1].split("\t")[1:]]
    return unifrac

def parse_unifrac_v1_9(unifrac, file_data):
    unifrac["eigvals"] = [float(entry) for entry in file_data[0].split("\t")]
    unifrac["varexp"] = [float(entry)*100 for entry in file_data[3].split("\t")]

    for line in file_data[8:]:
        if line == "":
            break
        line = line.split("\t")
        unifrac["pcd"][line[0]] = [float(e) for e in line[1:]]
    return unifrac

def color_mapping(sample_map, header, group_column, color_column=None):
    group_colors = OrderedDict()
    group_gather = gather_categories(sample_map, header, [group_column])

    if color_column is not None:
        color_gather = gather_categories(sample_map, header, [color_column])
        # match sample IDs between color_gather and group_gather
        for group in group_gather:
            for color in color_gather:
                # allow incomplete assignment of colors, if group sids overlap at
                # all with the color sids, consider it a match
                if group_gather[group].sids.intersection(color_gather[color].sids):
                    group_colors[group] = color
    else:
        bcolors = itertools.cycle(Set3_12.hex_colors)
        for group in group_gather:
            group_colors[group] = bcolors.next()

    return group_colors

def rev_c(read):
    rc = []
    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}
    for base in read:
        rc.extend(rc_nucs[base.upper()])
    return rc[::-1]

def shuffle_genome(genome, cat, fraction = float(100), plot = True, \
        alpha = 0.1, beta = 100000, \
        min_length = 1000, max_length = 200000):
    header = '>randomized_%s' % (genome.name)
    sequence = list(''.join([i[1] for i in parse_fasta(genome)]))
    length = len(sequence)
    shuffled = []
    # break genome into pieces
    while sequence is not False:
        s = int(random.gammavariate(alpha, beta))
        if s <= min_length or s >= max_length:
            continue
        if len(sequence) < s:
            seq = sequence[0:]
        else:
            seq = sequence[0:s]
        sequence = sequence[s:]
#        if bool(random.getrandbits(1)) is True:
#            seq = rev_c(seq)
#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)
#        else:
#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)
        shuffled.append(''.join(seq))
        if sequence == []:
            break
    # shuffle pieces
    random.shuffle(shuffled)
    # subset fragments
    if fraction == float(100):
        subset = shuffled
    else:
        max_pieces = int(length * fraction/100)
        subset, total = [], 0
        for fragment in shuffled:
            length = len(fragment)
            if total + length <= max_pieces:
                subset.append(fragment)
                total += length
            else:
                diff = max_pieces - total
                subset.append(fragment[0:diff])
                break
    # combine sequences, if requested
    if cat is True:
        yield [header, ''.join(subset)]
    else:
        for i, seq in enumerate(subset):
            yield ['%s fragment:%s' % (header, i), seq]

def _prune(self, fit, p_max):


        def remove_from_model_desc(x, model_desc):

            rhs_termlist = []
            for t in model_desc.rhs_termlist:
                if not t.factors:
                    # intercept, add anyway
                    rhs_termlist.append(t)
                elif not x == t.factors[0]._varname:
                    # this is not the term with x
                    rhs_termlist.append(t)

            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)
            return md

        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])
        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
        try:
            pars_to_prune.remove('Intercept')
        except:
            pass
        while pars_to_prune:
            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)
            fit = fm.ols(corrected_model_desc, data=self.df).fit()
            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()
            try:
                pars_to_prune.remove('Intercept')
            except:
                pass
        return fit

def find_best_rsquared(list_of_fits):
        Return a df with predictions and confidence interval

        Notes
        -----
        The df will contain the following columns:
        - 'predicted': the model output
        - 'interval_u', 'interval_l': upper and lower confidence bounds.

        The result will depend on the following attributes of self:

        confint : float (default=0.95)
            Confidence level for two-sided hypothesis

        allow_negative_predictions : bool (default=True)
            If False, correct negative predictions to zero (typically for energy consumption predictions)

        Parameters
        ----------
        fit : Statsmodels fit

        df : pandas DataFrame or None (default)
            If None, use self.df


        Returns
        -------
        df_res : pandas DataFrame
            Copy of df with additional columns 'predicted', 'interval_u' and 'interval_l'
    Calculate the relative abundance of each OTUID in a Sample.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :rtype: dict
    :return: Returns a keyed on SampleIDs, and the values are dictionaries keyed on
             OTUID's and their values represent the relative abundance of that OTUID in
             that SampleID.
    Calculate the mean OTU abundance percentage.

    :type ra: Dict
    :param ra: 'ra' refers to a dictionary keyed on SampleIDs, and the values are
               dictionaries keyed on OTUID's and their values represent the relative
               abundance of that OTUID in that SampleID. 'ra' is the output of
               relative_abundance() function.

    :type otuIDs: List
    :param otuIDs: A list of OTUID's for which the percentage abundance needs to be
                   measured.

    :rtype: dict
    :return: A dictionary of OTUID and their percent relative abundance as key/value pair.
    Calculate the mean relative abundance percentage.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: list
    :param sampleIDs: A list of sample id's from BIOM format OTU table.

    :param transform: Mathematical function which is used to transform smax to another

                      format. By default, the function has been set to None.

    :rtype: dict
    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given
             number of sampleIDs.
    Calculate the total number of sequences in each OTU or SampleID.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :type sampleIDs: List

    :param sampleIDs: A list of column id's from BIOM format OTU table. By default, the
                      list has been set to None.

    :type sample_abd: Boolean
    :param sample_abd: A boolean operator to provide output for OTUID's or SampleID's. By

                       default, the output will be provided for SampleID's.

    :rtype: dict
    :return: Returns a dictionary keyed on either OTUID's or SampleIDs and their
             respective abundance as values.
    Function to transform the total abundance calculation for each sample ID to another
    format based on user given transformation function.

    :type biomf: A BIOM file.
    :param biomf: OTU table format.

    :param fn: Mathematical function which is used to transform smax to another format.

               By default, the function has been given as base 10 logarithm.

    :rtype: dict
    :return: Returns a dictionary similar to output of raw_abundance function but with

             the abundance values modified by the mathematical operation. By default, the
             operation performed on the abundances is base 10 logarithm.
    Compute the Mann-Whitney U test for unequal group sample sizes.
    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that
    each group must have at least 5 measurements.
    parser = argparse.ArgumentParser(description="Calculate the alpha diversity\
                                     of a set of samples using one or more \
                                     metrics and output a kernal density \
                                     estimator-smoothed histogram of the \
                                     results.")
    parser.add_argument("-m", "--map_file",
                        help="QIIME mapping file.")
    parser.add_argument("-i", "--biom_fp",
                        help="Path to the BIOM table")
    parser.add_argument("-c", "--category",
                        help="Specific category from the mapping file.")

    parser.add_argument("-d", "--diversity", default=["shannon"], nargs="+",
                        help="The alpha diversity metric. Default \
                             value is 'shannon', which will calculate the Shannon\
                             entropy. Multiple metrics can be specified (space separated).\
                             The full list of metrics is available at:\
                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\
                             Beta diversity metrics will be supported in the future.")

    parser.add_argument("--x_label", default=[None], nargs="+",
                        help="The name of the diversity metric to be displayed on the\
                        plot as the X-axis label. If multiple metrics are specified,\
                        then multiple entries for the X-axis label should be given.")
    parser.add_argument("--color_by",
                        help="A column name in the mapping file containing\
                              hexadecimal (#FF0000) color values that will\
                              be used to color the groups. Each sample ID must\
                              have a color entry.")

    parser.add_argument("--plot_title", default="",
                        help="A descriptive title that will appear at the top \
                        of the output plot. Surround with quotes if there are\
                        spaces in the title.")

    parser.add_argument("-o", "--output_dir", default=".",
                        help="The directory plots will be saved to.")

    parser.add_argument("--image_type", default="png",
                        help="The type of image to save: png, svg, pdf, eps, etc...")
    parser.add_argument("--save_calculations",
                        help="Path and name of text file to store the calculated "
                        "diversity metrics.")
    parser.add_argument("--suppress_stats", action="store_true", help="Do not display "

                        "significance testing results which are shown by default.")
    parser.add_argument("--show_available_metrics", action="store_true",
                        help="Supply this parameter to see which alpha diversity metrics "
                             " are available for usage. No calculations will be performed"
                             " if this parameter is provided.")
    return parser.parse_args()

def blastdb(fasta, maxfile = 10000000):
    db = fasta.rsplit('.', 1)[0]
    type = check_type(fasta)
    if type == 'nucl':
        type = ['nhr', type]
    else:
        type = ['phr', type]
    if os.path.exists('%s.%s' % (db, type[0])) is False \
            and os.path.exists('%s.00.%s' % (db, type[0])) is False:
        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)
        os.system('makeblastdb \
                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \
                % (fasta, db, type[1], maxfile))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db

def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):
    if '.udb' in fasta:
        print('# ... database found: %s' % (fasta), file=sys.stderr)
        return fasta
    type = check_type(fasta)
    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)
    if os.path.exists(db) is False:
        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)
        if alignment == 'local':
            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))
        elif alignment == 'global':
            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))
    else:
        print('# ... database found for: %s' % (fasta), file=sys.stderr)
    return db

def _pp(dict_data):

    :param argparse.Namespace params: parameter
    :param bootstrap_py.classifier.Classifiers metadata: package metadata

    :param argparse.Namespace params: parameters

    :param argparse.Namespace params: parameters
    print single reads to stderr
    convert sam to fastq
    sort sam file
    randomly subset sam file
    convert fq to fa
    Converts the returned value of wrapped function to the type of the
    first arg or to the type specified by a kwarg key return_type's value.
    Converts all args to 'set' type via self.setify function.

        :param laman: Laman respons yang dikembalikan oleh KBBI daring.
        :type laman: Response

        :param dasar: ResultSet untuk label HTML dengan class="rootword"
        :type dasar: ResultSet

        :returns: Dictionary hasil serialisasi
        :rtype: dict

        :returns: String representasi makna-makna
        :rtype: str

        :returns: String representasi nama entri
        :rtype: str
        Dapat digunakan untuk "Varian" maupun "Bentuk tidak baku".

        :param varian: List bentuk tidak baku atau varian
        :type varian: list
        :returns: String representasi varian atau bentuk tidak baku
        :rtype: str

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup

        :param makna_label: BeautifulSoup untuk makna yang ingin diproses.
        :type makna_label: BeautifulSoup

        :returns: Dictionary hasil serialisasi
        :rtype: dict

    :rtype: int
    :return: subprocess.call return code

    :param `bootstrap_py.control.PackageData` pkg_data: package meta data
    :param str projectdir: project root directory
    make bowtie db
    generate bowtie2 command
    map all read sets against all fasta files
        Returns a connection object from the router given ``args``.

        Useful in cases where a connection cannot be automatically determined
        during all steps of the process. An example of this would be
        Redis pipelines.
        return the non-direct init if the direct algorithm has been selected.
        reflect a data word, i.e. reverts the bit order.
        Classic simple and slow CRC implementation.  This function iterates bit
        by bit over the augmented input message and returns the calculated CRC
        value at the end.
        This function generates the CRC table used for the table_driven CRC
        algorithm.  The Python version cannot handle tables of an index width
        other than 8.  See the generated C code for tables with different sizes
        instead.
        The Standard table_driven CRC algorithm.
    parse masked sequence into non-masked and masked regions
    remove masked regions from fasta file as long as
    they are longer than min_len
    Return arcsine transformed relative abundance from a BIOM format file.

    :type biomfile: BIOM format file
    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in
                     a SampleID, which are used as node sizes in network plots.

    :type return: Dictionary of dictionaries.
    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name
             whose value is the arc sine tranfsormed relative abundance value for that
             SampleID-OTU Name pair.
    Find an OTU ID in a Newick-format tree.
    Return the starting position of the ID or None if not found.
    Replace the OTU ids in the Newick phylogenetic tree format with truncated
    OTU names
    return genome info for choosing representative

    if ggKbase table provided - choose rep based on SCGs and genome length
        - priority for most SCGs - extra SCGs, then largest genome

    otherwise, based on largest genome
    choose represenative genome and
    print cluster information

    *if ggKbase table is provided, use SCG info to choose best genome
    convert ggKbase genome info tables to dictionary
    convert checkM genome info tables to dictionary
    get genome lengths
        Returns a list of db keys to route the given call to.

        :param attr: Name of attribute being called on the connection.
        :param args: List of arguments being passed to ``attr``.
        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.

        >>> redis = Cluster(router=BaseRouter)
        >>> router = redis.router
        >>> router.get_dbs('incr', args=('key name', 1))
        [0,1,2]

        Call method to perform any setup
        Perform routing and return db_nums
        Iterates through all connections which were previously listed as unavailable
        and marks any that have expired their retry_timeout as being up.
        Marks all connections which were previously listed as unavailable as being up.
    Compute standby power

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Electricity Power

    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'

    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    df : pandas.Series with DateTimeIndex in the given resolution
    Compute the share of the standby power in the total consumption.

    Parameters
    ----------
    df : pandas.DataFrame or pandas.Series
        Power (typically electricity, can be anything)

    resolution : str, default='d'
        Resolution of the computation.  Data will be resampled to this resolution (as mean) before computation
        of the minimum.
        String that can be parsed by the pandas resample function, example ='h', '15min', '6h'

    time_window : tuple with start-hour and end-hour, default=None
        Specify the start-time and end-time for the analysis.
        Only data within this time window will be considered.
        Both times have to be specified as string ('01:00', '06:30') or as datetime.time() objects

    Returns
    -------
    fraction : float between 0-1 with the share of the standby consumption
    Toggle counter for gas boilers

    Counts the number of times the gas consumption increases with more than 3kW

    Parameters
    ----------
    ts: Pandas Series
        Gas consumption in minute resolution

    Returns
    -------
    int
    Calculate the ratio of input vs. norm over a given interval.

    Parameters
    ----------
    ts : pandas.Series
        timeseries
    resolution : str, optional
        interval over which to calculate the ratio

        default: resolution of the input timeseries
    norm : int | float, optional
        denominator of the ratio

        default: the maximum of the input timeseries

    Returns
    -------
    pandas.Series
    get top hits after sorting by column number
    parse b6 output with sorting
    parse b6 output
    parse hmm domain table output
    this version is faster but does not work unless the table is sorted
    convert stockholm to fasta

    Parameters
    ----------
    index : pandas.DatetimeIndex
        Datetime index
    on_time : str or datetime.time
        Daily opening time. Default: '09:00'
    off_time : str or datetime.time
        Daily closing time. Default: '17:00'
    off_days : list of str
        List of weekdays. Default: ['Sunday', 'Monday']

    Returns
    -------
    pandas.Series of bool
        True when on, False otherwise for given datetime index

    Examples
    --------
    >>> import pandas as pd
    >>> from opengrid.library.utils import week_schedule
    >>> index = pd.date_range('20170701', '20170710', freq='H')
    >>> week_schedule(index)
    Draw a carpet plot of a pandas timeseries.

    The carpet plot reads like a letter. Every day one line is added to the
    bottom of the figure, minute for minute moving from left (morning) to right
    (evening).
    The color denotes the level of consumption and is scaled logarithmically.
    If vmin and vmax are not provided as inputs, the minimum and maximum of the
    colorbar represent the minimum and maximum of the (resampled) timeseries.

    Parameters
    ----------
    timeseries : pandas.Series
    vmin, vmax : If not None, either or both of these values determine the range
    of the z axis. If None, the range is given by the minimum and/or maximum
    of the (resampled) timeseries.
    zlabel, title : If not None, these determine the labels of z axis and/or

    title. If None, the name of the timeseries is used if defined.

    cmap : matplotlib.cm instance, default coolwarm

    Examples
    --------
    >>> import numpy as np
    >>> import pandas as pd
    >>> from opengrid.library import plotting
    >>> plt = plotting.plot_style()
    >>> index = pd.date_range('2015-1-1','2015-12-31',freq='h')
    >>> ser = pd.Series(np.random.normal(size=len(index)), index=index, name='abc')
    >>> im = plotting.carpet(ser)
    calculate percent identity
    skip column if either is a gap
    compare pairs of sequences
    calculate Levenshtein ratio of sequences
    make pairwise sequence comparisons between aligned sequences
    print matrix of pidents to stdout
    print stats for comparisons
    print min. pident within each clade and then matrix of between-clade max.
    convert matrix to dictionary of comparisons
    parser.add_argument('-v', action='version',
                        version=__version__)
    subparsers = parser.add_subparsers(help='sub commands help')
    create_cmd = subparsers.add_parser('create')
    create_cmd.add_argument('name',
                            help='Specify Python package name.')
    create_cmd.add_argument('-d', dest='description', action='store',
                            help='Short description about your package.')
    create_cmd.add_argument('-a', dest='author', action='store',
                            required=True,
                            help='Python package author name.')
    create_cmd.add_argument('-e', dest='email', action='store',
                            required=True,
                            help='Python package author email address.')
    create_cmd.add_argument('-l', dest='license',
                            choices=metadata.licenses().keys(),

                            default='GPLv3+',

                            help='Specify license. (default: %(default)s)')
    create_cmd.add_argument('-s', dest='status',
                            choices=metadata.status().keys(),

                            default='Alpha',
                            help=('Specify development status. '

                                  '(default: %(default)s)'))
    create_cmd.add_argument('--no-check', action='store_true',
                            help='No checking package name in PyPI.')
    create_cmd.add_argument('--with-samples', action='store_true',
                            help='Generate package with sample code.')
    group = create_cmd.add_mutually_exclusive_group(required=True)
    group.add_argument('-U', dest='username', action='store',
                       help='Specify GitHub username.')
    group.add_argument('-u', dest='url', action='store', type=valid_url,
                       help='Python package homepage url.')
    create_cmd.add_argument('-o', dest='outdir', action='store',

                            default=os.path.abspath(os.path.curdir),

                            help='Specify output directory. (default: $PWD)')
    list_cmd = subparsers.add_parser('list')
    list_cmd.add_argument('-l', dest='licenses', action='store_true',
                          help='show license choices.')

def parse_options(metadata):
    try:
        pkg_version = Update()
        if pkg_version.updatable():
            pkg_version.show_message()
        metadata = control.retreive_metadata()
        parser = parse_options(metadata)
        argvs = sys.argv
        if len(argvs) <= 1:
            parser.print_help()
            sys.exit(1)
        args = parser.parse_args()
        control.print_licences(args, metadata)
        control.check_repository_existence(args)
        control.check_package_existence(args)
        control.generate_package(args)
    except (RuntimeError, BackendFailure, Conflict) as exc:
        sys.stderr.write('{0}\n'.format(exc))
        sys.exit(1)

def _check_or_set_default_params(self):
        if not os.path.isdir(self.outdir):
            os.makedirs(self.outdir)
        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))

def vcs_init(self):
  Finds the location of the current Steam installation on Windows machines.
  Returns None for any non-Windows machines, or for Windows machines where
  Steam is not installed.
    Plot PCoA principal coordinates scaled by the relative abundances of
    otu_name.
    Split up the column data in a biom table by mapping category value.
    print line if starts with ...
    convert stockholm to single line format
    Statics the methods. wut.
    Show stats when pings are done
        if self.latest_version > self.current_version:
            updatable_version = self.latest_version
        else:
            updatable_version = False
        return updatable_version

def show_message(self):
    Traverse the input otu-sequence file, collect the non-unique OTU IDs and

    file the sequences associated with then under the unique OTU ID as defined
    by the input matrix.

    :@type otuF: file
    :@param otuF: The output file from QIIME's pick_otus.py
    :@type nuniqueF: file
    :@param nuniqueF: The matrix of unique OTU IDs associated to the list of
                      non-unique OTU IDs they replaced.

    :@rtype: dict
    :@return: The new condensed table of unique OTU IDs and the sequence IDs
              associated with them.
    determine if read overlaps with rna, if so count bases
    parse ggKbase scaffold-to-bin mapping
        - scaffolds-to-bins and bins-to-scaffolds
    remove any bins that don't have 16S
    calculate bin coverage
        Make sure there is at least a translation has been filled in. If a

        default language has been specified, make sure that it exists amongst
        translations.

        If a default language has been set, and is still available in
        `self.available_languages`, return it and remove it from the list.

        If not, simply pop the first available language.
        Construct the form, overriding the initial value for `language_code`.
    merge separate fastq files
            Creates hash ring.
            Return long integer for a given key, that represent it place on
            the hash ring.
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  return any(map(os.path.exists, possible_paths))

def get_custom_image(user_context, app_id):
  possible_paths = _valid_custom_image_paths(user_context, app_id)
  existing_images = filter(os.path.exists, possible_paths)
  if len(existing_images) > 0:
    return existing_images[0]

def set_custom_image(user_context, app_id, image_path):
  if image_path is None:
    return False

  if not os.path.exists(image_path):
    return False

  (root, ext) = os.path.splitext(image_path)
  if not is_valid_extension(ext):
    # TODO: Maybe log that this happened?
    return False
  # If we don't remove the old image then theres no guarantee that Steam will
  # show our new image when it launches.
  if has_custom_image(user_context, app_id):
    img = get_custom_image(user_context, app_id)
    assert(img is not None)
    os.remove(img)
  
  # Set the new image
  parent_dir = paths.custom_images_directory(user_context)
  new_path = os.path.join(parent_dir, app_id + ext)
  shutil.copyfile(image_path, new_path)
  return True

def from_file(cls, fname, form=None):
        try:
            tg = TableGroup.from_file(fname)
            opfname = None
        except JSONDecodeError:
            tg = TableGroup.fromvalue(cls.MD)
            opfname = fname
        if len(tg.tables) != 1:
            raise ValueError('profile description must contain exactly one table')
        metadata = tg.common_props
        metadata.update(fname=Path(fname), form=form)
        return cls(
            *[{k: None if (k != cls.GRAPHEME_COL and v == cls.NULL) else v for k, v in d.items()}
              for d in tg.tables[0].iterdicts(fname=opfname)],
            **metadata)

def from_text(cls, text, mapping='mapping'):
        graphemes = Counter(grapheme_pattern.findall(text))
        specs = [
            OrderedDict([
                (cls.GRAPHEME_COL, grapheme),
                ('frequency', frequency),
                (mapping, grapheme)])
            for grapheme, frequency in graphemes.most_common()]
        return cls(*specs)

def split_fasta(f, id2f):
    opened = {}
    for seq in parse_fasta(f):
        id = seq[0].split('>')[1].split()[0]
        if id not in id2f:
            continue
        fasta = id2f[id]
        if fasta not in opened:
            opened[fasta] = '%s.fa' % fasta
        seq[1] += '\n'
        with open(opened[fasta], 'a+') as f_out:
            f_out.write('\n'.join(seq))

def _is_user_directory(self, pathname):
      fullpath = os.path.join(self.userdata_location(), pathname)
      # SteamOS puts a directory named 'anonymous' in the userdata directory

      # by default. Since we assume that pathname is a userID, ignore any name
      # that can't be converted to a number
      return os.path.isdir(fullpath) and pathname.isdigit()

def local_users(self):
    Calculates degree days, starting with a series of temperature equivalent values

    Parameters
    ----------
    temperature_equivalent : Pandas Series
    base_temperature : float
    cooling : bool
        Set True if you want cooling degree days instead of heating degree days

    Returns
    -------
    Pandas Series called HDD_base_temperature for heating degree days or
    CDD_base_temperature for cooling degree days.
        return {self._acronym_status(l): l for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_status)}

def licenses(self):
        return {self._acronym_lic(l): l.split(self.prefix_lic)[1]
                for l in self.resp_text.split('\n')
                if l.startswith(self.prefix_lic)}

def _acronym_lic(self, license_statement):
    calc MD5 based on path
    download files with wget
    check that at least one of
    queries is in list, l
    search entrez using specified database
    and accession
    attempt to use NCBI Entrez to get
    BioSample ID
    download genome info from NCBI
    download genomes from NCBI
    remove pesky characters from fasta file header
    Compute a DataFrame summary of a Stats object.
    get unmapped reads
    execute jobs in processes using N threads
    the final log processor that structlog requires to render.
        Establish the connection. This is done automatically for you.

        If you lose the connection, you can manually run this to be re-connected.
        Create a folder on S3.

        Examples
        --------
            >>> s3utils.mkdir("path/to/my_folder")
            Making directory: path/to/my_folder
        Delete the path and anything under the path.

        Example
        -------
            >>> s3utils.rm("path/to/file_or_folder")
        action_word = "moving" if del_after_upload else "copying"

        try:
            self.k.key = target_file  # setting the path (key) of file in the container

            if source == "filename":
                # grabs the contents from local_file address. Note that it loads the whole file into memory
                self.k.set_contents_from_filename(local_file, self.AWS_HEADERS)
            elif source == "fileobj":
                self.k.set_contents_from_file(local_file, self.AWS_HEADERS)
            elif source == "string":
                self.k.set_contents_from_string(local_file, self.AWS_HEADERS)
            else:
                raise Exception("%s is not implemented as a source." % source)
            self.k.set_acl(acl)  # setting the file permissions
            self.k.close()  # not sure if it is needed. Somewhere I read it is recommended.

            self.printv("%s %s to %s" % (action_word, local_file, target_file))
            # if it is supposed to delete the local file after uploading
            if del_after_upload and source == "filename":
                try:
                    os.remove(local_file)
                except:
                    logger.error("Unable to delete the file: ", local_file, exc_info=True)

            return True

        except:
            logger.error("Error in writing to %s", target_file, exc_info=True)
            return False

def cp(self, local_path, target_path, acl='public-read',
           del_after_upload=False, overwrite=True, invalidate=False):
        result = None
        if overwrite:
            list_of_files = []
        else:
            list_of_files = self.ls(folder=target_path, begin_from_file="", num=-1, get_grants=False, all_grant_data=False)

        # copying the contents of the folder and not folder itself
        if local_path.endswith("/*"):
            local_path = local_path[:-2]
            target_path = re.sub(r"^/|/$", "", target_path)  # Amazon S3 doesn't let the name to begin with /
        # copying folder too
        else:
            local_base_name = os.path.basename(local_path)

            local_path = re.sub(r"/$", "", local_path)
            target_path = re.sub(r"^/", "", target_path)

            if not target_path.endswith(local_base_name):
                target_path = os.path.join(target_path, local_base_name)

        if os.path.exists(local_path):

            result = self.__find_files_and_copy(local_path, target_path, acl, del_after_upload, overwrite, invalidate, list_of_files)

        else:
            result = {'file_does_not_exist': local_path}
            logger.error("trying to upload to s3 but file doesn't exist: %s" % local_path)

        return result

def mv(self, local_file, target_file, acl='public-read', overwrite=True, invalidate=False):
        self.cp(local_file, target_file, acl=acl, del_after_upload=True, overwrite=overwrite, invalidate=invalidate)

def cp_cropduster_image(self, the_image_path, del_after_upload=False, overwrite=False, invalidate=False):

        local_file = os.path.join(settings.MEDIA_ROOT, the_image_path)

        # only try to upload things if the origin cropduster file exists (so it is not already uploaded to the CDN)
        if os.path.exists(local_file):

            the_image_crops_path = os.path.splitext(the_image_path)[0]
            the_image_crops_path_full_path = os.path.join(settings.MEDIA_ROOT, the_image_crops_path)

            self.cp(local_path=local_file,
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )

            self.cp(local_path=the_image_crops_path_full_path + "/*",
                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_crops_path),
                    del_after_upload=del_after_upload,
                    overwrite=overwrite,
                    invalidate=invalidate,
                    )

def chmod(self, target_file, acl='public-read'):
        self.k.key = target_file  # setting the path (key) of file in the container
        self.k.set_acl(acl)  # setting the file permissions
        self.k.close()

def ll(self, folder="", begin_from_file="", num=-1, all_grant_data=False):
        return self.ls(folder=folder, begin_from_file=begin_from_file, num=num, get_grants=True, all_grant_data=all_grant_data)

def get_path(url):

    url = urlsplit(url)
    path = url.path
    if url.query:
        path += "?{}".format(url.query)
    return path

def run(self):
		
		result = []
		strip = ''.join(self.PREFIX_MAP)
		
		for field in fields:
			direction = self.PREFIX_MAP['']
			
			if field[0] in self.PREFIX_MAP:
				direction = self.PREFIX_MAP[field[0]]
				field = field.lstrip(strip)
			
			result.append((field, direction))
		
		return result

def search_in_rubric(self, **kwargs):

        point = kwargs.pop('point', False)
        if point:
            kwargs['point'] = '%s,%s' % point

        bound = kwargs.pop('bound', False)
        if bound:
            kwargs['bound[point1]'] = bound[0]
            kwargs['bound[point2]'] = bound[1]

        filters = kwargs.pop('filters', False)
        if filters:
            for k, v in filters.items():
                kwargs['filters[%s]' % k] = v

        return self._search_in_rubric(**kwargs)

def refresh(self):
        '''
        self._screen.force_update()
        self._screen.refresh()
        self._update(1)

def start(self, activity, action):
        '''
        try:
            self._start_action(activity, action)
        except ValueError:
            retox_log.debug("Could not find action %s in env %s" % (activity, self.name))
        self.refresh()

def stop(self, activity, action):
        '''
        try:
            self._remove_running_action(activity, action)
        except ValueError:
            retox_log.debug("Could not find action %s in env %s" % (activity, self.name))
        self._mark_action_completed(activity, action)
        self.refresh()

def finish(self, status):
        '''
        retox_log.info("Completing %s with status %s" % (self.name, status))
        result = Screen.COLOUR_GREEN if not status else Screen.COLOUR_RED
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, result)
        for item in list(self._task_view.options):
            self._task_view.options.remove(item)
            self._completed_view.options.append(item)
        self.refresh()

def reset(self):
        '''
        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, Screen.COLOUR_BLUE)
        self._completed_view.options = []
        self._task_view.options = []
        self.refresh()

def default_arguments(cls):
        cls.check_arguments(kwargs)
        first_is_callable = True if any(args) and callable(args[0]) else False

        signature = cls.default_arguments()
        allowed_arguments = {k: v for k, v in kwargs.items() if k in signature}
        if (any(allowed_arguments) or any(args)) and not first_is_callable:
            if any(args) and not first_is_callable:
                return cls(args[0], **allowed_arguments)
            elif any(allowed_arguments):
                return cls(**allowed_arguments)

        return cls.instances[-1] if any(cls.instances) else cls()

def check_arguments(cls, passed):
        if type in history:
            return
        if type.enum():
            return
        history.append(type)
        resolved = type.resolve()
        value = None
        if type.multi_occurrence():
            value = []
        else:
            if len(resolved) > 0:
                if resolved.mixed():
                    value = Factory.property(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                else:
                    value = Factory.object(resolved.name)
                    md = value.__metadata__
                    md.sxtype = resolved
                    md.ordering = self.ordering(resolved)
        setattr(data, type.name, value)
        if value is not None:
            data = value
        if not isinstance(data, list):
            self.add_attributes(data, resolved)
            for child, ancestry in resolved.children():
                if self.skip_child(child, ancestry):
                    continue
                self.process(data, child, history[:])

def skip_child(self, child, ancestry):
    Checks whether knocks are enabled for the model given as argument

    :param obj: model instance
    :return True if knocks are active
    Context manager to suspend sending knocks for the given model

    :param obj: model instance

    Args:
        email (str/obj): A markdown string or EmailContent object 
        subject (str): subject line
        from_email (str): sender email address
        to_email (str/list): recipient email addresses
        cc (str/list): CC email addresses (string or a list)
        bcc (str/list): BCC email addresses (string or a list)
        reply_to (str): Reply-to email address
        smtp (dict): SMTP configuration (dict)

    Schema of smtp dict:
        host (str): SMTP server host. Default: localhost
        port (int): SMTP server port. Default: 25
        tls (bool): Use TLS. Default: False
        ssl (bool): Use SSL. Default: False
        user (bool): SMTP login user. Default empty
        password (bool): SMTP login password. Default empty
		

		def _tz(t):
			if t in (None, 'naive'):
				return t
			
			if t == 'local':
				if __debug__ and not localtz:
					raise ValueError("Requested conversion to local timezone, but `localtz` not installed.")
				
				t = localtz
			
			if not isinstance(t, tzinfo):
				if __debug__ and not localtz:
					raise ValueError("The `pytz` package must be installed to look up timezone: " + repr(t))
				
				t = get_tz(t)
			
			if not hasattr(t, 'normalize') and get_tz:  # Attempt to handle non-pytz tzinfo.
				t = get_tz(t.tzname(dt))
			
			return t
		
		naive = _tz(naive)
		tz = _tz(tz)
		
		if not dt.tzinfo and naive:
			if hasattr(naive, 'localize'):
				dt = naive.localize(dt)
			else:
				dt = dt.replace(tzinfo=naive)
		
		if not tz:
			return dt
		
		if hasattr(tz, 'normalize'):
			dt = tz.normalize(dt.astimezone(tz))
		elif tz == 'naive':
			dt = dt.replace(tzinfo=None)
		else:
			dt = dt.astimezone(tz)  # Warning: this might not always be entirely correct!
		
		return dt

def _prepare_defaults(self):
		
		if doc is None:  # To support simplified iterative use, None should return None.
			return None
		
		if isinstance(doc, Document):  # No need to perform processing on existing Document instances.
			return doc
		
		if cls.__type_store__ and cls.__type_store__ in doc:  # Instantiate specific class mentioned in the data.
			cls = load(doc[cls.__type_store__], 'marrow.mongo.document')
		
		# Prepare a new instance in such a way that changes to the instance will be reflected in the originating doc.

		instance = cls(_prepare_defaults=False)  # Construct an instance, but delay default value processing.
		instance.__data__ = doc  # I am Popeye of Borg (pattern); you will be askimilgrated.

		instance._prepare_defaults()  # pylint:disable=protected-access -- deferred default value processing.
		
		return instance

def pop(self, name, default=SENTINEL):
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._op(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError("{self!r} does not allow {op} comparison.".format(self=self, op=operation))
		
		if other is not None:
			other = f.transformer.foreign(other, (f, self._document))
		
		return Filter({self._name: {operation: other}})

def _iop(self, operation, other, *allowed):
		
		f = self._field
		
		if self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).
			return reduce(self._combining,
					(q._iop(operation, other, *allowed) for q in f))  # pylint:disable=protected-access
		
		# Optimize this away in production; diagnosic aide.
		if __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover
			raise NotImplementedError("{self!r} does not allow {op} comparison.".format(
					self=self, op=operation))
		

		def _t(o):
			for value in o:
				yield None if value is None else f.transformer.foreign(value, (f, self._document))
		
		other = other if len(other) > 1 else other[0]
		values = list(_t(other))
		
		return Filter({self._name: {operation: values}})

def __dfs(self, v, index, layers):
        if index == 0:
            path = [v]
            while self._dfs_parent[v] != v:
                path.append(self._dfs_parent[v])
                v = self._dfs_parent[v]
            self._dfs_paths.append(path)
            return True
        for neighbour in self._graph[v]:  # check the neighbours of vertex
            if neighbour in layers[index - 1]:
                # if neighbour is in left, we are traversing unmatched edges..
                if neighbour in self._dfs_parent:
                    continue
                if (neighbour in self._left and (v not in self._matching or neighbour != self._matching[v])) or \
                        (neighbour in self._right and (v in self._matching and neighbour == self._matching[v])):
                    self._dfs_parent[neighbour] = v
                    if self.__dfs(neighbour, index-1, layers):
                        return True
        return False

def method(self, symbol):
        '''
        assert issubclass(symbol, SymbolBase)

        def wrapped(fn):
            setattr(symbol, fn.__name__, fn)
        return wrapped

def _simpleparsefun(date):

        Connect signal to current model
        Disconnect signal from current model
        Returns a dictionary with the knock data built from _knocker_data
        Send the knock in the associated channels Group

    :param printable: interface whose has __str__ or __repr__ method

    :param color: the colors defined in COLOR_MAP to colorize the text
    :style: can be 'normal', 'bold' or 'underline'

    :returns: the 'printable' colorized with style

    Change text color for the linux terminal, defaults to green.
    Set "warning=True" for red.
	
	While not nessicarily recommended, you can use this to inject `tail` as a method into Collection, making it
	generally accessible.
		
		Additionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.
		
		Doc, collection, query, options = self._prepare_find(id=self.id, projection=fields, **kw)
		result = collection.find_one(query, **options)
		
		if fields:  # Refresh only the requested data.
			for k in result:  # TODO: Better merge algorithm.
				if k == ~Doc.id: continue
				self.__data__[k] = result[k]
		else:
			self.__data__ = result
		
		return self

def get(cls):
        results = {}

        hierarchy = cls.__hierarchy
        hierarchy.reverse()

        for storeMethod in hierarchy:
            cls.merger.merge(results, storeMethod.get())

        return results

def argv(cls, name, short_name=None, type=None, help=None):
        cls.__hierarchy.append(argv.Argv(name, short_name, type, help))

def env(cls, separator=None, match=None, whitelist=None, parse_values=None, to_lower=None, convert_underscores=None):
        cls.__hierarchy.append(env.Env(separator, match, whitelist, parse_values, to_lower, convert_underscores))

def file(cls, path, encoding=None, parser=None):
        cls.__hierarchy.append(file.File(path, encoding, parser))

def P(Document, *fields, **kw):
		
		Return True if valid, False if explicitly invalid, None if unknown.
		
		result = self._Document.get_collection().delete_one({'_id': sid})
		
		return result.deleted_count == 1

def persist(self, context):
    Channels connection setup.
    Register the current client on the related Group according to the language
    Channels connection close.
    Deregister the client
        if self.enabled:
            if autopush:
                self.push_message(self.message)
                self.spinner.message = ' - '.join(self.animation.messages)
            if not self.spinner.running:
                self.animation.thread = threading.Thread(target=_spinner,
                                                         args=(self.spinner,))
                self.spinner.running = True
                self.animation.thread.start()
                sys.stdout = stream.Clean(sys.stdout, self.spinner.stream)

def stop(cls):

        args: a set of args passed on the wrapper __call__ in

              the definition above.


        if the object already have some message (defined in __init__),
        we don't change that. If the first arg is a function, so is decorated
        without argument, use the func name as the message.


        If not self.message anyway, use the default_message global,

        another else  use the default self.message already

        self.streams.append(sys.stdout)
        sys.stdout = self.stream

def stop(cls):
		
		Attempting to prolong a lock not already owned will result in a Locked exception.
		
		Unless forcing, if we are not the current owners of the lock a Locked exception will be raised.
        super(Animation, self).write(message)
        self.last_message = message
        if autoerase:
            time.sleep(self.interval)
            self.erase(message)

def write(self, message, flush=False):
        for char in message:
            time.sleep(self.delay * (4 if char == '\n' else 1))
            super(Writting, self).write(char, flush)

def _get_default_projection(cls):
	
	amount = None
	
	if fields and isinstance(fields[0], int):
		amount, fields = fields[0], fields[1:]
	

	def adjust_inner(cls):
		for field in fields:
			if field not in cls.__dict__:

				# TODO: Copy the field definition.
				raise TypeError("Can only override sequence on non-inherited attributes.")
			
			# Adjust the sequence to re-order the field.
			if amount is None:
				cls.__dict__[field].__sequence__ = ElementMeta.sequence
			else:
				cls.__dict__[field].__sequence__ += amount  # Add the given amount.
		
		# Update the attribute collection.
		cls.__attributes__ = OrderedDict(
					(k, v) for k, v in \
					sorted(cls.__attributes__.items(),
						key=lambda i: i[1].__sequence__)
				)
		
		return cls
	
	return adjust_inner

def get_hashes(path, exclude=None):
    '''
    out = {}
    for f in Path(path).rglob('*'):
        if f.is_dir():
            # We want to watch files, not directories.
            continue
        if exclude and re.match(exclude, f.as_posix()):
            retox_log.debug("excluding '{}'".format(f.as_posix()))
            continue
        pytime = f.stat().st_mtime
        out[f.as_posix()] = pytime
    return out

def request(self, method, params=None, query_continue=None,
                files=None, auth=None, continuation=False):
        normal_params = _normalize_params(params, query_continue)
        if continuation:
            return self._continuation(method, params=normal_params, auth=auth,
                                      files=files)
        else:
            return self._request(method, params=normal_params, auth=auth,
                                 files=files)

def login(self, username, password, login_token=None):
        if login_token is None:
            token_doc = self.post(action='query', meta='tokens', type='login')
            login_token = token_doc['query']['tokens']['logintoken']

        login_doc = self.post(
            action="clientlogin", username=username, password=password,
            logintoken=login_token, loginreturnurl="http://example.org/")

        if login_doc['clientlogin']['status'] == "UI":
            raise ClientInteractionRequest.from_doc(
                login_token, login_doc['clientlogin'])
        elif login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']

def continue_login(self, login_token, **params):

        login_params = {
            'action': "clientlogin",
            'logintoken': login_token,
            'logincontinue': 1
        }
        login_params.update(params)
        login_doc = self.post(**login_params)
        if login_doc['clientlogin']['status'] != 'PASS':
            raise LoginError.from_doc(login_doc['clientlogin'])
        return login_doc['clientlogin']

def get(self, query_continue=None, auth=None, continuation=False,
            **params):

        return self.request('GET', params=params, auth=auth,
                            query_continue=query_continue,
                            continuation=continuation)

def post(self, query_continue=None, upload_file=None, auth=None,
             continuation=False, **params):
        if upload_file is not None:
            files = {'file': upload_file}
        else:
            files = None

        return self.request('POST', params=params, auth=auth,
                            query_continue=query_continue, files=files,
                            continuation=continuation)

def promote(self, cls, update=False, preserve=True):
    cutting nodes away from menus
		
		value = super(Expires, cls).from_mongo(data, **kw)
		
		if not expired and value.is_expired:
			return None
		
		return value

def S(Document, *fields):
        self._assure_output_dir(self.output)
        companies = self.read()
        print '%s CNPJs found' % len(companies)

        pbar = ProgressBar(
            widgets=[Counter(), ' ', Percentage(), ' ', Bar(), ' ', Timer()],
            maxval=len(companies)).start()

        resolved = 0
        runner = Runner(companies, self.days, self.token)

        try:
            for data in runner:
                self.write(data)
                resolved = resolved + 1
                pbar.update(resolved)
        except KeyboardInterrupt:
            print '\naborted: waiting current requests to finish.'
            runner.stop()
            return

        pbar.finish()

def read(self):
        cnpj, data = data

        path = os.path.join(self.output, '%s.json' % cnpj)
        with open(path, 'w') as f:
            json.dump(data, f, encoding='utf-8')

def valid(self, cnpj):
        if len(cnpj) != 14:
            return False

        tam = 12
        nums = cnpj[:tam]
        digs = cnpj[tam:]

        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[0]):
            return False

        tam = tam + 1
        nums = cnpj[:tam]
        tot = 0
        pos = tam-7
        for i in range(tam, 0, -1):
            tot = tot + int(nums[tam-i])*pos
            pos = pos - 1
            if pos < 2:
                pos = 9
        res = 0 if tot % 11 < 2 else 11 - (tot % 11)
        if res != int(digs[1]):
            return False

        return True

def get_default_config_filename():
    global _CONFIG_FN
    if _CONFIG_FN is not None:
        return _CONFIG_FN
    with _CONFIG_FN_LOCK:
        if _CONFIG_FN is not None:
            return _CONFIG_FN
        if 'PEYOTL_CONFIG_FILE' in os.environ:
            cfn = os.path.abspath(os.environ['PEYOTL_CONFIG_FILE'])
        else:
            cfn = os.path.expanduser("~/.peyotl/config")
        if not os.path.isfile(cfn):
            # noinspection PyProtectedMember
            if 'PEYOTL_CONFIG_FILE' in os.environ:
                from peyotl.utility.get_logger import warn_from_util_logger
                msg = 'Filepath "{}" specified via PEYOTL_CONFIG_FILE={} was not found'.format(cfn, os.environ[
                    'PEYOTL_CONFIG_FILE'])
                warn_from_util_logger(msg)
            from pkg_resources import Requirement, resource_filename
            pr = Requirement.parse('peyotl')

            cfn = resource_filename(pr, 'peyotl/default.conf')
        if not os.path.isfile(cfn):
            raise RuntimeError('The peyotl configuration file cascade failed looking for "{}"'.format(cfn))
        _CONFIG_FN = os.path.abspath(cfn)
    return _CONFIG_FN

def get_raw_default_config_and_read_file_list():
    global _DEFAULT_CONFIG_WRAPPER
    if _DEFAULT_CONFIG_WRAPPER is not None:
        return _DEFAULT_CONFIG_WRAPPER
    with _DEFAULT_CONFIG_WRAPPER_LOCK:
        if _DEFAULT_CONFIG_WRAPPER is not None:
            return _DEFAULT_CONFIG_WRAPPER
        _DEFAULT_CONFIG_WRAPPER = ConfigWrapper()
        return _DEFAULT_CONFIG_WRAPPER

def get_from_config_setting_cascade(self, sec_param_list, default=None, warn_on_none_level=logging.WARN):
        for section, param in sec_param_list:

            r = self.get_config_setting(section, param, default=None, warn_on_none_level=None)
            if r is not None:
                return r
        section, param = sec_param_list[-1]

        if default is None:
            _warn_missing_setting(section, param, self._config_filename, warn_on_none_level)

        return default

def parse(input_: Union[str, FileStream], source: str) -> Optional[str]:

    # Step 1: Tokenize the input stream
    error_listener = ParseErrorListener()
    if not isinstance(input_, FileStream):
        input_ = InputStream(input_)
    lexer = jsgLexer(input_)
    lexer.addErrorListener(error_listener)
    tokens = CommonTokenStream(lexer)
    tokens.fill()
    if error_listener.n_errors:
        return None

    # Step 2: Generate the parse tree
    parser = jsgParser(tokens)
    parser.addErrorListener(error_listener)
    parse_tree = parser.doc()
    if error_listener.n_errors:
        return None

    # Step 3: Transform the results the results
    parser = JSGDocParser()
    parser.visit(parse_tree)


    if parser.undefined_tokens():

        for tkn in parser.undefined_tokens():

            print("Undefined token: " + tkn)
        return None

    return parser.as_python(source)

def fetch(self, request, callback=None, raise_error=True, **kwargs):
        # accepts request as string then convert it to HTTPRequest
        if isinstance(request, str):
            request = HTTPRequest(request, **kwargs)

        try:
            # The first request calls tornado-client ignoring the
            # possible exception, in case of 401 response,
            # renews the access token and replay it
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=False,
                                                    **kwargs)

            if response.code == BAD_TOKEN:
                yield self._token_manager.reset_token()
            elif response.error and raise_error:
                raise response.error
            else:
                raise gen.Return(response)

            # The request with renewed token
            response = yield self._authorized_fetch(request,
                                                    callback,
                                                    raise_error=raise_error,
                                                    **kwargs)
            raise gen.Return(response)

        except TokenError as err:
            yield self._token_manager.reset_token()
            raise err

def validate_config(key: str, config: dict) -> None:

    try:
        jsonschema.validate(config, CONFIG_JSON_SCHEMA[key])
    except jsonschema.ValidationError as x_validation:
        raise JSONValidation('JSON validation error on {} configuration: {}'.format(key, x_validation.message))
    except jsonschema.SchemaError as x_schema:
        raise JSONValidation('JSON schema error on {} specification: {}'.format(key, x_schema.message))

def __make_id(receiver):
    if __is_bound_method(receiver):
        return (id(receiver.__func__), id(receiver.__self__))
    return id(receiver)

def __purge():
    global __receivers

    newreceivers = collections.defaultdict(list)

    for signal, receivers in six.iteritems(__receivers):
        alive = [x for x in receivers if not __is_dead(x)]
        newreceivers[signal] = alive

    __receivers = newreceivers

def __live_receivers(signal):
    with __lock:
        __purge()
        receivers = [funcref() for funcref in __receivers[signal]]

    return receivers

def __is_bound_method(method):
    if not(hasattr(method, "__func__") and hasattr(method, "__self__")):
        return False

    # Bound methods have a __self__ attribute pointing to the owner instance
    return six.get_method_self(method) is not None

def disconnect(signal, receiver):
    inputkey = __make_id(receiver)

    with __lock:
        __purge()
        receivers = __receivers.get(signal)

        for idx in six.moves.range(len(receivers)):
            connected = receivers[idx]()

            if inputkey != __make_id(connected):
                continue

            del receivers[idx]
            return True  # receiver successfully disconnected!

    return False

def emit(signal, *args, **kwargs):
    if signal not in __receivers:
        return

    receivers = __live_receivers(signal)

    for func in receivers:
        func(*args, **kwargs)

def arrayuniqify(X, retainorder=False):
    s = X.argsort()
    X = X[s]
    D = np.append([True],X[1:] != X[:-1])
    if retainorder:
        DD = np.append(D.nonzero()[0],len(X))
        ind = [min(s[x:DD[i+1]]) for (i,x) in enumerate(DD[:-1])]
        ind.sort()
        return ind
    else:
        return [D,s]

def equalspairs(X, Y):
    T = Y.copy()
    R = (T[1:] != T[:-1]).nonzero()[0]
    R = np.append(R,np.array([len(T)-1]))
    M = R[R.searchsorted(range(len(T)))]
    D = T.searchsorted(X)
    T = np.append(T,np.array([0]))
    M = np.append(M,np.array([0]))
    A = (T[D] == X) * D
    B = (T[D] == X) * (M[D] + 1)
    return [A,B]

def isin(X,Y):
    if len(Y) > 0:
        T = Y.copy()
        T.sort()
        D = T.searchsorted(X)
        T = np.append(T,np.array([0]))
        W = (T[D] == X)
        if isinstance(W,bool):
            return np.zeros((len(X),),bool)
        else:
            return (T[D] == X)
    else:
        return np.zeros((len(X),),bool)

def arraydifference(X,Y):
    if len(Y) > 0:
        Z = isin(X,Y)
        return X[np.invert(Z)]
    else:
        return X

def arraymax(X,Y):
    Z = np.zeros((len(X),), int)
    A = X <= Y
    B = Y < X
    Z[A] = Y[A]
    Z[B] = X[B]
    return Z

async def _seed2did(self) -> str:

        rv = None
        dids_with_meta = json.loads(await did.list_my_dids_with_meta(self.handle))  # list

        if dids_with_meta:
            for did_with_meta in dids_with_meta:  # dict
                if 'metadata' in did_with_meta:
                    try:
                        meta = json.loads(did_with_meta['metadata'])
                        if isinstance(meta, dict) and meta.get('seed', None) == self._seed:
                            rv = did_with_meta.get('did')
                    except json.decoder.JSONDecodeError:
                        continue  # it's not one of ours, carry on

        if not rv:  # seed not in metadata, generate did again on temp wallet
            temp_wallet = await Wallet(
                self._seed,
                '{}.seed2did'.format(self.name),
                None,
                {'auto-remove': True}).create()

            rv = temp_wallet.did
            await temp_wallet.remove()

        return rv

async def remove(self) -> None:

        LOGGER.debug('Wallet.remove >>>')

        try:
            LOGGER.info('Removing wallet: %s', self.name)
            await wallet.delete_wallet(json.dumps(self.cfg), json.dumps(self.access_creds))
        except IndyError as x_indy:
            LOGGER.info('Abstaining from wallet removal; indy-sdk error code %s', x_indy.error_code)

        LOGGER.debug('Wallet.remove <<<')

def loadSV(fname, shape=None, titles=None, aligned=False, byteorder=None,  
           renamer=None, **kwargs):
    [columns, metadata] = loadSVcols(fname, **kwargs)
    
    if 'names' in metadata.keys():
        names = metadata['names']
    else:
        names = None
 
    if 'formats' in metadata.keys():
        formats = metadata['formats']
    else:
        formats = None
    
    if 'dtype' in metadata.keys():
        dtype = metadata['dtype']
    else:
        dtype = None
 
    if renamer is not None:
        print 'Trying user-given renamer ...'
        renamed = renamer(names)
        if len(renamed) == len(uniqify(renamed)):
            names = renamed
                     metadata)'''
        else:
            print '... renamer failed to produce unique names, not using.'
            
    if names and len(names) != len(uniqify(names)):

        print 'Names are not unique, reverting to default naming scheme.'
        names = None


    return [utils.fromarrays(columns, type=np.ndarray, dtype=dtype, 
                             shape=shape, formats=formats, names=names, 
                             titles=titles, aligned=aligned, 
                             byteorder=byteorder), metadata]

def loadSVrecs(fname, uselines=None, skiprows=0, linefixer=None, 
               delimiter_regex=None, verbosity=DEFAULT_VERBOSITY, **metadata):
    if delimiter_regex and isinstance(delimiter_regex, types.StringType):
        import re
        delimiter_regex = re.compile(delimiter_regex) 
   
    [metadata, inferedlines, WHOLETHING] = getmetadata(fname, skiprows=skiprows,
                                                linefixer=linefixer, 
                                                delimiter_regex=delimiter_regex, 
                                                verbosity=verbosity, **metadata)

    if uselines is None:
        uselines = (0,False)
    
    if is_string_like(fname):
        fh = file(fname, 'rU')
    elif hasattr(fname, 'readline'):
        fh = fname
    else:
        raise ValueError('fname must be a string or file handle') 
 
    for _ind in range(skiprows+uselines[0] + metadata['headerlines']):
        fh.readline()
        
    if linefixer or delimiter_regex:
        fh2 = tempfile.TemporaryFile('w+b')
        F = fh.read().strip('\n').split('\n')
        if linefixer:
            F = map(linefixer,F)
        if delimiter_regex:
            F = map(lambda line: 
                    delimiter_regex.sub(metadata['dialect'].delimiter, line), F)       
        fh2.write('\n'.join(F))        
        fh2.seek(0)
        fh = fh2        

    reader = csv.reader(fh, dialect=metadata['dialect'])

    if uselines[1]:
        linelist = []
        for ln in reader:
            if reader.line_num <= uselines[1] - uselines[0]:
                linelist.append(ln)
            else:
                break
    else:
        linelist = list(reader)
      
    fh.close()

    if linelist[-1] == []:
        linelist.pop(-1)

    return [linelist,metadata]

def parsetypes(dtype):
    return [dtype[i].name.strip('1234567890').rstrip('ing') 
            for i in range(len(dtype))]

def thresholdcoloring(coloring, names):
    for key in coloring.keys():
        if len([k for k in coloring[key] if k in names]) == 0:
            coloring.pop(key)
        else:
            coloring[key] = utils.uniqify([k for k in coloring[key] if k in 
                                           names])
    return coloring

def makedir(dir_name):
    if os.path.exists(dir_name):
        delete(dir_name)
    os.mkdir(dir_name)

def pass_community(f):

    def decorator(f):
        @wraps(f)

        def inner(community, *args, **kwargs):
            permission = current_permission_factory(community, action=action)
            if not permission.can():
                abort(403)
            return f(community, *args, **kwargs)
        return inner
    return decorator

def format_item(item, template, name='item'):
    form = CommunityForm(formdata=request.values)

    ctx = mycommunities_ctx()
    ctx.update({
        'form': form,
        'is_new': True,
        'community': None,
    })

    if form.validate_on_submit():
        data = copy.deepcopy(form.data)

        community_id = data.pop('identifier')
        del data['logo']

        community = Community.create(
            community_id, current_user.get_id(), **data)

        file = request.files.get('logo', None)
        if file:
            if not community.save_logo(file.stream, file.filename):
                form.logo.errors.append(_(
                    'Cannot add this file as a logo. Supported formats: '
                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))
                db.session.rollback()
                community = None

        if community:
            db.session.commit()
            flash("Community was successfully created.", category='success')
            return redirect(url_for('.edit', community_id=community.id))

    return render_template(
        current_app.config['COMMUNITIES_NEW_TEMPLATE'],
        community_form=form,
        **ctx
    )

def edit(community):
    deleteform = DeleteCommunityForm(formdata=request.values)
    ctx = mycommunities_ctx()
    ctx.update({
        'deleteform': deleteform,
        'is_new': False,
        'community': community,
    })

    if deleteform.validate_on_submit():
        community.delete()
        db.session.commit()
        flash("Community was deleted.", category='success')
        return redirect(url_for('.index'))
    else:
        flash("Community could not be deleted.", category='warning')
        return redirect(url_for('.edit', community_id=community.id))

def ot_find_tree(arg_dict, exact=True, verbose=False, oti_wrapper=None):
    if oti_wrapper is None:
        from peyotl.sugar import oti
        oti_wrapper = oti
    return oti_wrapper.find_trees(arg_dict,
                                  exact=exact,
                                  verbose=verbose,
                                  wrap_response=True)

def is_iterable(etype) -> bool:
    to do all of the real work.
        Send schema to ledger, then retrieve it as written to the ledger and return it.
        If schema already exists on ledger, log error and return schema.

        :param schema_data_json: schema data json with name, version, attribute names; e.g.,

        ::

            {
                'name': 'my-schema',
                'version': '1.234',
                'attr_names': ['favourite_drink', 'height', 'last_visit_date']
            }

        :return: schema json as written to ledger (or existed a priori)
        using the mirroring strategy to cut down on locking of the working repo.

        `doc_id` is used to determine which shard should be pushed.
        if `doc_id` is None, all shards are pushed.
        and returns the filesystem path to each doc.
        Order is by shard, but arbitrary within shards.
        @TEMP not locked to prevent doc creation/deletion
        d = super(CommunityForm, self).data
        d.pop('csrf_token', None)
        return d

def validate_identifier(self, field):
    with codecs.open(filepath, 'r', encoding=encoding) as fo:
        return fo.read()

def download(url, encoding='utf-8'):
    b = StringIO()
    write_pretty_dict_str(b, d, indent=indent)
    return b.getvalue()

def write_pretty_dict_str(out, obj, indent=2):

    :param serializer: Serializer instance.
    :param mimetype: MIME type of response.
        if isinstance(data, Community):
            last_modified = data.updated
            response_data = schema_class(
                context=dict(item_links_factory=links_item_factory)
            ).dump(data).data
        else:
            last_modified = None
            response_data = schema_class(
                context=dict(
                    total=data.query.count(),
                    item_links_factory=links_item_factory,
                    page=page,
                    urlkwargs=urlkwargs,
                    pagination_links_factory=links_pagination_factory)
            ).dump(data.items, many=True).data

        response = current_app.response_class(
            json.dumps(response_data, **_format_args()),
            mimetype=mimetype)
        response.status_code = code

        if last_modified:
            response.last_modified = last_modified

        if headers is not None:
            response.headers.extend(headers)
        return response
    return view

def from_error(exc_info, json_encoder, debug_url=None):
        exc = exc_info[1]
        data = exc.__dict__.copy()
        for key, value in data.items():
            try:
                json_encoder.encode(value)
            except TypeError:
                data[key] = repr(value)
        data["traceback"] = "".join(traceback.format_exception(*exc_info))
        if debug_url is not None:
            data["debug_url"] = debug_url
        return InternalError(data)

def contains(self, index: Union[SchemaKey, int, str]) -> bool:

        LOGGER.debug('SchemaCache.contains >>> index: %s', index)

        rv = None
        if isinstance(index, SchemaKey):
            rv = (index in self._schema_key2schema)
        elif isinstance(index, int) or (isinstance(index, str) and ':2:' not in index):
            rv = (int(index) in self._seq_no2schema_key)
        elif isinstance(index, str):
            rv = (schema_key(index) in self._schema_key2schema)
        else:
            rv = False

        LOGGER.debug('SchemaCache.contains <<< %s', rv)
        return rv

def cull(self, delta: bool) -> None:

        LOGGER.debug('RevoCacheEntry.cull >>> delta: %s', delta)

        rr_frames = self.rr_delta_frames if delta else self.rr_state_frames
        mark = 4096**0.5  # max rev reg size = 4096; heuristic: hover max around sqrt(4096) = 64
        if len(rr_frames) > int(mark * 1.25):
            rr_frames.sort(key=lambda x: -x.qtime)  # order by descending query time
            del rr_frames[int(mark * 0.75):]  # retain most recent, grow again from here
            LOGGER.info(
                'Pruned revocation cache entry %s to %s %s frames',

                self.rev_reg_def['id'],
                len(rr_frames),
                'delta' if delta else 'state')

        LOGGER.debug('RevoCacheEntry.cull <<<')

def dflt_interval(self, cd_id: str) -> (int, int):

        LOGGER.debug('RevocationCache.dflt_interval >>>')

        fro = None
        to = None

        for rr_id in self:

            if cd_id != rev_reg_id2cred_def_id(rr_id):
                continue
            entry = self[rr_id]
            if entry.rr_delta_frames:
                to = max(entry.rr_delta_frames, key=lambda f: f.to).to
                fro = min(fro or to, to)

        if not (fro and to):
            LOGGER.debug(

                'RevocationCache.dflt_interval <!< No data for default non-revoc interval on cred def id %s',
                cd_id)

            raise CacheIndex('No data for default non-revoc interval on cred def id {}'.format(cd_id))

        rv = (fro, to)
        LOGGER.debug('RevocationCache.dflt_interval <<< %s', rv)
        return rv

def parse(base_dir: str, timestamp: int = None) -> int:

        LOGGER.debug('parse >>> base_dir: %s, timestamp: %s', base_dir, timestamp)

        if not isdir(base_dir):
            LOGGER.info('No cache archives available: not feeding cache')
            LOGGER.debug('parse <<< None')
            return None

        if not timestamp:
            timestamps = [int(t) for t in listdir(base_dir) if t.isdigit()]
            if timestamps:
                timestamp = max(timestamps)
            else:
                LOGGER.info('No cache archives available: not feeding cache')
                LOGGER.debug('parse <<< None')
                return None

        timestamp_dir = join(base_dir, str(timestamp))
        if not isdir(timestamp_dir):
            LOGGER.error('No such archived cache directory: %s', timestamp_dir)
            LOGGER.debug('parse <<< None')
            return None

        with SCHEMA_CACHE.lock:
            with open(join(timestamp_dir, 'schema'), 'r') as archive:
                schemata = json.loads(archive.read())
                SCHEMA_CACHE.feed(schemata)

        with CRED_DEF_CACHE.lock:

            with open(join(timestamp_dir, 'cred_def'), 'r') as archive:

                cred_defs = json.loads(archive.read())

                for cd_id in cred_defs:
                    if cd_id in CRED_DEF_CACHE:

                        LOGGER.warning('Cred def cache already has cred def on %s: skipping', cd_id)
                    else:

                        CRED_DEF_CACHE[cd_id] = cred_defs[cd_id]

                        LOGGER.info('Cred def cache imported cred def for cred def id %s', cd_id)

        with REVO_CACHE.lock:
            with open(join(timestamp_dir, 'revocation'), 'r') as archive:
                rr_cache_entries = json.loads(archive.read())
                for (rr_id, entry) in rr_cache_entries.items():
                    if rr_id in REVO_CACHE:
                        LOGGER.warning('Revocation cache already has entry on %s: skipping', rr_id)
                    else:

                        rr_cache_entry = RevoCacheEntry(entry['rev_reg_def'])

                        rr_cache_entry.rr_delta_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_delta_frames']
                        ]
                        rr_cache_entry.cull(True)

                        rr_cache_entry.rr_state_frames = [
                            RevRegUpdateFrame(
                                f['_to'],
                                f['_timestamp'],
                                f['_rr_update']) for f in entry['rr_state_frames']
                        ]
                        rr_cache_entry.cull(False)

                        REVO_CACHE[rr_id] = rr_cache_entry
                        LOGGER.info('Revocation cache imported entry for rev reg id %s', rr_id)

        LOGGER.debug('parse <<< %s', timestamp)
        return timestamp

def detect_nexson_version(blob):
    the value of for the key to list.

    This is used in the BadgerFish mapping convention.

    This is a simple multi-dict that is only suitable when you know that you'll never
    store a list or `None` as a value in the dict.
    element in under key `k` has the same value.
    import xml.dom.minidom
    s = [el.nodeName]
    att_container = el.attributes
    for i in range(att_container.length):
        attr = att_container.item(i)
        s.append('  @{a}="{v}"'.format(a=attr.name, v=attr.value))
    for c in el.childNodes:
        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:
            s.append('  {a} type="TEXT" data="{d}"'.format(a=c.nodeName, d=c.data))
        else:
            s.append('  {a} child'.format(a=c.nodeName))
    return '\n'.join(s)

def _convert_hbf_meta_val_for_xml(key, val):
    if isinstance(val, list):
        return [_convert_hbf_meta_val_for_xml(key, i) for i in val]
    is_literal = True
    content = None
    if isinstance(val, dict):
        ret = val
        if '@href' in val:
            is_literal = False
        else:
            content = val.get('$')
            if isinstance(content, dict) and _contains_hbf_meta_keys(val):
                is_literal = False
    else:
        ret = {}
        content = val
    if is_literal:

        ret.setdefault('@xsi:type', 'nex:LiteralMeta')

        ret.setdefault('@property', key)
        if content is not None:

            ret.setdefault('@datatype', _python_instance_to_nexml_meta_datatype(content))
        if ret is not val:
            ret['$'] = content
    else:

        ret.setdefault('@xsi:type', 'nex:ResourceMeta')

        ret.setdefault('@rel', key)
    return ret

def find_nested_meta_first(d, prop_name, version):
    Decode encoded credential attribute value.

    :param value: numeric string to decode
    :return: decoded value, stringified if original was neither str, bool, int, nor float

    :param method: The method to be called
    :type method: function
    :param parameters: The parameters to use in the call
    :type parameters: dict[str, object] | list[object]

    :param parameters: List of (name, value) pairs of the given parameters
    :type parameters: dict[str, object]
    :param parameter_types: Parameter type by name.
    :type parameter_types: dict[str, type]
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool

    :param parameter_names: The names of the parameters in the method declaration
    :type parameter_names: list[str]
    :param parameter_types: Parameter type by name
    :type parameter_types: dict[str, type]

    :param value: Value returned by the method
    :type value: object
    :param expected_type: Expected return type
    :type expected_type: type
    :param strict_floats: If False, treat integers as floats
    :type strict_floats: bool
        branch_name = git(self.gitdir, self.gitwd, "symbolic-ref", "HEAD")
        return branch_name.replace('refs/heads/', '').strip()

def branch_exists(self, branch):
        git(self.gitdir, "fetch", remote, _env=self.env())

def get_version_history_for_file(self, filepath):

        # define the desired fields for logout output, matching the order in these lists!
        GIT_COMMIT_FIELDS = ['id',
                             'author_name',
                             'author_email',
                             'date',
                             'date_ISO_8601',
                             'relative_date',
                             'message_subject',
                             'message_body']
        GIT_LOG_FORMAT = ['%H', '%an', '%ae', '%aD', '%ai', '%ar', '%s', '%b']
        # make the final format string, using standard ASCII field/record delimiters
        GIT_LOG_FORMAT = '%x1f'.join(GIT_LOG_FORMAT) + '%x1e'
        try:
            log = git(self.gitdir,
                      self.gitwd,
                      '--no-pager',
                      'log',
                      '--format=%s' % GIT_LOG_FORMAT,
                      '--follow',  # Track file's history when moved/renamed...
                      '--find-renames=100%',  # ... but only if the contents are identical!
                      '--',
                      filepath)
            # _LOG.debug('log said "{}"'.format(log))
            log = log.strip('\n\x1e').split("\x1e")
            log = [row.strip().split("\x1f") for row in log]
            log = [dict(zip(GIT_COMMIT_FIELDS, row)) for row in log]
        except:
            _LOG.exception('git log failed')
            raise
        return log

def _add_and_commit(self, doc_filepath, author, commit_msg):
        Remove a document on the given branch and attribute the commit to author.
        Returns the SHA of the commit on branch.

        Deprecated but needed until we merge api local-dep to master...

        Given a amendment_id, branch and optionally an
        author, remove an amendment on the given branch
        and attribute the commit to author.
        Returns the SHA of the commit on branch.

        :param community: Community object.
        :param record: Record API object.
        :param expires_at: Time after which the request expires and shouldn't
            be resolved anymore.
        return cls.query.filter_by(
            id_record=record_uuid, id_community=community_id
        ).one_or_none()

def filter_communities(cls, p, so, with_deleted=False):
        query = cls.query if with_deleted else \
            cls.query.filter(cls.deleted_at.is_(None))

        if p:
            p = p.replace(' ', '%')
            query = query.filter(db.or_(
                cls.id.ilike('%' + p + '%'),
                cls.title.ilike('%' + p + '%'),
                cls.description.ilike('%' + p + '%'),
            ))

        if so in current_app.config['COMMUNITIES_SORTING_OPTIONS']:
            order = so == 'title' and db.asc or db.desc
            query = query.order_by(order(getattr(cls, so)))
        else:
            query = query.order_by(db.desc(cls.ranking))
        return query

def add_record(self, record):
        key = current_app.config['COMMUNITIES_RECORD_KEY']

        record.setdefault(key, [])

        if self.has_record(record):
            current_app.logger.warning(
                'Community addition: record {uuid} is already in community '
                '"{comm}"'.format(uuid=record.id, comm=self.id))
        else:
            record[key].append(self.id)
            record[key] = sorted(record[key])
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if not self.oaiset.has_record(record):
                self.oaiset.add_record(record)

def remove_record(self, record):
        if not self.has_record(record):
            current_app.logger.warning(
                'Community removal: record {uuid} was not in community '
                '"{comm}"'.format(uuid=record.id, comm=self.id))
        else:
            key = current_app.config['COMMUNITIES_RECORD_KEY']
            record[key] = [c for c in record[key] if c != self.id]

        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            if self.oaiset.has_record(record):
                self.oaiset.remove_record(record)

def accept_record(self, record):
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()
            self.add_record(record)
            self.last_record_accepted = datetime.utcnow()

def reject_record(self, record):
        with db.session.begin_nested():
            req = InclusionRequest.get(self.id, record.id)
            if req is None:
                raise InclusionRequestMissingError(community=self,
                                                   record=record)
            req.delete()

def delete(self):
        if self.deleted_at is not None:
            raise CommunitiesError(community=self)
        else:
            self.deleted_at = datetime.utcnow()

def logo_url(self):
        if self.logo_ext:
            return '/api/files/{bucket}/{key}'.format(
                bucket=current_app.config['COMMUNITIES_BUCKET_UUID'],
                key='{0}/logo.{1}'.format(self.id, self.logo_ext),
            )
        return None

def oaiset(self):
        if current_app.config['COMMUNITIES_OAI_ENABLED']:
            from invenio_oaiserver.models import OAISet
            return OAISet.query.filter_by(spec=self.oaiset_spec).one()
        else:
            return None

def oaiset_url(self):
        return url_for(
            'invenio_oaiserver.response',
            verb='ListRecords',
            metadataPrefix='oai_dc', set=self.oaiset_spec, _external=True)

def version_id(self):
        return hashlib.sha1('{0}__{1}'.format(
            self.id, self.updated).encode('utf-8')).hexdigest()

def get_featured_or_none(cls, start_date=None):
        start_date = start_date or datetime.utcnow()

        comm = cls.query.filter(
            FeaturedCommunity.start_date <= start_date
        ).order_by(
            cls.start_date.desc()
        ).first()
        return comm if comm is None else comm.community

def getConnectorVersion(self):
		result = asyncResult()
		data = self._getURL("/",versioned=False)
		result.fill(data)
		if data.status_code == 200:
			result.error = False
		else:
			result.error = response_codes("get_mdc_version",data.status_code)
		result.is_done = True
		return result

def setHandler(self,handler,cbfn):
		'''
		if handler == "async-responses":
			self.async_responses_callback = cbfn
		elif handler == "registrations-expired":
			self.registrations_expired_callback = cbfn
		elif handler == "de-registrations":
			self.de_registrations_callback = cbfn
		elif handler == "reg-updates":
			self.reg_updates_callback = cbfn
		elif handler == "registrations":
			self.registrations_callback = cbfn
		elif handler == "notifications":
			self.notifications_callback = cbfn
		else:
			self.log.warn("'%s' is not a legitimate notification channel option. Please check your spelling.",handler)

def as_python(self, infile, include_original_shex: bool=False):
	Generate a dummy date list for testing without 
	hitting the server
	Convert the webserver formatted dates
	to an integer format by stripping the
	leading char and casting

        Note that if `prefix` is not provided, it will be `guid`, even if the
        `method` is `METHOD_INT`.
    List of gray-scale colors in HSV space as web hex triplets.

    For integer argument k, returns list of `k` gray-scale colors, increasingly 
    light, linearly in the HSV color space, as web hex triplets.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    **Parameters**

            **k** :  positive integer

                    Number of gray-scale colors to return.

    **Returns**

            **glist** :  list of strings

                    List of `k` gray-scale colors.

    Append one or more records to the end of a numpy recarray or ndarray .

    Can take a single record, void or tuple, or a list of records, voids or 
    tuples.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addrecords`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The array to add records to.

            **new** :  record, void or tuple, or list of them

                    Record(s) to add to `X`.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new records.

    **See also:**  :func:`tabular.spreadsheet.rowstack`

    Add one or more columns to a numpy ndarray.

    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.addcols`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype or recarray

                    The recarray to add columns to.

            **cols** :  numpy ndarray, or list of arrays of columns
            
                    Column(s) to add.

            **names**:  list of strings, optional

                    Names of the new columns. Only applicable when `cols` is a 
                    list of arrays.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy array made up of `X` plus the new columns.

    **See also:**  :func:`tabular.spreadsheet.colstack`

    Delete columns from a numpy ndarry or recarray.

    Can take a string giving a column name or comma-separated list of column 
    names, or a list of string column names.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.deletecols`.

    **Parameters**

            **X** :  numpy recarray or ndarray with structured dtype

                    The numpy array from which to delete columns.

            **cols** :  string or list of strings

                    Name or list of names of columns in `X`.  This can be
                    a string giving a column name or comma-separated list of 
                    column names, or a list of string column names.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    New numpy ndarray with structured dtype
                    given by `X`, excluding the columns named in `cols`.

    Rename column of a numpy ndarray with structured dtype, in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.renamecol`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    The numpy array for which a column is to be renamed.

            **old** :  string

                    Old column name, e.g. a name in `X.dtype.names`.

            **new** :  string

                    New column name to replace `old`.

    Replace value `old` with `new` everywhere it appears in-place.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.replace`.

    **Parameters**

            **X** :  numpy ndarray with structured dtype

                    Numpy array for which in-place replacement of `old` with 
                    `new` is to be done.

            **old** : string

            **new** : string

            **strict** :  boolean, optional

            *   If `strict` = `True`, replace only exact occurences of `old`.

            *   If `strict` = `False`, assume `old` and `new` are strings and   
                replace all occurences of substrings (e.g. like 
                :func:`str.replace`)

            **cols** :  list of strings, optional

                    Names of columns to make replacements in; if `None`, make 
                    replacements everywhere.

            **rows** : list of booleans or integers, optional

                    Rows to make replacements in; if `None`, make replacements 
                    everywhere.

    Note:  This function does in-place replacements.  Thus there are issues 
    handling data types here when replacement dtype is larger than original 
    dtype.  This can be resolved later by making a new array when necessary ...

    Horizontally stack a sequence of numpy ndarrays with structured dtypes

    Analog of numpy.hstack for recarrays.

    Implemented by the tabarray method 
    :func:`tabular.tab.tabarray.colstack` which uses 
    :func:`tabular.tabarray.tab_colstack`.

    **Parameters**

            **seq** :  sequence of numpy ndarray with structured dtype

                    List, tuple, etc. of numpy recarrays to stack vertically.

            **mode** :  string in ['first','drop','abort','rename']

                    Denotes how to proceed if when multiple recarrays share the 
                    same column name:

                    *   if `mode` == ``first``, take the column from the first
                        recarray in `seq` containing the shared column name.

                    *   elif `mode` == ``abort``, raise an error when the 
                        recarrays to stack share column names; this is the

                        default mode.

                    *   elif `mode` == ``drop``, drop any column that shares    
                        its name with any other column among the sequence of 
                        recarrays.

                    *   elif `mode` == ``rename``, for any set of all columns
                        sharing the same name, rename all columns by appending 
                        an underscore, '_', followed by an integer, starting 
                        with '0' and incrementing by 1 for each subsequent 
                        column.

    **Returns**

            **out** :  numpy ndarray with structured dtype

                    Result of horizontally stacking the arrays in `seq`.

    **See also:**  `numpy.hstack 
    <http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html>`_.

    Renames overlapping column names of numpy ndarrays with structured dtypes

    Rename the columns by using a simple convention:

    *   If `L` is a list, it will append the number in the list to the key 
        associated with the array.

    *   If `L` is a dictionary, the algorithm will append the string 
        representation of the key associated with an array to the overlapping 
        columns from that array.

    Default renamer function used by :func:`tabular.spreadsheet.join`

    **Parameters**

            **L** :  list or dictionary

                    Numpy recarrays with columns to be renamed.

    **Returns**

            **D** :  dictionary of dictionaries

                    Dictionary mapping each input numpy recarray to a 
                    dictionary mapping each original column name to its new 
                    name following the convention above.


    :param load_module: Module that contains the various types
    :param pairs: key/value tuples (In our case, they are str/str)
    :return:

    :param s: string representation of JSON document
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string

    :param fp: file-like object to deserialize
    :param load_module: module that contains declarations for types
    :param kwargs: arguments see: json.load for details
    :return: JSGObject representing the json string
    if is_union(A_tuple):
        return any(isinstance_(x, t) for t in A_tuple.__args__)
    elif getattr(A_tuple, '__origin__', None) is not None:
        return isinstance(x, A_tuple.__origin__)
    else:
        return isinstance(x, A_tuple)

def is_valid(obj: JSGValidateable, log: Optional[Union[TextIO, Logger]] = None) -> bool:
    return obj._is_valid(log)

def arg_tup_to_dict(argument_tuples):
    status_code = response.status_code

    if status_code not in A_OK_HTTP_CODES:
        error_explanation = A_ERROR_HTTP_CODES.get(status_code)
        raise_error = "{}: {}".format(status_code, error_explanation)
        raise Exception(raise_error)
    else:
        return True

async def open(self) -> '_BaseAgent':

        LOGGER.debug('_BaseAgent.open >>>')

        # Do not open pool independently: let relying party decide when to go on-line and off-line
        await self.wallet.open()

        LOGGER.debug('_BaseAgent.open <<<')
        return self

async def _get_rev_reg_def(self, rr_id: str) -> str:


        LOGGER.debug('_BaseAgent._get_rev_reg_def >>> rr_id: %s', rr_id)

        rv_json = json.dumps({})

        with REVO_CACHE.lock:
            revo_cache_entry = REVO_CACHE.get(rr_id, None)

            rr_def = revo_cache_entry.rev_reg_def if revo_cache_entry else None

            if rr_def:

                LOGGER.info('_BaseAgent._get_rev_reg_def: rev reg def for %s from cache', rr_id)

                rv_json = json.dumps(rr_def)
            else:

                get_rrd_req_json = await ledger.build_get_revoc_reg_def_request(self.did, rr_id)
                resp_json = await self._submit(get_rrd_req_json)
                try:

                    (_, rv_json) = await ledger.parse_get_revoc_reg_def_response(resp_json)

                    rr_def = json.loads(rv_json)
                except IndyError:  # ledger replied, but there is no such rev reg

                    LOGGER.debug('_BaseAgent._get_rev_reg_def: <!< no rev reg exists on %s', rr_id)
                    raise AbsentRevReg('No rev reg exists on {}'.format(rr_id))

                if revo_cache_entry is None:

                    REVO_CACHE[rr_id] = RevoCacheEntry(rr_def, None)
                else:

                    REVO_CACHE[rr_id].rev_reg_def = rr_def


        LOGGER.debug('_BaseAgent._get_rev_reg_def <<< %s', rv_json)
        return rv_json

async def get_cred_def(self, cd_id: str) -> str:


        LOGGER.debug('_BaseAgent.get_cred_def >>> cd_id: %s', cd_id)

        rv_json = json.dumps({})

        with CRED_DEF_CACHE.lock:
            if cd_id in CRED_DEF_CACHE:

                LOGGER.info('_BaseAgent.get_cred_def: got cred def for %s from cache', cd_id)
                rv_json = json.dumps(CRED_DEF_CACHE[cd_id])

                LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
                return rv_json


            req_json = await ledger.build_get_cred_def_request(self.did, cd_id)
            resp_json = await self._submit(req_json)
            resp = json.loads(resp_json)
            if not ('result' in resp and resp['result'].get('data', None)):

                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)

                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            try:

                (_, rv_json) = await ledger.parse_get_cred_def_response(resp_json)

            except IndyError:  # ledger replied, but there is no such cred def

                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)

                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))
            CRED_DEF_CACHE[cd_id] = json.loads(rv_json)

            LOGGER.info('_BaseAgent.get_cred_def: got cred def %s from ledger', cd_id)


        LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)
        return rv_json

def is_union(etype) -> bool:

    Args:
        entity: A mixbox.Entity object.
        *types: A variable-length list of TypedField subclasses. If not

            provided, defaults to TypedField.
    that match the input parameters.

    Args:
        field: A TypedField instance.
        params: A dictionary of TypedField instance attribute-to-value mappings.

    Returns:
        True if the input TypedField matches the input parameters.

    Args:
        klass: A class (usually an Entity subclass).

    Yields:
        (class attribute name, TypedField instance) tuples.
        if value is None:
            return None
        elif self.type_ is None:
            return value
        elif self.check_type(value):
            return value
        elif self.is_type_castable:  # noqa
            return self.type_(value)

        error_fmt = "%s must be a %s, not a %s"
        error = error_fmt % (self.name, self.type_, type(value))
        raise TypeError(error)

def remove_collection(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        if fourth_arg is None:
            collection_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_collection_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, collection_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = "Delete Collection '%s' via OpenTree API" % collection_id
        return self._remove_document(gh_user, collection_id, parent_sha, author, commit_msg)

async def load_cache(self, archive: bool = False) -> int:

        LOGGER.debug('Verifier.load_cache >>> archive: %s', archive)

        rv = int(time())
        for s_id in self.cfg.get('archive-on-close', {}).get('schema_id', {}):
            with SCHEMA_CACHE.lock:
                await self.get_schema(s_id)

        for cd_id in self.cfg.get('archive-on-close', {}).get('cred_def_id', {}):
            with CRED_DEF_CACHE.lock:

                await self.get_cred_def(cd_id)
        for rr_id in self.cfg.get('archive-on-close', {}).get('rev_reg_id', {}):

            await self._get_rev_reg_def(rr_id)
            with REVO_CACHE.lock:
                revo_cache_entry = REVO_CACHE.get(rr_id, None)
                if revo_cache_entry:
                    try:
                        await revo_cache_entry.get_state_json(self._build_rr_state_json, rv, rv)
                    except ClosedPool:
                        LOGGER.warning(
                            'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s',
                            self.wallet.name,
                            self.pool.name,
                            rr_id,
                            rv)

        if archive:
            Caches.archive(self.dir_cache)
        LOGGER.debug('Verifier.load_cache <<< %s', rv)
        return rv

def can(self):
    Take the union of a list of lists.

    Take a Python list of Python lists::

            [[l11,l12, ...], [l21,l22, ...], ... , [ln1, ln2, ...]]

    and return the aggregated list::

            [l11,l12, ..., l21, l22 , ...]

    For a list of two lists, e.g. `[a, b]`, this is like::

            a.extend(b)

    **Parameters**

            **ListOfLists** :  Python list

                    Python list of Python lists.

    **Returns**

            **u** :  Python list

                    Python list created by taking the union of the
                    lists in `ListOfLists`.

    Returns a null value for each of various kinds of test values.

    **Parameters**

            **test** :  bool, int, float or string

                    Value to test.


    **Returns**
            **null** :  element in `[False, 0, 0.0, '']`

                    Null value corresponding to the given test value:

                    *   if `test` is a `bool`, return `False`
                    *   else if `test` is an `int`, return `0`
                    *   else if `test` is a `float`, return `0.0`
                    *   else `test` is a `str`, return `''`

        if self._map_valuetype:
            return self.map_as_python(name)
        else:
            return self.obj_as_python(name)

def members_entries(self, all_are_optional: bool=False) -> List[Tuple[str, str]]:
        rval = []
        if self._members:
            for member in self._members:
                rval += member.members_entries(all_are_optional)
        elif self._choices:
            for choice in self._choices:
                rval += self._context.reference(choice).members_entries(True)
        else:
            return []
        return rval

def _get_filtered_study_ids(shard, include_aliases=False):

        Checks out master branch as a side effect!
        Returns the current numeric part of the next study ID, advances
        the counter to the next value, and stores that value in the
        file in case the server is restarted.

    :param l: list to be flattened
    :return:
    rval = OrderedDict()
    for e in l:
        if not isinstance(e, str) and isinstance(e, Iterable):
            for ev in flatten_unique(e):
                rval[ev] = None
        else:
            rval[e] = None
    return list(rval.keys())

def as_tokens(ctx: List[ParserRuleContext]) -> List[str]:
    return [as_token(e) for e in ctx]

def is_valid_python(tkn: str) -> bool:
    try:
        root = ast.parse(tkn)
    except SyntaxError:
        return False
    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)

def remove_study(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):
        if fourth_arg is None:
            study_id, branch_name, author = first_arg, sec_arg, third_arg
            gh_user = branch_name.split('_study_')[0]
            parent_sha = self.get_master_sha()
        else:
            gh_user, study_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg
        if commit_msg is None:
            commit_msg = "Delete Study #%s via OpenTree API" % study_id
        return self._remove_document(gh_user, study_id, parent_sha, author, commit_msg)

def init():
    # Create the bucket
    c = Community.get(community_id)
    if not c:
        click.secho('Community {0} does not exist.'.format(community_id),
                    fg='red')
        return
    ext = save_and_validate_logo(logo, logo.name, c.id)
    c.logo_ext = ext
    db.session.commit()

def request(community_id, record_id, accept):
    c = Community.get(community_id)
    assert c is not None
    c.remove_record(record_id)
    db.session.commit()
    RecordIndexer().index_by_id(record_id)

def gen_otu_dict(nex_obj, nexson_version=None):
    if nexson_version is None:
        nexson_version = detect_nexson_version(nex_obj)
    if _is_by_id_hbf(nexson_version):
        otus = nex_obj['nexml']['otusById']
        if len(otus) > 1:
            d = {}
            for v in otus.values():
                d.update(v['otuById'])
            return d
        else:
            return otus.values()[0]['otuById']
    o_dict = {}
    for ob in nex_obj.get('otus', []):
        for o in ob.get('otu', []):
            oid = o['@id']
            o_dict[oid] = o
    return o_dict

def set_country(request):
    if request.method == 'POST':
        next = request.POST.get('next', request.GET.get('next'))
        if is_safe_url(url=next, host=request.get_host()):
            response = http.HttpResponseRedirect(next)
        else:
            response = http.HttpResponse()

        country_code = request.POST.get('country', '').upper()
        if country_code != geo.get_supported_country(country_code):
            return http.HttpResponseBadRequest()

        if hasattr(request, 'session'):
            request.session[geo.COUNTRY_SESSION_KEY] = country_code
        else:
            response.set_cookie(geo.COUNTRY_COOKIE_NAME,
                               country_code,
                               max_age=geo.COUNTRY_COOKIE_AGE,
                               path=geo.COUNTRY_COOKIE_PATH)
        return response
    else:
        return http.HttpResponseNotAllowed(['POST'])

def reference(self, tkn: str):

        :param tkn: 
        :return:

        :param tkn:
        :return:
        return as_set([[d for d in self.dependencies(k) if d not in self.grammarelts]
                       for k in self.grammarelts.keys()])

def new_request(sender, request=None, notify=True, **kwargs):
    if index and not index.startswith(
            current_app.config['COMMUNITIES_INDEX_PREFIX']):
        return

    json['provisional_communities'] = list(sorted([
        r.id_community for r in InclusionRequest.get_by_record(record.id)
    ]))

def find_nodes(self, query_dict=None, exact=False, verbose=False, **kwargs):
        if self.use_v1:
            uri = '{p}/singlePropertySearchForTrees'.format(p=self.query_prefix)
        else:
            uri = '{p}/find_trees'.format(p=self.query_prefix)
        resp = self._do_query(uri,
                              query_dict=query_dict,
                              exact=exact,
                              verbose=verbose,
                              valid_keys=self.tree_search_term_set,
                              kwargs=kwargs)
        if wrap_response:
            return TreeRefList(resp)
        return resp

def find_studies(self, query_dict=None, exact=False, verbose=False, **kwargs):
    '''
    Converts HTML5 character references within text_string to their corresponding unicode characters
    and returns converted string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Coverts Latin character references within text_string to their corresponding unicode characters
    and returns converted string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a string or NoneType not be passed as an argument
    '''
    Splits string and converts words not found within a pre-built dictionary to their
    most likely actual word based on a relative probability dictionary. Returns edited
    string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a string or NoneType not be passed as an argument
    '''
    Splits text_string into a list of sentences based on NLTK's english.pickle tokenizer, and
    returns said list as type list of str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Extracts keywords from text_string using NLTK's list of English stopwords, ignoring words of a
    length smaller than 3, and returns the new string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
        Returns base from of text_string using NLTK's WordNetLemmatizer as type str.

        Keyword argument:

        - text_string: string instance

        Exceptions raised:

        - InputError: occurs should a non-string argument be passed
    '''
    Converts text_string into lowercase and returns the converted string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Given each function within function_list, applies the order of functions put forward onto
    text_string, returning the processed string as type str.

    Keyword argument:

    - function_list: list of functions available in preprocessing.text
    - text_string: string instance

    Exceptions raised:
    
    - FunctionError: occurs should an invalid function be passed within the list of functions
    - InputError: occurs should text_string be non-string, or function_list be non-list
    '''
    Removes any escape character within text_string and returns the new string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Removes any digit value discovered within text_string and returns the new string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Removes any integer represented as a word within text_string and returns the new string as
    type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Removes all URLs within text_string and returns the new string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a non-string argument be passed
    '''
    Removes all whitespace found within text_string and returns new string as type str.

    Keyword argument:

    - text_string: string instance

    Exceptions raised:

    - InputError: occurs should a string or NoneType not be passed as an argument
    description = '''Takes a series of at least 2 OTT ids and reports the OTT of their least inclusive taxonomic ancestor and that taxon's ancestors.'''
    parser = argparse.ArgumentParser(prog='ot-taxo-mrca-to-root', description=description)
    parser.add_argument('ids', nargs='+', type=int, help='OTT IDs')
    args = parser.parse_args(argv)
    id_list = args.ids
    last_id = id_list.pop()
    anc_list = get_taxonomic_ancestor_ids(last_id)
    common_anc = set(anc_list)
    for curr_id in id_list:
        curr_anc_set = set(get_taxonomic_ancestor_ids(curr_id))
        common_anc &= curr_anc_set
        if not common_anc:
            break
    for anc_id in anc_list:
        if anc_id in common_anc:
            out.write('{}\n'.format(anc_id))

def is_sequence(value):
    return (hasattr(value, "__iter__") and not
            isinstance(value, (six.string_types, six.binary_type)))

def import_class(classpath):
    modname, classname = classpath.rsplit(".", 1)
    module = importlib.import_module(modname)
    klass  = getattr(module, classname)
    return klass

def resolve_class(classref):
    if classref is None:
        return None
    elif isinstance(classref, six.class_types):
        return classref
    elif isinstance(classref, six.string_types):
        return import_class(classref)
    else:
        raise ValueError("Unable to resolve class for '%s'" % classref)

def needkwargs(*argnames):
    required = set(argnames)


    def decorator(func):

        def inner(*args, **kwargs):
            missing = required - set(kwargs)
            if missing:
                err = "%s kwargs are missing." % list(missing)
                raise ValueError(err)
            return func(*args, **kwargs)
        return inner
    return decorator

def get(host="localhost", port=3551, timeout=30):
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(timeout)
    sock.connect((host, port))
    sock.send(CMD_STATUS)
    buffr = ""
    while not buffr.endswith(EOF):
        buffr += sock.recv(BUFFER_SIZE).decode()
    sock.close()
    return buffr

def strip_units_from_lines(lines):
    for line in lines:
        for unit in ALL_UNITS:
            if line.endswith(" %s" % unit):
                line = line[:-1-len(unit)]
        yield line

def print_status(raw_status, strip_units=False):
    lines = split(raw_status)
    if strip_units:
        lines = strip_units_from_lines(lines)
    for line in lines:
        print(line)

def get_cached_parent_for_taxon(self, child_taxon):
        if self._ott_id2taxon is None:
            resp = child_taxon._taxonomic_lineage[0]
            tl = child_taxon._taxonomic_lineage[1:]
            assert 'taxonomic_lineage' not in resp
            resp['taxonomic_lineage'] = tl
            return TaxonWrapper(taxonomy=child_taxon.taxonomy,
                                taxomachine_wrapper=self._wr,
                                prop_dict=resp)  # TODO recursive (indirectly)
        else:
            anc = []
            prev = None
            for resp in reversed(child_taxon._taxonomic_lineage):
                ott_id = resp['ot:ottId']
                curr = self._ott_id2taxon.get(ott_id)
                if curr is None:
                    assert 'taxonomic_lineage' not in resp
                    assert 'parent' not in resp
                    resp['parent'] = prev
                    resp['taxonomic_lineage'] = anc
                    curr = TaxonWrapper(taxonomy=child_taxon.taxonomy,
                                        taxomachine_wrapper=self._wr,
                                        prop_dict=resp)
                elif curr._parent is None and prev is not None:
                    curr._parent = prev
                prev = curr
                anc.insert(0, curr)
            return prev

def update_empty_fields(self, **kwargs):

    ebs = defaultdict(dict)
    for edge in ebt.values():
        source_id = edge['@source']
        edge_id = edge['@id']
        ebs[source_id][edge_id] = edge
    assert ebs == tree['edgeBySourceId']

def _create_edge_by_target(self):
        ebt = {}
        for edge_dict in self._edge_by_source.values():
            for edge_id, edge in edge_dict.items():
                target_id = edge['@target']
                edge['@id'] = edge_id
                assert target_id not in ebt
                ebt[target_id] = edge
        # _check_rev_dict(self._tree, ebt)
        return ebt

def prune_to_ingroup(self):
        # circumvent the node with nd_id
        to_child_edge_id = to_child_edge['@id']
        par = to_par_edge['@source']
        self._edge_by_source[par][to_child_edge_id] = to_child_edge
        to_child_edge['@source'] = par
        # make it a tip...
        del self._edge_by_source[nd_id]
        # delete it
        self._del_tip(nd_id)

def describe(self):
        return {
            "name": self.name,
            "params": self.params,
            "returns": self.returns,
            "description": self.description,
        }

def params(self):
        return [{"name": p_name, "type": p_type.__name__}
                for (p_name, p_type) in self.signature.parameter_types]

def returns(self):
        return_type = self.signature.return_type
        none_type = type(None)
        if return_type is not None and return_type is not none_type:
            return return_type.__name__

def create(parameter_names, parameter_types, return_type):
        ordered_pairs = [(name, parameter_types[name]) for name in parameter_names]
        return MethodSignature(ordered_pairs, return_type)

def _hbf_handle_child_elements(self, obj, ntl):
        # accumulate a list of the children names in ko, and
        #   the a dictionary of tag to xml elements.
        # repetition of a tag means that it will map to a list of
        #   xml elements
        cd = {}
        ko = []
        ks = set()
        for child in ntl:
            k = child.nodeName
            if k == 'meta' and (not self._badgerfish_style_conversion):
                matk, matv = self._transform_meta_key_value(child)
                if matk is not None:
                    _add_value_to_dict_bf(obj, matk, matv)
            else:
                if k not in ks:
                    ko.append(k)
                    ks.add(k)
                _add_value_to_dict_bf(cd, k, child)

        # Converts the child XML elements to dicts by recursion and
        #   adds these to the dict.
        for k in ko:
            v = _index_list_of_values(cd, k)
            dcl = []
            ct = None
            for xc in v:
                ct, dc = self._gen_hbf_el(xc)
                dcl.append(dc)
            # this assertion will trip is the hacky stripping of namespaces
            #   results in a name clash among the tags of the children
            assert ct not in obj
            obj[ct] = dcl

        # delete redundant about attributes that are used in XML, but not JSON (last rule of HoneyBadgerFish)
        _cull_redundant_about(obj)
        return obj

def get_xml_parser(encoding=None):

    Args:
        doc: The input XML document. Can be an instance of
            ``lxml.etree._Element``, ``lxml.etree._ElementTree``, a file-like
            object, or a string filename.
        encoding: The character encoding of `doc`. If ``None``, an attempt
            will be made to determine the character encoding by the XML
            parser.

    Returns:
        An ``lxml.etree._Element`` instance for `doc`.

    Raises:
        IOError: If `doc` cannot be found.
        lxml.ParseError: If `doc` is a malformed XML document.


    Note:
        If the function contains escaped XML characters outside of a
        CDATA block, they will be unescaped.

    Args:
        A string containing one or more CDATA blocks.

    Returns:
        An XML unescaped string with CDATA block qualifiers removed.

        inner list.

        Args:
            value: An object about to be inserted.

        Subclasses can override this function.
        reference types

        :param all_are_optional: If true, all types are forced optional
        :return: raw name/ signature type for all elements in this pair

        :param raw_name: name unadjusted for python compatibility.
        :param cooked_name: name that may or may not be python compatible

        :param prefix: owner of the element - used when objects passed as arguments

        :return: Initialization statements
        Raise AbsentLinkSecret if link secret is not set.

        :param action: action requiring link secret
        Return list of revocation registry identifiers for which HolderProver has tails files.

        :return: list of revocation registry identifiers for which HolderProver has tails files
        Create credential request as HolderProver and store in wallet; return credential json and metadata json.

        Raise AbsentLinkSecret if link secret not set.

        :param cred_offer_json: credential offer json

        :param cd_id: credential definition identifier
        :return: cred request json and corresponding metadata json as created and stored in wallet
        Load caches and archive enough to go offline and be able to generate proof
        on all credentials in wallet.

        Return timestamp (epoch seconds) of cache load event, also used as subdirectory
        for cache archives.

        :return: cache load event timestamp (epoch seconds)
        Get credentials from HolderProver wallet corresponding to proof request and
        filter criteria; return credential identifiers from wallet and credentials json.
        Return empty set and empty production for no such credentials.

        :param proof_req_json: proof request json as Verifier creates; has entries for proof request's
            nonce, name, and version; plus credential's requested attributes, requested predicates. I.e.,

        ::

            {
                'nonce': string,  # indy-sdk makes no semantic specification on this value
                'name': string,  # indy-sdk makes no semantic specification on this value
                'version': numeric-string,  # indy-sdk makes no semantic specification on this value
                'requested_attributes': {
                    '<attr_uuid>': {  # aka attr_referent, a proof-request local identifier
                        'name': string,  # attribute name (matches case- and space-insensitively)
                        'restrictions' [  # optional
                            {
                                "schema_id": string,  # optional
                                "schema_issuer_did": string,  # optional
                                "schema_name": string,  # optional
                                "schema_version": string,  # optional
                                "issuer_did": string,  # optional

                                "cred_def_id": string  # optional
                            },
                            {
                                ...  # if more than one restriction given, combined disjunctively (i.e., via OR)
                            }
                        ],
                        'non_revoked': {  # optional - indy-sdk ignores when getting creds from wallet
                            'from': int,  # optional, epoch seconds
                            'to': int  # optional, epoch seconds
                        }
                    },
                    ...
                },
                'requested_predicates': {
                    '<pred_uuid>': {  # aka predicate_referent, a proof-request local predicate identifier
                        'name': string,  # attribute name (matches case- and space-insensitively)
                        'p_type': '>=',
                        'p_value': int,  # predicate value
                        'restrictions': [  # optional
                            {
                                "schema_id": string,  # optional
                                "schema_issuer_did": string,  # optional
                                "schema_name": string,  # optional
                                "schema_version": string,  # optional
                                "issuer_did": string,  # optional

                                "cred_def_id": string  # optional
                            },
                            {
                                ...  # if more than one restriction given, combined disjunctively (i.e., via OR)
                            }
                        ],
                        'non_revoked': {  # optional - indy-sdk ignores when getting creds from wallet
                            'from': int,  # optional, epoch seconds
                            'to': int  # optional, epoch seconds
                        }
                    },
                    ...
                },
                'non_revoked': {  # optional - indy-sdk ignores when getting creds from wallet
                    'from': Optional<int>,
                    'to': Optional<int>
                }
            }

        :param filt: filter for matching attribute-value pairs and predicates; dict mapping each

            cred def id to dict (specify empty dict or none for no filter, matching all)
            mapping attributes to values to match or compare. E.g.,

        ::

            {
                'Vx4E82R17q...:3:CL:16:0': {
                    'attr-match': {
                        'name': 'Alex',
                        'sex': 'M',
                        'favouriteDrink': None
                    },
                    'minima': {  # if both attr-match and minima present, combined conjunctively (i.e., via AND)
                        'favouriteNumber' : 10,
                        'score': '100'  # nicety: implementation converts to int for caller
                    },
                },
                'R17v42T4pk...:3:CL:19:0': {
                    'attr-match': {
                        'height': 175,
                        'birthdate': '1975-11-15'  # combined conjunctively (i.e., via AND)
                    }
                },
                'Z9ccax812j...:3:CL:27:0': {

                    'attr-match': {}  # match all attributes on this cred def
                }
                ...
            }

        :param filt_dflt_incl: whether to include (True) all credentials from wallet that filter does not

            identify by cred def, or to exclude (False) all such credentials
        :return: tuple with (set of referents, creds json for input proof request);
            empty set and empty production for no such credential
        Get creds structure from HolderProver wallet by credential identifiers.

        :param proof_req_json: proof request as per get_creds() above
        :param cred_ids: set of credential identifiers of interest
        :return: json with cred(s) for input credential identifier(s)

    :param data: The data to histogram
    :type data: list[object]
    :return: The histogram
    :rtype: dict[object, int]

    :param data: The dict to print
    :type data: dict
    :rtype: None
    Find all files in a subdirectory and return paths relative to dir

    This is similar to (and uses) setuptools.findall
    However, the paths returned are in the form needed for package_data
    For a list of packages, find the package_data

    This function scans the subdirectories of a package and considers all
    non-submodule subdirectories as resources, including them in
    the package_data

    Returns a dictionary suitable for setup(package_data=<result>)
    file_metrics = OrderedDict()

    # TODO make available the includes and excludes feature
    gitignore = []
    if os.path.isfile('.gitignore'):
        with open('.gitignore', 'r') as ifile:
            gitignore = ifile.read().splitlines()

    in_files = glob_files(context['root_dir'], context['in_file_names'], gitignore=gitignore)
    # main loop
    for in_file, key in in_files:
        # print 'file %i: %s' % (i, in_file)
        try:
            with open(in_file, 'rb') as ifile:
                code = ifile.read()
            # lookup lexicographical scanner to use for this run
            try:
                lex = guess_lexer_for_filename(in_file, code, encoding='guess')
                # encoding is 'guess', chardet', 'utf-8'
            except:
                pass
            else:
                token_list = lex.get_tokens(code)  # parse code

                file_metrics[key] = OrderedDict()
                file_metrics[key].update(compute_file_metrics(file_processors, lex.name, key, token_list))
                file_metrics[key]['language'] = lex.name

        except IOError as e:
            sys.stderr.writelines(str(e) + " -- Skipping input file.\n\n")

    return file_metrics

def process_build_metrics(context, build_processors):
    # display aggregated metric values on language level

    def display_header(processors, before='', after=''):
        print(before, end=' ')
        for processor in processors:
            processor.display_separator()
        print(after)


    def display_metrics(processors, before='', after='', metrics=[]):
    of the same type at the same bus aggregated.
        if hasattr(self, tag_name):
            getattr(self, tag_name)(*args, **kwargs)

def der(self, x: Sym):
        assert std > 0
        ng = self.sym.sym('ng_{:d}'.format(len(self.scope['ng'])))
        self.scope['ng'].append(ng)
        return mean + std*ng

def noise_uniform(self, lower_bound, upper_bound):
        if self.verbose:
            print('   ' * self.depth, *args, **kwargs)

def get_case6ww():
    path = os.path.dirname(pylon.__file__)
    path = os.path.join(path, "test", "data")
    path = os.path.join(path, "case6ww", "case6ww.pkl")

    case = pylon.Case.load(path)
    case.generators[0].p_cost = (0.0, 4.0, 200.0)
    case.generators[1].p_cost = (0.0, 3.0, 200.0)

#    case.generators[0].p_cost = (0.0, 5.1, 200.0) # 10%
#    case.generators[1].p_cost = (0.0, 4.5, 200.0) # 30%

    case.generators[2].p_cost = (0.0, 6.0, 200.0) # passive

#    case.generators[0].c_shutdown = 100.0
#    case.generators[1].c_shutdown = 100.0
#    case.generators[2].c_shutdown = 100.0

    case.generators[0].p_min = 0.0 # TODO: Unit-decommitment.
    case.generators[1].p_min = 0.0
    case.generators[2].p_min = 0.0

    case.generators[0].p_max = 110.0
    case.generators[1].p_max = 110.0
    case.generators[2].p_max = 220.0 # passive

    # FIXME: Correct generator naming order.
    for g in case.generators:
        g.name

    #pyreto.util.plotGenCost(case.generators)

    return case

def get_case24_ieee_rts():
    path = os.path.dirname(pylon.__file__)
    path = os.path.join(path, "test", "data")
    path = os.path.join(path, "case24_ieee_rts", "case24_ieee_rts.pkl")

    case = pylon.Case.load(path)

    # FIXME: Correct generator naming order.
    for g in case.generators:
        g.name

    return case

def get_discrete_task_agent(generators, market, nStates, nOffer, markups,
        withholds, maxSteps, learner, Pd0=None, Pd_min=0.0):
    env = pyreto.discrete.MarketEnvironment(generators, market,
                                            numStates=nStates,
                                            numOffbids=nOffer,
                                            markups=markups,
                                            withholds=withholds,
                                            Pd0=Pd0,
                                            Pd_min=Pd_min)
    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)

    nActions = len(env._allActions)
    module = ActionValueTable(numStates=nStates, numActions=nActions)

    agent = LearningAgent(module, learner)

    return task, agent

def get_zero_task_agent(generators, market, nOffer, maxSteps):
    env = pyreto.discrete.MarketEnvironment(generators, market, nOffer)
    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)
    agent = pyreto.util.ZeroAgent(env.outdim, env.indim)
    return task, agent

def get_neg_one_task_agent(generators, market, nOffer, maxSteps):
    env = pyreto.discrete.MarketEnvironment(generators, market, nOffer)
    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)
    agent = pyreto.util.NegOneAgent(env.outdim, env.indim)
    return task, agent

def run_experiment(experiment, roleouts, episodes, in_cloud=False,
                   dynProfile=None):

    def run():
        if dynProfile is None:
            maxsteps = len(experiment.profile) # episode length
        else:
            maxsteps = dynProfile.shape[1]
        na = len(experiment.agents)
        ni = roleouts * episodes * maxsteps

        all_action = zeros((na, 0))
        all_reward = zeros((na, 0))
        epsilon = zeros((na, ni)) # exploration rate

        # Converts to action vector in percentage markup values.
        vmarkup = vectorize(get_markup)

        for roleout in range(roleouts):
            if dynProfile is not None:
                # Apply new load profile before each roleout (week).
                i = roleout * episodes # index of first profile value
                experiment.profile = dynProfile[i:i + episodes, :]

#            print "PROFILE:", experiment.profile, episodes

            experiment.doEpisodes(episodes) # number of samples per learning step

            nei = episodes * maxsteps # num interactions per role
            epi_action = zeros((0, nei))
            epi_reward = zeros((0, nei))

            for i, (task, agent) in \
            enumerate(zip(experiment.tasks, experiment.agents)):
                action = copy(agent.history["action"])
                reward = copy(agent.history["reward"])

                for j in range(nei):
                    if isinstance(agent.learner, DirectSearchLearner):
                        action[j, :] = task.denormalize(action[j, :])
                        k = nei * roleout
                        epsilon[i, k:k + nei] = agent.learner.explorer.sigma[0]
                    elif isinstance(agent.learner, ValueBasedLearner):
                        action[j, :] = vmarkup(action[j, :], task)
                        k = nei * roleout
                        epsilon[i, k:k + nei] = agent.learner.explorer.epsilon
                    else:
                        action = vmarkup(action, task)

                # FIXME: Only stores action[0] for all interactions.
                epi_action = c_[epi_action.T, action[:, 0].flatten()].T
                epi_reward = c_[epi_reward.T, reward.flatten()].T

                if hasattr(agent, "module"):
                    print "PARAMS:", agent.module.params

                agent.learn()
                agent.reset()

            all_action = c_[all_action, epi_action]
            all_reward = c_[all_reward, epi_reward]

        return all_action, all_reward, epsilon

    if in_cloud:
        import cloud
        job_id = cloud.call(run, _high_cpu=False)
        result = cloud.result(job_id)
        all_action, all_reward, epsilon = result
    else:
        all_action, all_reward, epsilon = run()

    return all_action, all_reward, epsilon

def get_full_year():
    weekly = get_weekly()
    daily = get_daily()
    hourly_winter_wkdy, hourly_winter_wknd = get_winter_hourly()
    hourly_summer_wkdy, hourly_summer_wknd = get_summer_hourly()
    hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd = \
        get_spring_autumn_hourly()

    fullyear = zeros(364 * 24)
    c = 0
    l = [(0, 7, hourly_winter_wkdy, hourly_winter_wknd),
         (8, 16, hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd),
         (17, 29, hourly_summer_wkdy, hourly_summer_wknd),
         (30, 42, hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd),
         (43, 51, hourly_winter_wkdy, hourly_winter_wknd)]

    for start, end, wkdy, wknd in l:
        for w in weekly[start:end + 1]:
            for d in daily[:5]:
                for h in wkdy:
                    fullyear[c] = w * (d / 100.0) * (h / 100.0)
                    c += 1
            for d in daily[5:]:
                for h in wknd:
                    fullyear[c] = w * (d / 100.0) * (h / 100.0)
                    c += 1
    return fullyear

def get_all_days():
    weekly = get_weekly()
    daily = get_daily()

    return [w * (d / 100.0) for w in weekly for d in daily]

def get_q_experiment(case, minor=1):
    gen = case.generators

    profile = array([1.0])
    maxSteps = len(profile)

    if minor == 1:
        alpha = 0.3 # Learning rate.
        gamma = 0.99 # Discount factor
        # The closer epsilon gets to 0, the more greedy and less explorative.
        epsilon = 0.9
        decay = 0.97

        tau = 150.0 # Boltzmann temperature.
        qlambda = 0.9
    elif minor == 2:
        alpha = 0.1 # Learning rate.
        gamma = 0.99 # Discount factor
        # The closer epsilon gets to 0, the more greedy and less explorative.
        epsilon = 0.9
        decay = 0.99

        tau = 150.0 # Boltzmann temperature.
        qlambda = 0.9
    else:
        raise ValueError

    market = pyreto.SmartMarket(case, priceCap=cap, decommit=decommit,
                                auctionType=auctionType)

    experiment = pyreto.continuous.MarketExperiment([], [], market, profile)

    for g in gen[0:2]:
        learner = Q(alpha, gamma)
    #    learner = QLambda(alpha, gamma, qlambda)
    #    learner = SARSA(alpha, gamma)

        learner.explorer.epsilon = epsilon
        learner.explorer.decay = decay
#        learner.explorer = BoltzmannExplorer(tau, decay)

        task, agent = get_discrete_task_agent([g], market, nStates, nOffer,
            markups, withholds, maxSteps, learner)

        experiment.tasks.append(task)
        experiment.agents.append(agent)

    # Passive agent.
    task, agent = get_zero_task_agent(gen[2:3], market, nOffer, maxSteps)
    experiment.tasks.append(task)
    experiment.agents.append(agent)

    return experiment

def q_limited(self):
        if (self.q >= self.q_max) or (self.q <= self.q_min):
            return True
        else:
            return False

def total_cost(self, p=None, p_cost=None, pcost_model=None):
        p = self.p if p is None else p
        p_cost = self.p_cost if p_cost is None else p_cost
        pcost_model = self.pcost_model if pcost_model is None else pcost_model

        p = 0.0 if not self.online else p

        if pcost_model == PW_LINEAR:
            n_segments = len(p_cost) - 1
            # Iterate over the piece-wise linear segments.
            for i in range(n_segments):
                x1, y1 = p_cost[i]
                x2, y2 = p_cost[i + 1]
                m = (y2 - y1) / (x2 - x1)
                c = y1 - m * x1
                if x1 <= p <= x2:
                    result = m*p + c
                    break
            else:
#                print "TOTC:", self.name, p, self.p_max, p_cost

#                raise ValueError, "Value [%f] outwith pwl cost curve." % p

                # Use the last segment for values outwith the cost curve.
                logger.error("Value [%f] outside pwl cost curve [%s]." %
                             (p, p_cost[-1][0]))
                result = m*p + c
        elif pcost_model == POLYNOMIAL:
#            result = p_cost[-1]
#            for i in range(1, len(p_cost)):
#                result += p_cost[-(i + 1)] * p**i
            result = polyval(p_cost, p)
        else:
            raise ValueError

        if self.is_load:
            return -result
        else:
            return result

def poly_to_pwl(self, n_points=4):
        assert self.pcost_model == POLYNOMIAL
        p_min = self.p_min
        p_max = self.p_max
        p_cost = []

        if p_min > 0.0:
            # Make the first segment go from the origin to p_min.
            step = (p_max - p_min) / (n_points - 2)

            y0 = self.total_cost(0.0)
            p_cost.append((0.0, y0))

            x = p_min
            n_points -= 1
        else:
            step = (p_max - p_min) / (n_points - 1)
            x = 0.0

        for _ in range(n_points):
            y = self.total_cost(x)
            p_cost.append((x, y))
            x += step

        # Change the cost model and set the new cost.
        self.pcost_model = PW_LINEAR
        self.p_cost = p_cost

def get_offers(self, n_points=6):
        from pyreto.smart_market import Offer

        qtyprc = self._get_qtyprc(n_points)
        return [Offer(self, qty, prc) for qty, prc in qtyprc]

def get_bids(self, n_points=6):
        from pyreto.smart_market import Bid

        qtyprc = self._get_qtyprc(n_points)
        return [Bid(self, qty, prc) for qty, prc in qtyprc]

def offers_to_pwl(self, offers):
        assert not self.is_load
        # Only apply offers associated with this generator.
        g_offers = [offer for offer in offers if offer.generator == self]
        # Fliter out zero quantity offers.
        gt_zero = [offr for offr in g_offers if round(offr.quantity, 4) > 0.0]
        # Ignore withheld offers.
        valid = [offer for offer in gt_zero if not offer.withheld]

        p_offers = [v for v in valid if not v.reactive]
        q_offers = [v for v in valid if v.reactive]

        if p_offers:
            self.p_cost = self._offbids_to_points(p_offers)
            self.pcost_model = PW_LINEAR
            self.online = True
        else:
            self.p_cost = [(0.0, 0.0), (self.p_max, 0.0)]
            self.pcost_model = PW_LINEAR
            if q_offers:
                # Dispatch at zero real power without shutting down
                # if capacity offered for reactive power.
                self.p_min = 0.0
                self.p_max = 0.0
                self.online = True
            else:
                self.online = False

        if q_offers:
            self.q_cost = self._offbids_to_points(q_offers)
            self.qcost_model = PW_LINEAR
        else:
            self.q_cost = None#[(0.0, 0.0), (self.q_max, 0.0)]
            self.qcost_model = PW_LINEAR

        if not len(p_offers) and not len(q_offers):
            logger.info("No valid offers for generator [%s], shutting down." %
                        self.name)
            self.online = False

        self._adjust_limits()

def bids_to_pwl(self, bids):
        assert self.is_load
        # Apply only those bids associated with this dispatchable load.
        vl_bids = [bid for bid in bids if bid.vLoad == self]
        # Filter out zero quantity bids.
        gt_zero = [bid for bid in vl_bids if round(bid.quantity, 4) > 0.0]
        # Ignore withheld offers.
        valid_bids = [bid for bid in gt_zero if not bid.withheld]

        p_bids = [v for v in valid_bids if not v.reactive]
        q_bids = [v for v in valid_bids if v.reactive]

        if p_bids:
            self.p_cost = self._offbids_to_points(p_bids, True)
            self.pcost_model = PW_LINEAR
            self.online = True
        else:
            self.p_cost = [(0.0, 0.0), (self.p_max, 0.0)]
            self.pcost_model = PW_LINEAR
            logger.info("No valid active power bids for dispatchable load "
                        "[%s], shutting down." % self.name)
            self.online = False

        if q_bids:
            self.q_cost = self._offbids_to_points(q_bids, True)
            self.qcost_model = PW_LINEAR
            self.online = True
        else:
            self.q_cost = [(self.q_min, 0.0), (0.0, 0.0), (self.q_max, 0.0)]
            self.qcost_model = PW_LINEAR
#            logger.info("No valid bids for dispatchable load, shutting down.")
#            self.online = False

        self._adjust_limits()

def _adjust_limits(self):
        if not self.is_load:
#            self.p_min = min([point[0] for point in self.p_cost])
            self.p_max = max([point[0] for point in self.p_cost])
        else:
            p_min = min([point[0] for point in self.p_cost])
            self.p_max = 0.0
            self.q_min = self.q_min * p_min / self.p_min
            self.q_max = self.q_max * p_min / self.p_min
            self.p_min = p_min

def indim(self):
        indim = self.numOffbids * len(self.generators)

        if self.maxWithhold is not None:
            return indim * 2
        else:
            return indim

def _getBusVoltageLambdaSensor(self):
        muVmin = array([b.mu_vmin for b in self.market.case.connected_buses])
        muVmax = array([b.mu_vmax for b in self.market.case.connected_buses])
        muVmin = -1.0 * muVmin
        diff = muVmin + muVmax
        return diff

def DoxyfileParse(file_contents):
   data = {}

   import shlex
   lex = shlex.shlex(instream = file_contents, posix = True)
   lex.wordchars += "*+./-:"
   lex.whitespace = lex.whitespace.replace("\n", "")
   lex.escape = ""

   lineno = lex.lineno
   token = lex.get_token()
   key = token   # the first token should be a key
   last_token = ""
   key_token = False
   next_key = False
   new_data = True


   def append_data(data, key, new_data, token):
      if new_data or len(data[key]) == 0:
         data[key].append(token)
      else:
         data[key][-1] += token

   while token:
      if token in ['\n']:
         if last_token not in ['\\']:
            key_token = True
      elif token in ['\\']:
         pass
      elif key_token:
         key = token
         key_token = False
      else:
         if token == "+=":
            if not data.has_key(key):
               data[key] = list()
         elif token == "=":
            if key == "TAGFILES" and data.has_key(key):
               append_data( data, key, False, "=" )
               new_data=False
            else:
               data[key] = list()
         else:
            append_data( data, key, new_data, token )
            new_data = True

      last_token = token
      token = lex.get_token()

      if last_token == '\\' and token != '\n':
         new_data = False
         append_data( data, key, new_data, '\\' )

   # compress lists of len 1 into single strings
   for (k, v) in data.items():
      if len(v) == 0:
         data.pop(k)

      # items in the following list will be kept as lists and not converted to strings
      if k in ["INPUT", "FILE_PATTERNS", "EXCLUDE_PATTERNS", "TAGFILES"]:
         continue

      if len(v) == 1:
         data[k] = v[0]

   return data

def DoxySourceScan(node, env, path):

   default_file_patterns = [
      '*.c', '*.cc', '*.cxx', '*.cpp', '*.c++', '*.java', '*.ii', '*.ixx',
      '*.ipp', '*.i++', '*.inl', '*.h', '*.hh ', '*.hxx', '*.hpp', '*.h++',
      '*.idl', '*.odl', '*.cs', '*.php', '*.php3', '*.inc', '*.m', '*.mm',
      '*.py',
   ]


   default_exclude_patterns = [
      '*~',
   ]

   sources = []

   data = DoxyfileParse(node.get_contents())

   if data.get("RECURSIVE", "NO") == "YES":
      recursive = True
   else:
      recursive = False


   file_patterns = data.get("FILE_PATTERNS", default_file_patterns)

   exclude_patterns = data.get("EXCLUDE_PATTERNS", default_exclude_patterns)

   # We're running in the top-level directory, but the doxygen
   # configuration file is in the same directory as node; this means
   # that relative pathnames in node must be adjusted before they can
   # go onto the sources list
   conf_dir = os.path.dirname(str(node))
   
   for node in data.get("INPUT", []):
      if not os.path.isabs(node):
         node = os.path.join(conf_dir, node)
      if os.path.isfile(node):
         sources.append(node)
      elif os.path.isdir(node):
         if recursive:
            for root, dirs, files in os.walk(node):
               for f in files:
                  filename = os.path.join(root, f)

                  pattern_check = reduce(lambda x, y: x or bool(fnmatch(filename, y)), file_patterns, False)
                  exclude_check = reduce(lambda x, y: x and fnmatch(filename, y), exclude_patterns, True)

                  if pattern_check and not exclude_check:
                     sources.append(filename)
         else:
            for pattern in file_patterns:
               sources.extend(glob.glob("/".join([node, pattern])))

   # Add tagfiles to the list of source files:
   for node in data.get("TAGFILES", []):
      file = node.split("=")[0]
      if not os.path.isabs(file):
         file = os.path.join(conf_dir, file)
      sources.append(file)
   
   # Add additional files to the list of source files:

   def append_additional_source(option):
      file = data.get(option, "")
      if file != "":
         if not os.path.isabs(file):
            file = os.path.join(conf_dir, file)
         if os.path.isfile(file):
            sources.append(file)

   append_additional_source("HTML_STYLESHEET")
   append_additional_source("HTML_HEADER")
   append_additional_source("HTML_FOOTER")

   sources = map( lambda path: env.File(path), sources )
   return sources

def DoxyEmitter(source, target, env):
   Add builders and construction variables for the
   Doxygen tool.  This is currently for Doxygen 1.4.6.
        self._positions = []
        self._line = 1
        self._curr = None  # current scope we are analyzing
        self._scope = 0
        self.language = None

def add_scope(self, scope_type, scope_name, scope_start, is_method=False):
        if tok[0] == Token.Text:
            count = tok[1].count('\n')
            if count:
                self._line += count  # adjust linecount

        if self._detector.process(tok):
            pass  # works been completed in the detector
        elif tok[0] == Token.Punctuation:
            if tok[0] == Token.Punctuation and tok[1] == '{':
                self._scope += 1
            if tok[0] == Token.Punctuation and tok[1] == '}':
                self._scope += -1
                if self._scope == 0 and self._curr is not None:
                    self._curr['end'] = self._line  # close last scope
                    self._curr = None
        elif tok[0] == Token.Name.Class and self._scope == 0:
            self.add_scope('Class', tok[1], self._line)
        elif tok[0] == Token.Name.Function and self._scope in [0, 1]:
            self.add_scope('Function', tok[1], self._line, self._scope == 1)

def _unpack_model(self, om):
        buses = om.case.connected_buses
        branches = om.case.online_branches
        gens = om.case.online_generators

        cp = om.get_cost_params()

#        Bf = om._Bf
#        Pfinj = om._Pfinj

        return buses, branches, gens, cp

def _dimension_data(self, buses, branches, generators):
        ipol = [i for i, g in enumerate(generators)
                if g.pcost_model == POLYNOMIAL]
        ipwl = [i for i, g in enumerate(generators)
                if g.pcost_model == PW_LINEAR]
        nb = len(buses)
        nl = len(branches)
        # Number of general cost vars, w.
        nw = self.om.cost_N
        # Number of piece-wise linear costs.
        if "y" in [v.name for v in self.om.vars]:
            ny = self.om.get_var_N("y")
        else:
            ny = 0
        # Total number of control variables of all types.
        nxyz = self.om.var_N

        return ipol, ipwl, nb, nl, nw, ny, nxyz

def _linear_constraints(self, om):
        A, l, u = om.linear_constraints() # l <= A*x <= u

        # Indexes for equality, greater than (unbounded above), less than
        # (unbounded below) and doubly-bounded box constraints.
#        ieq = flatnonzero( abs(u - l) <= EPS )
#        igt = flatnonzero( (u >=  1e10) & (l > -1e10) )
#        ilt = flatnonzero( (l <= -1e10) & (u <  1e10) )
#        ibx = flatnonzero( (abs(u - l) > EPS) & (u < 1e10) & (l > -1e10) )

        # Zero-sized sparse matrices not supported.  Assume equality
        # constraints exist.
##        AA = A[ieq, :]
##        if len(ilt) > 0:
##            AA = vstack([AA, A[ilt, :]], "csr")
##        if len(igt) > 0:
##            AA = vstack([AA, -A[igt, :]], "csr")
##        if len(ibx) > 0:
##            AA = vstack([AA, A[ibx, :], -A[ibx, :]], "csr")
#
#        if len(ieq) or len(igt) or len(ilt) or len(ibx):
#            sig_idx = [(1, ieq), (1, ilt), (-1, igt), (1, ibx), (-1, ibx)]
#            AA = vstack([sig * A[idx, :] for sig, idx in sig_idx if len(idx)])
#        else:
#            AA = None
#
#        bb = r_[u[ieq, :], u[ilt], -l[igt], u[ibx], -l[ibx]]
#
#        self._nieq = ieq.shape[0]
#
#        return AA, bb

        return A, l, u

def _var_bounds(self):
        x0 = array([])
        xmin = array([])
        xmax = array([])

        for var in self.om.vars:
            x0 = r_[x0, var.v0]
            xmin = r_[xmin, var.vl]
            xmax = r_[xmax, var.vu]

        return x0, xmin, xmax

def _initial_interior_point(self, buses, generators, xmin, xmax, ny):
        Va = self.om.get_var("Va")
        va_refs = [b.v_angle * pi / 180.0 for b in buses
                   if b.type == REFERENCE]
        x0 = (xmin + xmax) / 2.0

        x0[Va.i1:Va.iN + 1] = va_refs[0] # Angles set to first reference angle.

        if ny > 0:
            yvar = self.om.get_var("y")

            # Largest y-value in CCV data
            c = []
            for g in generators:
                if g.pcost_model == PW_LINEAR:
                    for _, y in g.p_cost:
                        c.append(y)


            x0[yvar.i1:yvar.iN + 1] = max(c) * 1.1

        return x0

def solve(self):
        base_mva = self.om.case.base_mva
        Bf = self.om._Bf
        Pfinj = self.om._Pfinj
        # Unpack the OPF model.
        bs, ln, gn, cp = self._unpack_model(self.om)
        # Compute problem dimensions.
        ipol, ipwl, nb, nl, nw, ny, nxyz = self._dimension_data(bs, ln, gn)
        # Split the constraints in equality and inequality.
        AA, ll, uu = self._linear_constraints(self.om)
        # Piece-wise linear components of the objective function.
        Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl = self._pwl_costs(ny, nxyz, ipwl)
        # Quadratic components of the objective function.
        Npol, Hpol, Cpol, fparm_pol, polycf, npol = \
            self._quadratic_costs(gn, ipol, nxyz, base_mva)
        # Combine pwl, poly and user costs.
        NN, HHw, CCw, ffparm = \
            self._combine_costs(Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl,
                                Npol, Hpol, Cpol, fparm_pol, npol, nw)
        # Transform quadratic coefficients for w into coefficients for X.
        HH, CC, C0 = self._transform_coefficients(NN, HHw, CCw, ffparm, polycf,
                                                  any_pwl, npol, nw)
        # Bounds on the optimisation variables.
        _, xmin, xmax = self._var_bounds()

        # Select an interior initial point for interior point solver.
        x0 = self._initial_interior_point(bs, gn, xmin, xmax, ny)

        # Call the quadratic/linear solver.
        s = self._run_opf(HH, CC, AA, ll, uu, xmin, xmax, x0, self.opt)

        # Compute the objective function value.
        Va, Pg = self._update_solution_data(s, HH, CC, C0)

        # Set case result attributes.
        self._update_case(bs, ln, gn, base_mva, Bf, Pfinj, Va, Pg, s["lmbda"])

        return s

def _pwl_costs(self, ny, nxyz, ipwl):
        any_pwl = int(ny > 0)
        if any_pwl:
            y = self.om.get_var("y")
            # Sum of y vars.
            Npwl = csr_matrix((ones(ny), (zeros(ny), array(ipwl) + y.i1)))
            Hpwl = csr_matrix((1, 1))
            Cpwl = array([1])
            fparm_pwl = array([[1., 0., 0., 1.]])
        else:
            Npwl = None#zeros((0, nxyz))
            Hpwl = None#array([])
            Cpwl = array([])
            fparm_pwl = zeros((0, 4))

        return Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl

def _quadratic_costs(self, generators, ipol, nxyz, base_mva):
        npol = len(ipol)
        rnpol = range(npol)
        gpol = [g for g in generators if g.pcost_model == POLYNOMIAL]

        if [g for g in gpol if len(g.p_cost) > 3]:
            logger.error("Order of polynomial cost greater than quadratic.")

        iqdr = [i for i, g in enumerate(generators)
                if g.pcost_model == POLYNOMIAL and len(g.p_cost) == 3]
        ilin = [i for i, g in enumerate(generators)
                if g.pcost_model == POLYNOMIAL and len(g.p_cost) == 2]

        polycf = zeros((npol, 3))
        if npol > 0:
            if len(iqdr) > 0:
                polycf[iqdr, :] = array([list(g.p_cost)
                                         for g in generators])#[iqdr, :].T
            if len(ilin) > 0:
                polycf[ilin, 1:] = array([list(g.p_cost[:2])
                                          for g in generators])#[ilin, :].T
            # Convert to per-unit.
            polycf = polycf * array([base_mva**2, base_mva, 1])
            Pg = self.om.get_var("Pg")
            Npol = csr_matrix((ones(npol), (rnpol, Pg.i1 + array(ipol))),
                              (npol, nxyz))
            Hpol = csr_matrix((2 * polycf[:, 0], (rnpol, rnpol)), (npol, npol))
            Cpol = polycf[:, 1]
            fparm_pol = (ones(npol) * array([[1], [0], [0], [1]])).T
        else:
            Npol = Hpol = None
            Cpol = array([])
            fparm_pol = zeros((0, 4))

        return Npol, Hpol, Cpol, fparm_pol, polycf, npol

def _combine_costs(self, Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl,
                       Npol, Hpol, Cpol, fparm_pol, npol, nw):
        NN = vstack([n for n in [Npwl, Npol] if n is not None], "csr")

        if (Hpwl is not None) and (Hpol is not None):
            Hpwl = hstack([Hpwl, csr_matrix((any_pwl, npol))])
            Hpol = hstack([csr_matrix((npol, any_pwl)), Hpol])
#        if H is not None:
#            H = hstack([csr_matrix((nw, any_pwl+npol)), H])

        HHw = vstack([h for h in [Hpwl, Hpol] if h is not None], "csr")

        CCw = r_[Cpwl, Cpol]

        ffparm = r_[fparm_pwl, fparm_pol]

        return NN, HHw, CCw, ffparm

def _transform_coefficients(self, NN, HHw, CCw, ffparm, polycf,
                               any_pwl, npol, nw):
        nnw = any_pwl + npol + nw
        M = csr_matrix((ffparm[:, 3], (range(nnw), range(nnw))))
        MR = M * ffparm[:, 2] # FIXME: Possibly column 1.
        HMR = HHw * MR
        MN = M * NN
        HH = MN.T * HHw * MN
        CC = MN.T * (CCw - HMR)
        # Constant term of cost.
        C0 = 1./2. * MR.T * HMR + sum(polycf[:, 2])

        return HH, CC, C0[0]

def _ref_bus_angle_constraint(self, buses, Va, xmin, xmax):
        refs = [bus._i for bus in buses if bus.type == REFERENCE]
        Varefs = array([b.v_angle for b in buses if b.type == REFERENCE])

        xmin[Va.i1 - 1 + refs] = Varefs
        xmax[Va.iN - 1 + refs] = Varefs

        return xmin, xmax

def _f(self, x, user_data=None):
        p_gen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.
        q_gen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.

        # Polynomial cost of P and Q.
        xx = r_[p_gen, q_gen] * self._base_mva
        if len(self._ipol) > 0:
            f = sum([g.total_cost(xx[i]) for i,g in enumerate(self._gn)])
        else:
            f = 0

        # Piecewise linear cost of P and Q.
        if self._ny:
            y = self.om.get_var("y")
            self._ccost = csr_matrix((ones(self._ny),
                (range(y.i1, y.iN + 1), zeros(self._ny))),
                shape=(self._nxyz, 1)).T
            f = f + self._ccost * x
        else:
            self._ccost = zeros((1, self._nxyz))
        # TODO: Generalised cost term.

        return f

def _df(self, x, user_data=None):
        p_gen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.
        q_gen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.

        # Polynomial cost of P and Q.
        xx = r_[p_gen, q_gen] * self._base_mva

        iPg = range(self._Pg.i1, self._Pg.iN + 1)
        iQg = range(self._Qg.i1, self._Qg.iN + 1)

        # Polynomial cost of P and Q.
        df_dPgQg = zeros((2 * self._ng, 1))        # w.r.t p.u. Pg and Qg
#            df_dPgQg[ipol] = matrix([g.poly_cost(xx[i], 1) for g in gpol])
#            for i, g in enumerate(gn):
#                der = polyder(list(g.p_cost))
#                df_dPgQg[i] = polyval(der, xx[i]) * base_mva
        for i in self._ipol:
            p_cost = list(self._gn[i].p_cost)
            df_dPgQg[i] = \
                self._base_mva * polyval(polyder(p_cost), xx[i])

        df = zeros((self._nxyz, 1))
        df[iPg] = df_dPgQg[:self._ng]
        df[iQg] = df_dPgQg[self._ng:self._ng + self._ng]

        # Piecewise linear cost of P and Q.
        df = df + self._ccost.T
        # TODO: Generalised cost term.

        return asarray(df).flatten()

def _d2f(self, x):
        d2f_dPg2 = lil_matrix((self._ng, 1)) # w.r.t p.u. Pg
        d2f_dQg2 = lil_matrix((self._ng, 1)) # w.r.t p.u. Qg]

        for i in self._ipol:
            p_cost = list(self._gn[i].p_cost)
            d2f_dPg2[i, 0] = polyval(polyder(p_cost, 2),
                self._Pg.v0[i] * self._base_mva) * self._base_mva**2
#            for i in ipol:
#                d2f_dQg2[i] = polyval(polyder(list(gn[i].p_cost), 2),
#                                      Qg.v0[i] * base_mva) * base_mva**2

        i = r_[range(self._Pg.i1, self._Pg.iN + 1),
               range(self._Qg.i1, self._Qg.iN + 1)]

        d2f = csr_matrix((vstack([d2f_dPg2, d2f_dQg2]).toarray().flatten(),
                          (i, i)), shape=(self._nxyz, self._nxyz))
        return d2f

def _gh(self, x):
        Pgen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.
        Qgen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.

        for i, gen in enumerate(self._gn):
            gen.p = Pgen[i] * self._base_mva # active generation in MW
            gen.q = Qgen[i] * self._base_mva # reactive generation in MVAr

        # Rebuild the net complex bus power injection vector in p.u.
        Sbus = self.om.case.getSbus(self._bs)

        Vang = x[self._Va.i1:self._Va.iN + 1]
        Vmag = x[self._Vm.i1:self._Vm.iN + 1]
        V = Vmag * exp(1j * Vang)

        # Evaluate the power flow equations.
        mis = V * conj(self._Ybus * V) - Sbus

        # Equality constraints (power flow).
        g = r_[mis.real,  # active power mismatch for all buses
               mis.imag]  # reactive power mismatch for all buses

        # Inequality constraints (branch flow limits).
        # (line constraint is actually on square of limit)
        flow_max = array([(l.rate_a / self._base_mva)**2 for l in self._ln])
        # FIXME: There must be a more elegant method for this.
        for i, v in enumerate(flow_max):
            if v == 0.0:
                flow_max[i] = Inf

        if self.flow_lim == IFLOW:
            If = self._Yf * V
            It = self._Yt * V
            # Branch current limits.
            h = r_[(If * conj(If)) - flow_max,
                   (It * conj(It)) - flow_max]
        else:
            i_fbus = [e.from_bus._i for e in self._ln]
            i_tbus = [e.to_bus._i for e in self._ln]
            # Complex power injected at "from" bus (p.u.).
            Sf = V[i_fbus] * conj(self._Yf * V)
            # Complex power injected at "to" bus (p.u.).
            St = V[i_tbus] * conj(self._Yt * V)
            if self.flow_lim == PFLOW: # active power limit, P (Pan Wei)
                # Branch real power limits.
                h = r_[Sf.real()**2 - flow_max,
                       St.real()**2 - flow_max]
            elif self.flow_lim == SFLOW: # apparent power limit, |S|
                # Branch apparent power limits.
                h = r_[(Sf * conj(Sf)) - flow_max,
                       (St * conj(St)) - flow_max].real
            else:
                raise ValueError

        return h, g

def _costfcn(self, x):
        f = self._f(x)
        df = self._df(x)
        d2f = self._d2f(x)

        return f, df, d2f

def _consfcn(self, x):
        h, g = self._gh(x)
        dh, dg = self._dgh(x)

        return h, g, dh, dg

def read(self, file_or_filename):
        if isinstance(file_or_filename, basestring):
            fname = os.path.basename(file_or_filename)
            logger.info("Unpickling case file [%s]." % fname)

            file = None
            try:
                file = open(file_or_filename, "rb")
            except:
                logger.error("Error opening %s." % fname)
                return None
            finally:
                if file is not None:
                    case = pickle.load(file)
                    file.close()
        else:
            file = file_or_filename
            case = pickle.load(file)

        return case

def write(self, file_or_filename):
        if isinstance(file_or_filename, basestring):
            fname = os.path.basename(file_or_filename)
            logger.info("Pickling case [%s]." % fname)

            file = None
            try:
                file = open(file_or_filename, "wb")
            except:
                logger.error("Error opening '%s'." % (fname))
                return False
            finally:
                if file is not None:
                    pickle.dump(self.case, file)
                    file.close()
        else:
            file = file_or_filename
            pickle.dump(file, self.case)

        return True

def process_token(self, tok):
        if(self.sloc == 0):
            if(self.comments == 0):
                ratio_comment_to_code = 0.00
            else:
                ratio_comment_to_code = 1.00
        else:
            ratio_comment_to_code = float(self.comments) / self.sloc
        metrics = OrderedDict([('sloc', self.sloc), ('comments', self.comments),
                               ('ratio_comment_to_code', round(ratio_comment_to_code, 2))])
        return metrics

def performAction(self, action):
        gs = [g for g in self.case.online_generators if g.bus.type !=REFERENCE]

        assert len(action) == len(gs)

        logger.info("Action: %s" % list(action))

        # Set the output of each (non-reference) generator.
        for i, g in enumerate(gs):
            g.p = action[i]

        # Compute power flows and slack generator set-point.
        NewtonPF(self.case, verbose=False).solve()
        #FastDecoupledPF(self.case, verbose=False).solve()

        # Store all generator set-points (only used for plotting).
        self._Pg[:, self._step] = [g.p for g in self.case.online_generators]

        # Apply the next load profile value to the original demand at each bus.
        if self._step != len(self.profile) - 1:
            pq_buses = [b for b in self.case.buses if b.type == PQ]
            for i, b in enumerate(pq_buses):
                b.p_demand = self._Pd0[i] * self.profile[self._step + 1]

        self._step += 1

        logger.info("Entering step %d." % self._step)

def reset(self):
        logger.info("Reseting environment.")

        self._step = 0

        # Reset the set-point of each generator to its original value.
        gs = [g for g in self.case.online_generators if g.bus.type !=REFERENCE]
        for i, g in enumerate(gs):
            g.p = self._Pg0[i]

        # Apply load profile to the original demand at each bus.
        for i, b in enumerate([b for b in self.case.buses if b.type == PQ]):
            b.p_demand = self._Pd0[i] * self.profile[self._step]

        # Initialise the record of generator set-points.
        self._Pg = zeros((len(self.case.online_generators), len(self.profile)))

        # Apply the first load profile value.
#        self.step()

        self.case.reset()

def isFinished(self):
        finished = (self.env._step == len(self.env.profile))
        if finished:
            logger.info("Finished episode.")
        return finished

def _oneInteraction(self):
        if self.doOptimization:
            raise Exception('When using a black-box learning algorithm, only full episodes can be done.')
        else:
            self.stepid += 1
            self.agent.integrateObservation(self.task.getObservation())
            self.task.performAction(self.agent.getAction())

            # Save the cumulative sum of set-points for each period.
            for i, g in enumerate(self.task.env.case.online_generators):
                self.Pg[i, self.stepid - 1] = self.Pg[i, self.stepid - 1] + g.p

            reward = self.task.getReward()
            self.agent.giveReward(reward)
            return reward

def doEpisodes(self, number=1):
        env = self.task.env
        self.Pg = zeros((len(env.case.online_generators), len(env.profile)))

        rewards = super(OPFExperiment, self).doEpisodes(number)

        # Average the set-points for each period.
        self.Pg = self.Pg / number

        return rewards

def getMethodByName(obj, name):

    try:#to get a method by asking the service
        obj = obj._getMethodByName(name)
    except:
        #assumed a childObject is ment 
        #split the name from objName.childObjName... -> [objName, childObjName, ...]
        #and get all objects up to the last in list with name checking from the service object
        names = name.split(".")
        for name in names:
            if nameAllowed(name):
                obj = getattr(obj, name)
            else:
                raise MethodNameNotAllowed()
        
    return obj

def waitForResponse(self, timeOut=None):
        (respEvt, id) = self.newResponseEvent()
        self.sendMessage({"id":id, "method":name, "params": args})
        return respEvt

def sendResponse(self, id, result, error):
           When the reponse arrives it will be removed from the list. 
        id=resp["id"]
        evt = self.respEvents[id]
        del(self.respEvents[id])
        evt.handleResponse(resp)

def handleRequest(self, req):
        name = req["method"]
        params = req["params"]
        try: #to get a callable obj 
            obj = getMethodByName(self.service, name)
            rslt = obj(*params)
        except:
            pass

def read(self, file_or_filename):
        self.file_or_filename = file_or_filename

        logger.info("Parsing PSAT case file [%s]." % file_or_filename)

        t0 = time.time()

        self.case = Case()

        # Name the case
        if isinstance(file_or_filename, basestring):
            name, _ = splitext(basename(file_or_filename))
        else:
            name, _ = splitext(file_or_filename.name)

        self.case.name = name

        bus_array = self._get_bus_array_construct()
        line_array = self._get_line_array_construct()
        # TODO: Lines.con - Alternative line data format
        slack_array = self._get_slack_array_construct()
        pv_array = self._get_pv_array_construct()
        pq_array = self._get_pq_array_construct()
        demand_array = self._get_demand_array_construct()
        supply_array = self._get_supply_array_construct()
        # TODO: Varname.bus (Bus names)

        # Pyparsing case:
        case = \
            ZeroOrMore(matlab_comment) + bus_array + \
            ZeroOrMore(matlab_comment) + line_array + \
            ZeroOrMore(matlab_comment) + slack_array + \
            ZeroOrMore(matlab_comment) + pv_array + \
            ZeroOrMore(matlab_comment) + pq_array + \
            ZeroOrMore(matlab_comment) + demand_array + \
            ZeroOrMore(matlab_comment) + supply_array

        case.parseFile(file_or_filename)

        elapsed = time.time() - t0
        logger.info("PSAT case file parsed in %.3fs." % elapsed)

        return self.case

def _get_bus_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        v_base = real.setResultsName("v_base") # kV
        v_magnitude = Optional(real).setResultsName("v_magnitude")
        v_angle = Optional(real).setResultsName("v_angle") # radians
        area = Optional(integer).setResultsName("area") # not used yet
        region = Optional(integer).setResultsName("region") # not used yet

        bus_data = bus_no + v_base + v_magnitude + v_angle + \
            area + region + scolon

        bus_data.setParseAction(self.push_bus)

        bus_array = Literal("Bus.con") + "=" + "[" + "..." + \
            ZeroOrMore(bus_data + Optional("]" + scolon))

        # Sort buses according to their name (bus_no)
        bus_array.setParseAction(self.sort_buses)

        return bus_array

def _get_line_array_construct(self):
        from_bus = integer.setResultsName("fbus")
        to_bus = integer.setResultsName("tbus")
        s_rating = real.setResultsName("s_rating") # MVA
        v_rating = real.setResultsName("v_rating") # kV
        f_rating = real.setResultsName("f_rating") # Hz
        length = real.setResultsName("length") # km (Line only)
        v_ratio = real.setResultsName("v_ratio") # kV/kV (Transformer only)
        r = real.setResultsName("r") # p.u. or Ohms/km
        x = real.setResultsName("x") # p.u. or Henrys/km
        b = real.setResultsName("b") # p.u. or Farads/km (Line only)
        tap_ratio = real.setResultsName("tap") # p.u./p.u. (Transformer only)
        phase_shift = real.setResultsName("shift") # degrees (Transformer only)
        i_limit = Optional(real).setResultsName("i_limit") # p.u.
        p_limit = Optional(real).setResultsName("p_limit") # p.u.
        s_limit = Optional(real).setResultsName("s_limit") # p.u.
        status = Optional(boolean).setResultsName("status")

        line_data = from_bus + to_bus + s_rating + v_rating + \
            f_rating + length + v_ratio + r + x + b + tap_ratio + \
            phase_shift + i_limit + p_limit + s_limit + status + scolon

        line_data.setParseAction(self.push_line)

        line_array = Literal("Line.con") + "=" + "[" + "..." + \
            ZeroOrMore(line_data + Optional("]" + scolon))

        return line_array

def _get_slack_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        v_rating = real.setResultsName("v_rating") # kV
        v_magnitude = real.setResultsName("v_magnitude") # p.u.
        ref_angle = real.setResultsName("ref_angle") # p.u.
        q_max = Optional(real).setResultsName("q_max") # p.u.
        q_min = Optional(real).setResultsName("q_min") # p.u.
        v_max = Optional(real).setResultsName("v_max") # p.u.
        v_min = Optional(real).setResultsName("v_min") # p.u.
        p_guess = Optional(real).setResultsName("p_guess") # p.u.
        # Loss participation coefficient
        lp_coeff = Optional(real).setResultsName("lp_coeff")
        ref_bus = Optional(boolean).setResultsName("ref_bus")
        status = Optional(boolean).setResultsName("status")

        slack_data = bus_no + s_rating + v_rating + v_magnitude + \
            ref_angle + q_max + q_min + v_max + v_min + p_guess + \
            lp_coeff + ref_bus + status + scolon

        slack_data.setParseAction(self.push_slack)

        slack_array = Literal("SW.con") + "=" + "[" + "..." + \
            ZeroOrMore(slack_data + Optional("]" + scolon))

        return slack_array

def _get_pv_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        v_rating = real.setResultsName("v_rating") # kV
        p = real.setResultsName("p") # p.u.
        v = real.setResultsName("v") # p.u.
        q_max = Optional(real).setResultsName("q_max") # p.u.
        q_min = Optional(real).setResultsName("q_min") # p.u.
        v_max = Optional(real).setResultsName("v_max") # p.u.
        v_min = Optional(real).setResultsName("v_min") # p.u.
        # Loss participation coefficient
        lp_coeff = Optional(real).setResultsName("lp_coeff")
        status = Optional(boolean).setResultsName("status")

        pv_data = bus_no + s_rating + v_rating + p + v + q_max + \
            q_min + v_max + v_min + lp_coeff + status + scolon

        pv_data.setParseAction(self.push_pv)

        pv_array = Literal("PV.con") + "=" + "[" + "..." + \
            ZeroOrMore(pv_data + Optional("]" + scolon))

        return pv_array

def _get_pq_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        v_rating = real.setResultsName("v_rating") # kV
        p = real.setResultsName("p") # p.u.
        q = real.setResultsName("q") # p.u.
        v_max = Optional(real).setResultsName("v_max") # p.u.
        v_min = Optional(real).setResultsName("v_min") # p.u.
        # Allow conversion to impedance
        z_conv = Optional(boolean).setResultsName("z_conv")
        status = Optional(boolean).setResultsName("status")

        pq_data = bus_no + s_rating + v_rating + p + q + v_max + \
            v_min + z_conv + status + scolon

        pq_data.setParseAction(self.push_pq)

        pq_array = Literal("PQ.con") + "=" + "[" + "..." + \
            ZeroOrMore(pq_data + Optional("]" + scolon))

        return pq_array

def _get_demand_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        p_direction = real.setResultsName("p_direction") # p.u.
        q_direction = real.setResultsName("q_direction") # p.u.
        p_bid_max = real.setResultsName("p_bid_max") # p.u.
        p_bid_min = real.setResultsName("p_bid_min") # p.u.
        p_optimal_bid = Optional(real).setResultsName("p_optimal_bid")
        p_fixed = real.setResultsName("p_fixed") # $/hr
        p_proportional = real.setResultsName("p_proportional") # $/MWh
        p_quadratic = real.setResultsName("p_quadratic") # $/MW^2h
        q_fixed = real.setResultsName("q_fixed") # $/hr
        q_proportional = real.setResultsName("q_proportional") # $/MVArh
        q_quadratic = real.setResultsName("q_quadratic") # $/MVAr^2h
        commitment = boolean.setResultsName("commitment")
        cost_tie_break = real.setResultsName("cost_tie_break") # $/MWh
        cost_cong_up = real.setResultsName("cost_cong_up") # $/h
        cost_cong_down = real.setResultsName("cost_cong_down") # $/h
        status = Optional(boolean).setResultsName("status")

        demand_data = bus_no + s_rating + p_direction + q_direction + \
            p_bid_max + p_bid_min + p_optimal_bid + p_fixed + \
            p_proportional + p_quadratic + q_fixed + q_proportional + \
            q_quadratic + commitment + cost_tie_break + cost_cong_up + \
            cost_cong_down + status + scolon

        demand_data.setParseAction(self.push_demand)

        demand_array = Literal("Demand.con") + "=" + "[" + "..." + \
            ZeroOrMore(demand_data + Optional("]" + scolon))

        return demand_array

def _get_supply_array_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        p_direction = real.setResultsName("p_direction") # CPF
        p_bid_max = real.setResultsName("p_bid_max") # p.u.
        p_bid_min = real.setResultsName("p_bid_min") # p.u.
        p_bid_actual = real.setResultsName("p_bid_actual") # p.u.
        p_fixed = real.setResultsName("p_fixed") # $/hr
        p_proportional = real.setResultsName("p_proportional") # $/MWh
        p_quadratic = real.setResultsName("p_quadratic") # $/MW^2h
        q_fixed = real.setResultsName("q_fixed") # $/hr
        q_proportional = real.setResultsName("q_proportional") # $/MVArh
        q_quadratic = real.setResultsName("q_quadratic") # $/MVAr^2h
        commitment = boolean.setResultsName("commitment")
        cost_tie_break = real.setResultsName("cost_tie_break") # $/MWh
        lp_factor = real.setResultsName("lp_factor")# Loss participation factor
        q_max = real.setResultsName("q_max") # p.u.
        q_min = real.setResultsName("q_min") # p.u.
        cost_cong_up = real.setResultsName("cost_cong_up") # $/h
        cost_cong_down = real.setResultsName("cost_cong_down") # $/h
        status = Optional(boolean).setResultsName("status")

        supply_data = bus_no + s_rating + p_direction + p_bid_max + \
            p_bid_min + p_bid_actual + p_fixed + p_proportional + \
            p_quadratic + q_fixed + q_proportional + q_quadratic + \
            commitment + cost_tie_break + lp_factor + q_max + q_min + \
            cost_cong_up + cost_cong_down + status + scolon

        supply_data.setParseAction(self.push_supply)

        supply_array = Literal("Supply.con") + "=" + "[" + "..." + \
            ZeroOrMore(supply_data + Optional("]" + scolon))

        return supply_array

def _get_generator_ramping_construct(self):
        supply_no = integer.setResultsName("supply_no")
        s_rating = real.setResultsName("s_rating") # MVA
        up_rate = real.setResultsName("up_rate") # p.u./h
        down_rate = real.setResultsName("down_rate") # p.u./h
        min_period_up = real.setResultsName("min_period_up") # h
        min_period_down = real.setResultsName("min_period_down") # h
        initial_period_up = integer.setResultsName("initial_period_up")
        initial_period_down = integer.setResultsName("initial_period_down")
        c_startup = real.setResultsName("c_startup") # $
        status = boolean.setResultsName("status")

        g_ramp_data = supply_no + s_rating + up_rate + down_rate + \
            min_period_up + min_period_down + initial_period_up + \
            initial_period_down + c_startup + status + scolon

        g_ramp_array = Literal("Rmpg.con") + "=" + "[" + \
            ZeroOrMore(g_ramp_data + Optional("]" + scolon))

        return g_ramp_array

def _get_load_ramping_construct(self):
        bus_no = integer.setResultsName("bus_no")
        s_rating = real.setResultsName("s_rating") # MVA
        up_rate = real.setResultsName("up_rate") # p.u./h
        down_rate = real.setResultsName("down_rate") # p.u./h
        min_up_time = real.setResultsName("min_up_time") # min
        min_down_time = real.setResultsName("min_down_time") # min
        n_period_up = integer.setResultsName("n_period_up")
        n_period_down = integer.setResultsName("n_period_down")
        status = boolean.setResultsName("status")

        l_ramp_data = bus_no + s_rating + up_rate + down_rate + \
            min_up_time + min_down_time + n_period_up + \
            n_period_down + status + scolon

        l_ramp_array = Literal("Rmpl.con") + "=" + "[" + \
            ZeroOrMore(l_ramp_data + Optional("]" + scolon))

        return l_ramp_array

def push_bus(self, tokens):
        logger.debug("Pushing bus data: %s" % tokens)

        bus = Bus()
        bus.name = tokens["bus_no"]
        bus.v_magnitude = tokens["v_magnitude"]
        bus.v_angle = tokens["v_angle"]
        bus.v_magnitude = tokens["v_magnitude"]
        bus.v_angle = tokens["v_angle"]

        self.case.buses.append(bus)

def push_line(self, tokens):
        logger.debug("Pushing line data: %s" % tokens)

        from_bus = self.case.buses[tokens["fbus"]-1]
        to_bus = self.case.buses[tokens["tbus"]-1]

        e = Branch(from_bus=from_bus, to_bus=to_bus)
        e.r = tokens["r"]
        e.x = tokens["x"]
        e.b = tokens["b"]
        e.rate_a = tokens["s_limit"]
        e.rate_b = tokens["p_limit"]
        e.rate_c = tokens["i_limit"]
        # Optional parameter
        if tokens["tap"] == 0: #Transmission line
            e.ratio = 1.0
        else: # Transformer
            e.ratio = tokens["tap"]
        e.phase_shift = tokens["shift"]
        # Optional parameter
#        if "status" in tokens.keys:
#        e.online = tokens["status"]

        self.case.branches.append(e)

def push_slack(self, tokens):
        logger.debug("Pushing slack data: %s" % tokens)

        bus = self.case.buses[tokens["bus_no"] - 1]

        g = Generator(bus)
        g.q_max = tokens["q_max"]
        g.q_min = tokens["q_min"]
        # Optional parameter
#        if tokens.has_key("status"):
#        g.online = tokens["status"]

        self.case.generators.append(g)

        bus.type = "ref"

def push_pv(self, tokens):
        logger.debug("Pushing PV data: %s" % tokens)

        bus = self.case.buses[tokens["bus_no"]-1]

        g = Generator(bus)
        g.p = tokens["p"]
        g.q_max = tokens["q_max"]
        g.q_min = tokens["q_min"]
        # Optional parameter
#        if tokens.has_key("status"):
#        g.online = tokens["status"]

        self.case.generators.append(g)

def push_pq(self, tokens):
        logger.debug("Pushing PQ data: %s" % tokens)

        bus = self.case.buses[tokens["bus_no"] - 1]
        bus.p_demand = tokens["p"]
        bus.q_demand = tokens["q"]

def push_supply(self, tokens):
        logger.debug("Pushing supply data: %s" % tokens)

        bus = self.case.buses[tokens["bus_no"] - 1]
        n_generators = len([g for g in self.case.generators if g.bus == bus])

        if n_generators == 0:
            logger.error("No generator at bus [%s] for matching supply" % bus)
            return
        elif n_generators > 1:
            g = [g for g in self.case.generators if g.bus == bus][0]
            logger.warning(
                "More than one generator at bus [%s] for demand. Using the "
                "first one [%s]." % (bus, g)
            )
        else:
            g = [g for g in self.case.generators if g.bus == bus][0]

        g.pcost_model = "poly"
        g.poly_coeffs = (
            tokens["p_fixed"],
            tokens["p_proportional"],
            tokens["p_quadratic"]
        )

def _parse_file(self, file):
        case = Case()
        file.seek(0)

        line = file.readline().split()
        if line[0] != "function":
            logger.error("Invalid data file header.")
            return case
        if line[1] != "mpc":
            self._is_struct = False
            base = ""
        else:
            base = "mpc."
        case.name = line[-1]

        for line in file:
            if line.startswith("%sbaseMVA" % base):
                case_data = line.rstrip(";\n").split()
                case.base_mva = float(case_data[-1])
            elif line.startswith("%sbus" % base):
                self._parse_buses(case, file)
            elif line.startswith("%sgencost" % base):
                self._parse_gencost(case, file)
            elif line.startswith("%sgen" % base):
                self._parse_generators(case, file)
            elif line.startswith("%sbranch" % base):
                self._parse_branches(case, file)

        return case

def write(self, file_or_filename):
        if isinstance(file_or_filename, basestring):
            self._fcn_name, _ = splitext(basename(file_or_filename))
        else:
            self._fcn_name = self.case.name

        self._fcn_name = self._fcn_name.replace(",", "").replace(" ", "_")

        super(MATPOWERWriter, self).write(file_or_filename)

def write_case_data(self, file):
        file.write("function mpc = %s\n" % self._fcn_name)
        file.write('\n%%%% MATPOWER Case Format : Version %d\n' % 2)
        file.write("mpc.version = '%d';\n" % 2)

        file.write("\n%%%%-----  Power Flow Data  -----%%%%\n")
        file.write("%%%% system MVA base\n")
        file.write("%sbaseMVA = %g;\n" % (self._prefix, self.case.base_mva))

def write_generator_cost_data(self, file):
        file.write("\n%%%% generator cost data\n")
        file.write("%%\t1\tstartup\tshutdown\tn\tx1\ty1\t...\txn\tyn\n")
        file.write("%%\t2\tstartup\tshutdown\tn\tc(n-1)\t...\tc0\n")
        file.write("%sgencost = [\n" % self._prefix)

        for generator in self.case.generators:
            n = len(generator.p_cost)
            template = '\t%d\t%g\t%g\t%d'
            for _ in range(n):
                template = '%s\t%%g' % template
            template = '%s;\n' % template

            if generator.pcost_model == PW_LINEAR:
                t = 2
#                cp = [p for p, q in generator.p_cost]
#                cq = [q for p, q in generator.p_cost]
#                c = zip(cp, cq)
                c = [v for pc in generator.p_cost for v in pc]
            elif generator.pcost_model == POLYNOMIAL:
                t = 1
                c = list(generator.p_cost)
            else:
                raise

            vals = [t, generator.c_startup, generator.c_shutdown, n] + c

            file.write(template % tuple(vals))
        file.write("];\n")

def write_area_data(self, file):
        file.write("%% area data" + "\n")
        file.write("%\tno.\tprice_ref_bus" + "\n")
        file.write("areas = [" + "\n")
        # TODO: Implement areas
        file.write("\t1\t1;" + "\n")

        file.write("];" + "\n")

def process_file(self, language, key, token_list):
        self.language = language
        for tok in token_list:
            self.process_token(tok)

def draw_plot(self):
        pylab.ion()
        fig = pylab.figure(1)

        # State plot.
#        state_axis = fig.add_subplot(3, 1, 1) # numrows, numcols, fignum
#        state_axis.title = 'State'
#        state_axis.xlabel = 'Time (hours)'
#        state_axis.grid = True
#        for i in range(self.state_data.shape[0]):
#            lines = state_axis.plot(self.state_data[i, 0], "g+-")
#            self.state_lines.append(lines[0])

        # Action plot.
#        action_axis = fig.add_subplot(3, 1, 2)
#        action_axis.title = 'Action'
#        action_axis.xlabel = 'Time (hours)'
#        action_axis.ylabel = 'Price ($/MWh)'
#        action_axis.grid = True
#        for i in range(self.action_data.shape[0]):
#            lines = action_axis.plot(self.action_data[i, 0], "ro-")
#            self.action_lines.append(lines[0])

        # Reward plot.
        reward_axis = fig.add_subplot(3, 1, 3)
#        reward_axis.title = 'Reward'
#        reward_axis.xlabel = 'Time (hours)'
#        reward_axis.ylabel = 'Earnings ($)'
#        reward_axis.grid(True)
        reward_lines = reward_axis.plot(self.reward_data[0, 0], [0], "mx-")
        self.reward_line = reward_lines[0]

        pylab.draw()

def write_case_data(self, file):


        change_code = 0

        s_base = self.case.base_mva

        timestr = time.strftime("%Y%m%d%H%M", time.gmtime())

        file.write("%d, %8.2f, 30 / PSS(tm)E-30 RAW created by Pylon (%s).\n" %

                   (change_code, s_base, timestr))

        file.write("Modified by Hantao Cui, CURENT, UTK\n ")

        file.write("%s, %d BUSES, %d BRANCHES\n" %

                   (self.case.name, len(self.case.buses), len(self.case.branches)))

def plotGenCost(generators):
    figure()
    plots = []
    for generator in generators:
        if generator.pcost_model == PW_LINEAR:
            x = [x for x, _ in generator.p_cost]
            y = [y for _, y in generator.p_cost]
        elif generator.pcost_model == POLYNOMIAL:
            x = scipy.arange(generator.p_min, generator.p_max, 5)
            y = scipy.polyval(scipy.array(generator.p_cost), x)
        else:
            raise
        plots.append(plot(x, y))
        xlabel("P (MW)")
        ylabel("Cost ($)")
    legend(plots, [g.name for g in generators])
    show()

def write(self, file):
        # Write environment state data.
        file.write("State\n")
        file.write( ("-" * 5) + "\n")
        self.writeDataTable(file, type="state")

        # Write action data.
        file.write("Action\n")
        file.write( ("-" * 6) + "\n")
        self.writeDataTable(file, type="action")

        # Write reward data.
        file.write("Reward\n")
        file.write( ("-" * 6) + "\n")
        self.writeDataTable(file, type="reward")

def writeDataTable(self, file, type):
        agents = self.experiment.agents
        numAgents = len(self.experiment.agents)

        colWidth = 8
        idxColWidth = 3

        sep = ("=" * idxColWidth) + " " + \
            ("=" * colWidth + " ") * numAgents + "\n"

        file.write(sep)

        # Table column headers.
        file.write("..".rjust(idxColWidth) + " ")
        for agent in agents:
            # The end of the name is typically the unique part.
            file.write(agent.name[-colWidth:].center(colWidth) + " ")
        file.write("\n")

        file.write(sep)

        # Table values.
        if agents:
            rows, _ = agents[0].history.getField( type ).shape
        else:
            rows, _ = (0, 0)

        for sequence in range( min(rows, 999) ):
            file.write( str(sequence + 1).rjust(idxColWidth) + " " )

            for agent in agents:
                field = agent.history.getField( type )
                # FIXME: Handle multiple state values.
                file.write("%8.3f " % field[sequence, 0])

            file.write("\n")

        file.write(sep)

def performAction(self, action):
#        print "ACTION:", action
        self.t += 1
        Task.performAction(self, action)
#        self.addReward()
        self.samples += 1

def split_dae_alg(eqs: SYM, dx: SYM) -> Dict[str, SYM]:
    x_s = []
    for i in perm:
        x_s.append(x[i])
    return ca.vertcat(*x_s)

def blt(f: List[SYM], x: List[SYM]) -> Dict[str, Any]:
    J = ca.jacobian(f, x)
    nblock, rowperm, colperm, rowblock, colblock, coarserow, coarsecol = J.sparsity().btf()
    return {
        'J': J,
        'nblock': nblock,
        'rowperm': rowperm,
        'colperm': colperm,
        'rowblock': rowblock,
        'colblock': colblock,
        'coarserow': coarserow,
        'coarsecol': coarsecol
    }

def create_function_f_m(self):
        return ca.Function(
            'J',
            [self.t, self.x, self.y, self.m, self.p, self.c, self.ng, self.nu],
            [ca.jacobian(self.f_x_rhs, self.x)],
            ['t', 'x', 'y', 'm', 'p', 'c', 'ng', 'nu'], ['J'], self.func_opt)

def to_ode(self) -> HybridOde:
    _base, ext = os.path.splitext(fname)
    if not ext:
        return None
    try:
        format = known_extensions[ext.replace('.', '')]
    except KeyError:
        format = None
    return format

def pickle_matpower_cases(case_paths, case_format=2):
    import pylon.io

    if isinstance(case_paths, basestring):
        case_paths = [case_paths]

    for case_path in case_paths:
        # Read the MATPOWER case file.
        case = pylon.io.MATPOWERReader(case_format).read(case_path)

        # Give the new file the same name, but with a different extension.
        dir_path = os.path.dirname(case_path)
        case_basename = os.path.basename(case_path)
        root, _ = os.path.splitext(case_basename)
        pickled_case_path = os.path.join(dir_path, root + '.pkl')

        # Pickle the resulting Pylon Case object.
        pylon.io.PickleWriter(case).write(pickled_case_path)

def fair_max(x):
    value = max(x)
    # List indexes of max value.
    i = [x.index(v) for v in x if v == value]
    # Select index randomly among occurances.
    idx = random.choice(i)

    return idx, value

def factorial(n):
    f = 1
    while (n > 0):
        f = f * n
        n = n - 1
    return f

def _get_name(self):
        if self._name is None:
            self._name = self._generate_name()
        return self._name

def save_to_file_object(self, fd, format=None, **kwargs):
        format = 'pickle' if format is None else format
        save = getattr(self, "save_%s" % format, None)
        if save is None:
            raise ValueError("Unknown format '%s'." % format)
        save(fd, **kwargs)

def load_from_file_object(cls, fd, format=None):
        format = 'pickle' if format is None else format
        load = getattr(cls, "load_%s" % format, None)
        if load is None:
            raise ValueError("Unknown format '%s'." % format)
        return load(fd)

def save(self, filename, format=None, **kwargs):
        if format is None:
            # try to derive protocol from file extension
            format = format_from_extension(filename)
        with file(filename, 'wb') as fp:
            self.save_to_file_object(fp, format, **kwargs)

def load(cls, filename, format=None):
        if format is None:
            # try to derive protocol from file extension
            format = format_from_extension(filename)
        with file(filename, 'rbU') as fp:
            obj = cls.load_from_file_object(fp, format)
            obj.filename = filename
            return obj

def solve(self):
        case = self.case
        logger.info("Starting DC power flow [%s]." % case.name)
        t0 = time.time()
        # Update bus indexes.
        self.case.index_buses()

        # Find the index of the refence bus.
        ref_idx = self._get_reference_index(case)
        if ref_idx < 0:
            return False

        # Build the susceptance matrices.
        B, Bsrc, p_businj, p_srcinj = case.Bdc
        # Get the vector of initial voltage angles.
        v_angle_guess = self._get_v_angle_guess(case)
        # Calculate the new voltage phase angles.
        v_angle, p_ref = self._get_v_angle(case, B, v_angle_guess, p_businj,
                                           ref_idx)
        logger.debug("Bus voltage phase angles: \n%s" % v_angle)
        self.v_angle = v_angle

        # Push the results to the case.
        self._update_model(case, B, Bsrc, v_angle, p_srcinj, p_ref, ref_idx)

        logger.info("DC power flow completed in %.3fs." % (time.time() - t0))

        return True

def _get_reference_index(self, case):
        refs = [bus._i for bus in case.connected_buses if bus.type == REFERENCE]
        if len(refs) == 1:
            return refs [0]
        else:
            logger.error("Single swing bus required for DCPF.")
            return -1

def _get_v_angle_guess(self, case):
        v_angle = array([bus.v_angle * (pi / 180.0)
                         for bus in case.connected_buses])
        return v_angle

def _get_v_angle(self, case, B, v_angle_guess, p_businj, iref):
        buses = case.connected_buses

        pv_idxs = [bus._i for bus in buses if bus.type == PV]
        pq_idxs = [bus._i for bus in buses if bus.type == PQ]
        pvpq_idxs = pv_idxs + pq_idxs
        pvpq_rows = [[i] for i in pvpq_idxs]

        # Get the susceptance matrix with the column and row corresponding to
        # the reference bus removed.
        Bpvpq = B[pvpq_rows, pvpq_idxs]

        Bref = B[pvpq_rows, [iref]]

        # Bus active power injections (generation - load) adjusted for phase
        # shifters and real shunts.
        p_surplus = array([case.s_surplus(v).real for v in buses])
        g_shunt = array([bus.g_shunt for bus in buses])
        Pbus = (p_surplus - p_businj - g_shunt) / case.base_mva

        Pbus.shape = len(Pbus), 1

        A = Bpvpq
        b = Pbus[pvpq_idxs] - Bref * v_angle_guess[iref]

#        x, res, rank, s = linalg.lstsq(A.todense(), b)
        x = spsolve(A, b)

        # Insert the reference voltage angle of the slack bus.
        v_angle = r_[x[:iref], v_angle_guess[iref], x[iref:]]

        return v_angle, Pbus[iref]

def _update_model(self, case, B, Bsrc, v_angle, p_srcinj, p_ref, ref_idx):
        iref = ref_idx
        base_mva = case.base_mva
        buses = case.connected_buses
        branches = case.online_branches

        p_from = (Bsrc * v_angle + p_srcinj) * base_mva
        p_to = -p_from

        for i, branch in enumerate(branches):
            branch.p_from = p_from[i]
            branch.p_to = p_to[i]
            branch.q_from = 0.0
            branch.q_to = 0.0

        for j, bus in enumerate(buses):
            bus.v_angle = v_angle[j] * (180 / pi)
            bus.v_magnitude = 1.0

        # Update Pg for swing generator.
        g_ref = [g for g in case.generators if g.bus == buses[iref]][0]
        # Pg = Pinj + Pload + Gs
        # newPg = oldPg + newPinj - oldPinj
        p_inj = (B[iref, :] * v_angle - p_ref) * base_mva
        g_ref.p += p_inj[0]

def getSbus(self, buses=None):
        bs = self.buses if buses is None else buses
        s = array([self.s_surplus(v) / self.base_mva for v in bs])
        return s

def sort_generators(self):
        self.generators.sort(key=lambda gn: gn.bus._i)

def index_buses(self, buses=None, start=0):
        bs = self.connected_buses if buses is None else buses
        for i, b in enumerate(bs):
            b._i = start + i

def index_branches(self, branches=None, start=0):
        ln = self.online_branches if branches is None else branches
        for i, l in enumerate(ln):
            l._i = start + i

def s_supply(self, bus):
        Sg = array([complex(g.p, g.q) for g in self.generators if
                   (g.bus == bus) and not g.is_load], dtype=complex64)

        if len(Sg):
            return sum(Sg)
        else:
            return 0 + 0j

def s_demand(self, bus):
        Svl = array([complex(g.p, g.q) for g in self.generators if
                    (g.bus == bus) and g.is_load], dtype=complex64)

        Sd = complex(bus.p_demand, bus.q_demand)

        return -sum(Svl) + Sd

def reset(self):
        for bus in self.buses:
            bus.reset()
        for branch in self.branches:
            branch.reset()
        for generator in self.generators:
            generator.reset()

def save_matpower(self, fd):
        from pylon.io import MATPOWERWriter
        MATPOWERWriter(self).write(fd)

def load_psat(cls, fd):
        from pylon.io.psat import PSATReader
        return PSATReader().read(fd)

def save_rst(self, fd):
        from pylon.io import ReSTWriter
        ReSTWriter(self).write(fd)

def save_csv(self, fd):
        from pylon.io.excel import CSVWriter
        CSVWriter(self).write(fd)

def save_excel(self, fd):
        from pylon.io.excel import ExcelWriter
        ExcelWriter(self).write(fd)

def save_dot(self, fd):
        from pylon.io import DotWriter
        DotWriter(self).write(fd)

def solve(self):
        # Zero result attributes.
        self.case.reset()

        # Retrieve the contents of the case.
        b, l, g, _, _, _, _ = self._unpack_case(self.case)

        # Update bus indexes.
        self.case.index_buses(b)

        # Index buses accoding to type.
#        try:
#            _, pq, pv, pvpq = self._index_buses(b)
#        except SlackBusError:
#            logger.error("Swing bus required for DCPF.")
#            return {"converged": False}

        refs, pq, pv, pvpq = self._index_buses(b)
        if len(refs) != 1:
            logger.error("Swing bus required for DCPF.")
            return {"converged": False}

        # Start the clock.
        t0 = time()

        # Build the vector of initial complex bus voltages.
        V0 = self._initial_voltage(b, g)

        # Save index and angle of original reference bus.
#        if self.qlimit:
#            ref0 = ref
#            Varef0 = b[ref0].Va
#            # List of buses at Q limits.
#            limits = []
#            # Qg of generators at Q limits.
#            fixedQg = matrix(0.0, (g.size[0], 1))

        repeat = True
        while repeat:
            # Build admittance matrices.
            Ybus, Yf, Yt = self.case.getYbus(b, l)

            # Compute complex bus power injections (generation - load).
            Sbus = self.case.getSbus(b)

            # Run the power flow.
            V, converged, i = self._run_power_flow(Ybus, Sbus, V0, pv, pq, pvpq)

            # Update case with solution.
            self.case.pf_solution(Ybus, Yf, Yt, V)

            # Enforce generator Q limits.
            if self.qlimit:
                raise NotImplementedError
            else:
                repeat = False

        elapsed = time() - t0

        if converged and self.verbose:
            logger.info("AC power flow converged in %.3fs" % elapsed)

        return {"converged": converged, "elapsed": elapsed, "iterations": i,
                "V":V}

def _unpack_case(self, case):
        base_mva = case.base_mva
        b = case.connected_buses
        l = case.online_branches
        g = case.online_generators
        nb = len(b)
        nl = len(l)
        ng = len(g)

        return b, l, g, nb, nl, ng, base_mva

def _index_buses(self, buses):
        refs = [bus._i for bus in buses if bus.type == REFERENCE]
#        if len(refs) != 1:
#            raise SlackBusError
        pv = [bus._i for bus in buses if bus.type == PV]
        pq = [bus._i for bus in buses if bus.type == PQ]
        pvpq = pv + pq

        return refs, pq, pv, pvpq

def _initial_voltage(self, buses, generators):
        Vm = array([bus.v_magnitude for bus in buses])

        # Initial bus voltage angles in radians.
        Va = array([bus.v_angle * (pi / 180.0) for bus in buses])

        V = Vm * exp(1j * Va)

        # Get generator set points.
        for g in generators:
            i = g.bus._i
            V[i] = g.v_magnitude / abs(V[i]) * V[i]

        return V

def _one_iteration(self, F, Ybus, V, Vm, Va, pv, pq, pvpq):
        J = self._build_jacobian(Ybus, V, pv, pq, pvpq)

        # Update step.
        dx = -1 * spsolve(J, F)
#        dx = -1 * linalg.lstsq(J.todense(), F)[0]

        # Update voltage vector.
        npv = len(pv)
        npq = len(pq)
        if npv > 0:
            Va[pv] = Va[pv] + dx[range(npv)]
        if npq > 0:
            Va[pq] = Va[pq] + dx[range(npv, npv + npq)]
            Vm[pq] = Vm[pq] + dx[range(npv + npq, npv + npq + npq)]

        V = Vm * exp(1j * Va)
        Vm = abs(V) # Avoid wrapped round negative Vm.
        Va = angle(V)

        return V, Vm, Va

def _build_jacobian(self, Ybus, V, pv, pq, pvpq):
        pq_col = [[i] for i in pq]
        pvpq_col = [[i] for i in pvpq]

        dS_dVm, dS_dVa = self.case.dSbus_dV(Ybus, V)

        J11 = dS_dVa[pvpq_col, pvpq].real

        J12 = dS_dVm[pvpq_col, pq].real
        J21 = dS_dVa[pq_col, pvpq].imag
        J22 = dS_dVm[pq_col, pq].imag

        J = vstack([
            hstack([J11, J12]),
            hstack([J21, J22])
        ], format="csr")

        return J

def _evaluate_mismatch(self, Ybus, V, Sbus, pq, pvpq):
        mis = (multiply(V, conj(Ybus * V)) - Sbus) / abs(V)

        P = mis[pvpq].real
        Q = mis[pq].imag

        return P, Q

def _p_iteration(self, P, Bp_solver, Vm, Va, pvpq):
        dVa = -Bp_solver.solve(P)

        # Update voltage.
        Va[pvpq] = Va[pvpq] + dVa
        V = Vm * exp(1j * Va)

        return V, Vm, Va

def _q_iteration(self, Q, Bpp_solver, Vm, Va, pq):
        dVm = -Bpp_solver.solve(Q)

        # Update voltage.
        Vm[pq] = Vm[pq] + dVm
        V = Vm * exp(1j * Va)

        return V, Vm, Va

def fmsin(N, fnormin=0.05, fnormax=0.45, period=None, t0=None, fnorm0=0.25, pm1=1):

    if period==None:
	period = N
    if t0==None:
	t0 = N/2
    pm1 = nx.sign(pm1)

    fnormid=0.5*(fnormax+fnormin);
    delta  =0.5*(fnormax-fnormin);
    phi    =-pm1*nx.arccos((fnorm0-fnormid)/delta);
    time   =nx.arange(1,N)-t0;
    phase  =2*nx.pi*fnormid*time+delta*period*(nx.sin(2*nx.pi*time/period+phi)-nx.sin(phi));
    y      =nx.exp(1j*phase)
    iflaw  =fnormid+delta*nx.cos(2*nx.pi*time/period+phi);

    return y,iflaw

def _parse_rdf(self, file):
        store = Graph()
        store.parse(file)

        print len(store)

def load_plugins(group='metrics.plugin.10'):
    # on using entrypoints:
    # http://stackoverflow.com/questions/774824/explain-python-entry-points
    file_processors = []
    build_processors = []
    for ep in pkg_resources.iter_entry_points(group, name=None):
        log.debug('loading \'%s\'', ep)
        plugin = ep.load()  # load the plugin
        if hasattr(plugin, 'get_file_processors'):
            file_processors.extend(plugin.get_file_processors())
        if hasattr(plugin, 'get_build_processors'):
            build_processors.extend(plugin.get_build_processors())
    return file_processors, build_processors

def read_case(input, format=None):
    # Map of data file types to readers.
    format_map = {"matpower": MATPOWERReader,
        "psse": PSSEReader, "pickle": PickleReader}

    # Read case data.
    if format_map.has_key(format):
        reader_klass = format_map[format]
        reader = reader_klass()
        case = reader.read(input)
    else:
        # Try each of the readers at random.
        for reader_klass in format_map.values():
            reader = reader_klass()
            try:
                case = reader.read(input)
                if case is not None:
                    break
            except:
                pass
        else:
            case = None

    return case

def detect_data_file(input, file_name=""):
    _, ext = os.path.splitext(file_name)

    if ext == ".m":
        line = input.readline() # first line
        if line.startswith("function"):
            type = "matpower"
            logger.info("Recognised MATPOWER data file.")
        elif line.startswith("Bus.con" or line.startswith("%")):
            type = "psat"
            logger.info("Recognised PSAT data file.")
        else:
            type = "unrecognised"
        input.seek(0) # reset buffer for parsing

    elif (ext == ".raw") or (ext == ".psse"):
        type = "psse"
        logger.info("Recognised PSS/E data file.")

    elif (ext == ".pkl") or (ext == ".pickle"):
        type = "pickle"
        logger.info("Recognised pickled case.")

    else:
        type = None

    return type

def write(self, file_or_filename, prog=None, format='xdot'):
        if prog is None:
            file = super(DotWriter, self).write(file_or_filename)
        else:
            buf = StringIO.StringIO()
            super(DotWriter, self).write(buf)
            buf.seek(0)
            data = self.create(buf.getvalue(), prog, format)

            if isinstance(file_or_filename, basestring):
                file = None
                try:
                    file = open(file_or_filename, "wb")
                except:
                    logger.error("Error opening %s." % file_or_filename)
                finally:
                    if file is not None:
                        file.write(data)
                        file.close()
            else:
                file = file_or_filename
                file.write(data)

        return file

def write_bus_data(self, file, padding="    "):
        for bus in self.case.buses:
            attrs = ['%s="%s"' % (k, v) for k, v in self.bus_attr.iteritems()]
#            attrs.insert(0, 'label="%s"' % bus.name)
            attr_str = ", ".join(attrs)

            file.write("%s%s [%s];\n" % (padding, bus.name, attr_str))

def write_branch_data(self, file, padding="    "):
        attrs = ['%s="%s"' % (k,v) for k,v in self.branch_attr.iteritems()]
        attr_str = ", ".join(attrs)

        for br in self.case.branches:
            file.write("%s%s -> %s [%s];\n" % \
                (padding, br.from_bus.name, br.to_bus.name, attr_str))

def write_generator_data(self, file, padding="    "):
        attrs = ['%s="%s"' % (k, v) for k, v in self.gen_attr.iteritems()]
        attr_str = ", ".join(attrs)

        edge_attrs = ['%s="%s"' % (k,v) for k,v in {}.iteritems()]
        edge_attr_str = ", ".join(edge_attrs)

        for g in self.case.generators:
            # Generator node.
            file.write("%s%s [%s];\n" % (padding, g.name, attr_str))

            # Edge connecting generator and bus.
            file.write("%s%s -> %s [%s];\n" % \
                       (padding, g.name, g.bus.name, edge_attr_str))

def create(self, dotdata, prog="dot", format="xdot"):
        import os, tempfile
        from dot2tex.dotparsing import find_graphviz

        # Map Graphviz executable names to their paths.
        progs = find_graphviz()
        if progs is None:
            logger.warning("GraphViz executables not found.")
            return None
        if not progs.has_key(prog):
            logger.warning('Invalid program [%s]. Available programs are: %s' % \
                           (prog, progs.keys()))
            return None

        # Make a temporary file ...
        tmp_fd, tmp_name = tempfile.mkstemp()
        os.close(tmp_fd)
        # ... and save the graph to it.
        dot_fd = file(tmp_name, "w+b")
        dot_fd.write(dotdata) # DOT language.
        dot_fd.close()

        # Get the temporary file directory name.
        tmp_dir = os.path.dirname(tmp_name)

        # Process the file using the layout program, specifying the format.
        p = subprocess.Popen((progs[prog], '-T'+format, tmp_name),
            cwd=tmp_dir, stderr=subprocess.PIPE, stdout=subprocess.PIPE)

        stderr = p.stderr
        stdout = p.stdout

        # Make sense of the standard output form the process.
        stdout_output = list()
        while True:
            data = stdout.read()
            if not data:
                break
            stdout_output.append(data)
        stdout.close()

        if stdout_output:
            stdout_output = ''.join(stdout_output)

        # Similarly so for any standard error.
        if not stderr.closed:
            stderr_output = list()
            while True:
                data = stderr.read()
                if not data:
                    break
                stderr_output.append(data)
            stderr.close()

            if stderr_output:
                stderr_output = ''.join(stderr_output)

        status = p.wait()

        if status != 0 :
            logger.error("Program [%s] terminated with status: %d. stderr " \
                "follows: %s" % ( prog, status, stderr_output ) )
        elif stderr_output:
            logger.error( "%s", stderr_output )

        # Remove the temporary file.
        os.unlink(tmp_name)

        return stdout_output

def format(file_metrics, build_metrics):
    agree = False
    answer = raw_input(message).lower()
    if answer.startswith('y'):
        agree = True
    return agree

def main(prog_args=None):
    if prog_args is None:
        prog_args = sys.argv

    parser = optparse.OptionParser()
        # checking filepath
        if not os.path.isdir(file_path):
            raise InvalidFilePath("INVALID CONFIGURATION: file path %s is not a directory" %
                os.path.abspath(file_path)
            )

        if not test_program in IMPLEMENTED_TEST_PROGRAMS:
            raise InvalidTestProgram('The `%s` is unknown, or not yet implemented. Please chose another one.' % test_program)

        if custom_args:
            if not self.quiet and not ask("WARNING!!!\nYou are about to run the following command\n\n   $ %s\n\nAre you sure you still want to proceed [y/N]? " % self.get_cmd()):
                raise CancelDueToUserRequest('Test cancelled...')

def check_dependencies(self):
        "Checks if the test program is available in the python environnement"
        if self.test_program == 'nose':
            try:
                import nose
            except ImportError:
                sys.exit('Nosetests is not available on your system. Please install it and try to run it again')
        if self.test_program == 'py':
            try:
                import py
            except:
                sys.exit('py.test is not available on your system. Please install it and try to run it again')
        if self.test_program == 'django':
            try:
                import django
            except:
                sys.exit('django is not available on your system. Please install it and try to run it again')
        if self.test_program == 'phpunit':
            try:
                process = subprocess.check_call(['phpunit','--version']) 
            except:
                sys.exit('phpunit is not available on your system. Please install it and try to run it again')
        if self.test_program == 'tox':
            try:
                import tox
            except ImportError:
                sys.exit('tox is not available on your system. Please install it and try to run it again')

def get_cmd(self):
        for extension in IGNORE_EXTENSIONS:
            if path.endswith(extension):
                return False
        parts = path.split(os.path.sep)
        for part in parts:
            if part in self.ignore_dirs:
                return False
        return True

def diff_list(self, list1, list2):
        print datetime.datetime.now()
        output = subprocess.Popen(cmd, shell=True)
        output = output.communicate()[0]
        print output

def loop(self):
    metrics = {'files': file_metrics}
    if build_metrics:
        metrics['build'] = build_metrics
    body = json.dumps(metrics, sort_keys=True, indent=4) + '\n'
    return body

def split_linear_constraints(A, l, u):
    ieq = []
    igt = []
    ilt = []
    ibx = []
    for i in range(len(l)):
        if abs(u[i] - l[i]) <= EPS:
            ieq.append(i)
        elif (u[i] > 1e10) and (l[i] > -1e10):
            igt.append(i)
        elif (l[i] <= -1e10) and (u[i] < 1e10):
            ilt.append(i)
        elif (abs(u[i] - l[i]) > EPS) and (u[i] < 1e10) and (l[i] > -1e10):
            ibx.append(i)
        else:
            raise ValueError

    Ae = A[ieq, :]
    Ai = sparse([A[ilt, :], -A[igt, :], A[ibx, :], -A[ibx, :]])
    be = u[ieq, :]
    bi = matrix([u[ilt], -l[igt], u[ibx], -l[ibx]])

    return Ae, be, Ai, bi

def dSbus_dV(Y, V):
    I = Y * V

    diagV = spdiag(V)
    diagIbus = spdiag(I)
    diagVnorm = spdiag(div(V, abs(V))) # Element-wise division.

    dS_dVm = diagV * conj(Y * diagVnorm) + conj(diagIbus) * diagVnorm
    dS_dVa = 1j * diagV * conj(diagIbus - Y * diagV)

    return dS_dVm, dS_dVa

def dIbr_dV(Yf, Yt, V):
#        nb = len(V)

    Vnorm = div(V, abs(V))
    diagV = spdiag(V)
    diagVnorm = spdiag(Vnorm)
    dIf_dVa = Yf * 1j * diagV
    dIf_dVm = Yf * diagVnorm
    dIt_dVa = Yt * 1j * diagV
    dIt_dVm = Yt * diagVnorm

    # Compute currents.
    If = Yf * V
    It = Yt * V

    return dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It

def dSbr_dV(Yf, Yt, V, buses, branches):
    nl = len(branches)
    nb = len(V)

    f = matrix([l.from_bus._i for l in branches])
    t = matrix([l.to_bus._i for l in branches])

    # Compute currents.
    If = Yf * V
    It = Yt * V

    Vnorm = div(V, abs(V))

    diagVf = spdiag(V[f])
    diagIf = spdiag(If)
    diagVt = spdiag(V[t])
    diagIt = spdiag(It)
    diagV = spdiag(V)
    diagVnorm = spdiag(Vnorm)

    ibr = range(nl)
    size = (nl, nb)
    # Partial derivative of S w.r.t voltage phase angle.
    dSf_dVa = 1j * (conj(diagIf) *
        spmatrix(V[f], ibr, f, size) - diagVf * conj(Yf * diagV))

    dSt_dVa = 1j * (conj(diagIt) *
        spmatrix(V[t], ibr, t, size) - diagVt * conj(Yt * diagV))

    # Partial derivative of S w.r.t. voltage amplitude.
    dSf_dVm = diagVf * conj(Yf * diagVnorm) + conj(diagIf) * \
        spmatrix(Vnorm[f], ibr, f, size)

    dSt_dVm = diagVt * conj(Yt * diagVnorm) + conj(diagIt) * \
        spmatrix(Vnorm[t], ibr, t, size)

    # Compute power flow vectors.
    Sf = mul(V[f], conj(If))
    St = mul(V[t], conj(It))

    return dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St

def dAbr_dV(dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St):
    dAf_dPf = spdiag(2 * Sf.real())
    dAf_dQf = spdiag(2 * Sf.imag())
    dAt_dPt = spdiag(2 * St.real())
    dAt_dQt = spdiag(2 * St.imag())

    # Partial derivative of apparent power magnitude w.r.t voltage
    # phase angle.
    dAf_dVa = dAf_dPf * dSf_dVa.real() + dAf_dQf * dSf_dVa.imag()
    dAt_dVa = dAt_dPt * dSt_dVa.real() + dAt_dQt * dSt_dVa.imag()
    # Partial derivative of apparent power magnitude w.r.t. voltage
    # amplitude.
    dAf_dVm = dAf_dPf * dSf_dVm.real() + dAf_dQf * dSf_dVm.imag()
    dAt_dVm = dAt_dPt * dSt_dVm.real() + dAt_dQt * dSt_dVm.imag()

    return dAf_dVa, dAf_dVm, dAt_dVa, dAt_dVm

def d2Sbus_dV2(Ybus, V, lam):
    n = len(V)
    Ibus = Ybus * V
    diaglam = spdiag(lam)
    diagV = spdiag(V)

    A = spmatrix(mul(lam, V), range(n), range(n))
    B = Ybus * diagV
    C = A * conj(B)
    D = Ybus.H * diagV
    E = conj(diagV) * (D * diaglam - spmatrix(D*lam, range(n), range(n)))
    F = C - A * spmatrix(conj(Ibus), range(n), range(n))
    G = spmatrix(div(matrix(1.0, (n, 1)), abs(V)), range(n), range(n))

    Gaa = E + F
    Gva = 1j * G * (E - F)
    Gav = Gva.T
    Gvv = G * (C + C.T) * G

    return Gaa, Gav, Gva, Gvv

def d2Ibr_dV2(Ybr, V, lam):
    nb = len(V)
    diaginvVm = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))

    Haa = spdiag(mul(-(Ybr.T * lam), V))
    Hva = -1j * Haa * diaginvVm
    Hav = Hva
    Hvv = spmatrix([], [], [], (nb, nb))

    return Haa, Hav, Hva, Hvv

def d2Sbr_dV2(Cbr, Ybr, V, lam):
    nb = len(V)

    diaglam = spdiag(lam)
    diagV = spdiag(V)

    A = Ybr.H * diaglam * Cbr
    B = conj(diagV) * A * diagV
    D = spdiag(mul((A*V), conj(V)))
    E = spdiag(mul((A.T * conj(V)), V))
    F = B + B.T
    G = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))

    Haa = F - D - E
    Hva = 1j * G * (B - B.T - D + E)
    Hav = Hva.T
    Hvv = G * F * G

    return Haa, Hav, Hva, Hvv

def tocvx(B):
    Bcoo = B.tocoo()
    return spmatrix(Bcoo.data, Bcoo.row.tolist(), Bcoo.col.tolist())

def doInteractions(self, number=1):
        t0 = time.time()

        for _ in range(number):
            self._oneInteraction()

        elapsed = time.time() - t0
        logger.info("%d interactions executed in %.3fs." % (number, elapsed))

        return self.stepid

def exciter(self, Xexc, Pexc, Vexc):
        exciters = self.exciters

        F = zeros(Xexc.shape)

        typ1 = [e.generator._i for e in exciters if e.model ==CONST_EXCITATION]
        typ2 = [e.generator._i for e in exciters if e.model == IEEE_DC1A]

        # Exciter type 1: constant excitation
        F[typ1, :] = 0.0

        # Exciter type 2: IEEE DC1A
        Efd = Xexc[typ2, 0]
        Uf = Xexc[typ2, 1]
        Ur = Xexc[typ2, 2]

        Ka = Pexc[typ2, 0]
        Ta = Pexc[typ2, 1]
        Ke = Pexc[typ2, 2]
        Te = Pexc[typ2, 3]
        Kf = Pexc[typ2, 4]
        Tf = Pexc[typ2, 5]
        Aex = Pexc[typ2, 6]
        Bex = Pexc[typ2, 7]
        Ur_min = Pexc[typ2, 8]
        Ur_max = Pexc[typ2, 9]
        Uref = Pexc[typ2, 10]
        Uref2 = Pexc[typ2, 11]

        U = Vexc[typ2, 1]

        Ux = Aex * exp(Bex * Efd)
        dUr = 1 / Ta * (Ka * (Uref - U + Uref2 - Uf) - Ur)
        dUf = 1 / Tf * (Kf / Te * (Ur - Ux - Ke * Efd) - Uf)

        if sum(flatnonzero(Ur > Ur_max)) >= 1:
            Ur2 = Ur_max
        elif sum(flatnonzero(Ur < Ur_max)) >= 1:
            Ur2 = Ur_min
        else:
            Ur2 = Ur

        dEfd = 1 / Te * (Ur2 - Ux - Ke * Efd)
        F[typ2, :] = c_[dEfd, dUf, dUr]

        # Exciter type 3:

        # Exciter type 4:

        return F

def governor(self, Xgov, Pgov, Vgov):
        governors = self.governors
        omegas = 2 * pi * self.freq

        F = zeros(Xgov.shape)

        typ1 = [g.generator._i for g in governors if g.model == CONST_POWER]
        typ2 = [g.generator._i for g in governors if g.model == GENERAL_IEEE]

        # Governor type 1: constant power
        F[typ1, 0] = 0

        # Governor type 2: IEEE general speed-governing system
        Pm = Xgov[typ2, 0]
        P = Xgov[typ2, 1]
        x = Xgov[typ2, 2]
        z = Xgov[typ2, 3]

        K = Pgov[typ2, 0]
        T1 = Pgov[typ2, 1]
        T2 = Pgov[typ2, 2]
        T3 = Pgov[typ2, 3]
        Pup = Pgov[typ2, 4]
        Pdown = Pgov[typ2, 5]
        Pmax = Pgov[typ2, 6]
        Pmin = Pgov[typ2, 7]
        P0 = Pgov[typ2, 8]

        omega = Vgov[typ2, 0]

        dx = K * (-1 / T1 * x + (1 - T2 / T1) * (omega - omegas))
        dP = 1 / T1 * x + T2 / T1 * (omega - omegas)

        y = 1 / T3 * (P0 - P - Pm)

        y2 = y

        if sum(flatnonzero(y > Pup)) >= 1:
            y2 = (1 - flatnonzero(y > Pup)) * y2 + flatnonzero(y > Pup) * Pup
        if sum(flatnonzero(y < Pdown)) >= 1:
            y2 = (1 - flatnonzero(y<Pdown)) * y2 + flatnonzero(y<Pdown) * Pdown

        dz = y2

        dPm = y2

        if sum(flatnonzero(z > Pmax)) >= 1:
            dPm = (1 - flatnonzero(z > Pmax)) * dPm + flatnonzero(z > Pmax) * 0
        if sum(flatnonzero(z < Pmin)) >= 1:
            dPm = (1 - flatnonzero(z < Pmin)) * dPm + flatnonzero(z < Pmin) * 0

        F[typ2, :] = c_[dPm, dP, dx, dz]

        # Governor type 3:

        # Governor type 4:

        return F

def generator(self, Xgen, Xexc, Xgov, Vgen):
        generators = self.dyn_generators
        omegas = 2 * pi * self.freq

        F = zeros(Xgen.shape)

        typ1 = [g._i for g in generators if g.model == CLASSICAL]
        typ2 = [g._i for g in generators if g.model == FOURTH_ORDER]

        # Generator type 1: classical model
        omega = Xgen[typ1, 1]
        Pm0 = Xgov[typ1, 0]

        H = array([g.h for g in generators])[typ1]
        D = array([g.d for g in generators])[typ1]

        Pe = Vgen[typ1, 2]

        ddelta = omega = omegas
        domega = pi * self.freq / H * (-D * (omega - omegas) + Pm0 - Pe)
        dEq = zeros(len(typ1))

        F[typ1, :] = c_[ddelta, domega, dEq]

        # Generator type 2: 4th order model
        omega = Xgen[typ2, 1]
        Eq_tr = Xgen[typ2, 2]
        Ed_tr = Xgen[typ2, 3]

        H = array([g.h for g in generators])
        D = array([g.d for g in generators])
        xd = array([g.xd for g in generators])
        xq = array([g.xq for g in generators])
        xd_tr = array([g.xd_tr for g in generators])
        xq_tr = array([g.xq_tr for g in generators])
        Td0_tr = array([g.td for g in generators])
        Tq0_tr = array([g.tq for g in generators])

        Id = Vgen[typ2, 0]
        Iq = Vgen[typ2, 1]
        Pe = Vgen[typ2, 2]

        Efd = Xexc[typ2, 0]
        Pm = Xgov[typ2, 0]

        ddelta = omega - omegas
        domega = pi * self.freq / H * (-D * (omega - omegas) + Pm - Pe)
        dEq = 1 / Td0_tr * (Efd - Eq_tr + (xd - xd_tr) * Id)
        dEd = 1 / Tq0_tr * (-Ed_tr - (xq - xq_tr) * Iq)

        F[typ2, :] = c_[ddelta, domega, dEq, dEd]

        # Generator type 3:

        # Generator type 4:

        return F

def _write_data(self, file):
        self.write_case_data(file)

        file.write("Bus Data\n")
        file.write("-" * 8 + "\n")
        self.write_bus_data(file)
        file.write("\n")

        file.write("Branch Data\n")
        file.write("-" * 11 + "\n")
        self.write_branch_data(file)
        file.write("\n")

        file.write("Generator Data\n")
        file.write("-" * 14 + "\n")
        self.write_generator_data(file)
        file.write("\n")

def write_bus_data(self, file):
        report = CaseReport(self.case)
        buses = self.case.buses

        col_width = 8
        col_width_2 = col_width * 2 + 1
        col1_width = 6

        sep = "=" * 6 + " " + ("=" * col_width + " ") * 6 + "\n"

        file.write(sep)
        # Line one of column headers
        file.write("Name".center(col1_width) + " ")
        file.write("Voltage (pu)".center(col_width_2) + " ")
        file.write("Generation".center(col_width_2) + " ")
        file.write("Load".center(col_width_2) + " ")
        file.write("\n")

        file.write("-" * col1_width +" "+ ("-" * col_width_2 + " ") * 3 + "\n")

        # Line two of column header
        file.write("..".ljust(col1_width) + " ")
        file.write("Amp".center(col_width) + " ")
        file.write("Phase".center(col_width) + " ")
        file.write("P (MW)".center(col_width) + " ")
        file.write("Q (MVAr)".center(col_width) + " ")
        file.write("P (MW)".center(col_width) + " ")
        file.write("Q (MVAr)".center(col_width) + " ")
        file.write("\n")

        file.write(sep)

        # Bus rows
        for bus in buses:
            file.write(bus.name[:col1_width].ljust(col1_width))
            file.write(" %8.3f" % bus.v_magnitude)
            file.write(" %8.3f" % bus.v_angle)
            file.write(" %8.2f" % self.case.s_supply(bus).real)
            file.write(" %8.2f" % self.case.s_supply(bus).imag)
            file.write(" %8.2f" % self.case.s_demand(bus).real)
            file.write(" %8.2f" % self.case.s_demand(bus).imag)
            file.write("\n")

        # Totals
#        file.write("..".ljust(col1_width) + " ")
#        file.write(("..".ljust(col_width) + " ")*2)
#        file.write(("_"*col_width + " ")*4 + "\n")
        file.write("..".ljust(col1_width) + " " + "..".ljust(col_width) + " ")
        file.write("*Total:*".rjust(col_width) + " ")
        ptot = report.actual_pgen
        qtot = report.actual_qgen
        file.write("%8.2f " % ptot)
        file.write("%8.2f " % qtot)
        file.write("%8.2f " % report.p_demand)
        file.write("%8.2f " % report.q_demand)
        file.write("\n")
        file.write(sep)
        del report

def write_how_many(self, file):
        report = CaseReport(self.case)

        # Map component labels to attribute names
        components = [("Bus", "n_buses"), ("Generator", "n_generators"),
            ("Committed Generator", "n_online_generators"),
            ("Load", "n_loads"), ("Fixed Load", "n_fixed_loads"),
            ("Despatchable Load", "n_online_vloads"), ("Shunt", "n_shunts"),
            ("Branch", "n_branches"), ("Transformer", "n_transformers"),
            ("Inter-tie", "n_interties"), ("Area", "n_areas")
        ]

        # Column 1 width
        longest = max([len(c[0]) for c in components])

        col1_header = "Object"
        col1_width = longest
        col2_header = "Quantity"
        col2_width = len(col2_header)

        # Row separator
        sep = "="*col1_width + " " + "="*col2_width + "\n"

        # Row headers
        file.write(sep)

        file.write(col1_header.center(col1_width))
        file.write(" ")
        file.write("%s\n" % col2_header.center(col2_width))

        file.write(sep)

        # Rows
        for label, attr in components:
            col2_value = str(getattr(report, attr))
            file.write("%s %s\n" %
                (label.ljust(col1_width), col2_value.rjust(col2_width)))
        else:
            file.write(sep)
            file.write("\n")

        del report

def write_min_max(self, file):
        report = CaseReport(self.case)

        col1_header = "Attribute"
        col1_width  = 19
        col2_header = "Minimum"
        col3_header = "Maximum"
        col_width   = 22

        sep = "="*col1_width +" "+ "="*col_width +" "+ "="*col_width + "\n"

        # Row headers
        file.write(sep)

        file.write("%s" % col1_header.center(col1_width))
        file.write(" ")
        file.write("%s" % col2_header.center(col_width))
        file.write(" ")
        file.write("%s" % col3_header.center(col_width))
        file.write("\n")

        file.write(sep)

        # Rows
        min_val, min_i = getattr(report, "min_v_magnitude")
        max_val, max_i = getattr(report, "max_v_magnitude")
        file.write("%s %7.3f p.u. @ bus %2d %7.3f p.u. @ bus %2d\n" %
            ("Voltage Amplitude".ljust(col1_width),
             min_val, min_i, max_val, max_i))

        min_val, min_i = getattr(report, "min_v_angle")
        max_val, max_i = getattr(report, "max_v_angle")
        file.write("%s %16.3f %16.3f\n" %
            ("Voltage Phase Angle".ljust(col1_width), min_val, max_val))

        file.write(sep)
        file.write("\n")

        del report

def make_unique_name(base, existing=[], format="%s_%s"):
    count = 2
    name = base
    while name in existing:
        name = format % (base, count)
        count += 1

    return name

def call_antlr4(arg):
    "calls antlr4 on grammar file"
    # pylint: disable=unused-argument, unused-variable
    antlr_path = os.path.join(ROOT_DIR, "java", "antlr-4.7-complete.jar")
    classpath = os.pathsep.join([".", "{:s}".format(antlr_path), "$CLASSPATH"])
    generated = os.path.join(ROOT_DIR, 'src', 'pymoca', 'generated')
    cmd = "java -Xmx500M -cp \"{classpath:s}\" org.antlr.v4.Tool {arg:s}" \
          " -o {generated:s} -visitor -Dlanguage=Python3".format(**locals())
    print(cmd)
    proc = subprocess.Popen(cmd.split(), cwd=os.path.join(ROOT_DIR, 'src', 'pymoca'))
    proc.communicate()
    with open(os.path.join(generated, '__init__.py'), 'w') as fid:
        fid.write('')

def setup_package():
    with open('requirements.txt', 'r') as req_file:
        install_reqs = req_file.read().split('\n')

    cmdclass_ = {'antlr': AntlrBuildCommand}
    cmdclass_.update(versioneer.get_cmdclass())

    setup(
        version=versioneer.get_version(),
        name='pymoca',
        maintainer="James Goppert",
        maintainer_email="james.goppert@gmail.com",
        description=DOCLINES[0],
        long_description="\n".join(DOCLINES[2:]),
        url='https://github.com/pymoca/pymoca',
        author='James Goppert',
        author_email='james.goppert@gmail.com',
        download_url='https://github.com/pymoca/pymoca',
        license='BSD',
        classifiers=[_f for _f in CLASSIFIERS.split('\n') if _f],
        platforms=["Windows", "Linux", "Solaris", "Mac OS-X", "Unix"],
        install_requires=install_reqs,
        tests_require=['coverage >= 3.7.1', 'nose >= 1.3.1'],
        test_suite='nose.collector',
        python_requires='>=3.5',
        packages=find_packages("src"),
        package_dir={"": "src"},
        include_package_data=True,
        cmdclass=cmdclass_
    )

def body(self, frame):
        master = Frame(self)
        master.pack(padx=5, pady=0, expand=1, fill=BOTH)

        title = Label(master, text="Buses")
        title.pack(side=TOP)

        bus_lb = self.bus_lb = Listbox(master, selectmode=SINGLE, width=10)
        bus_lb.pack(side=LEFT)

        for bus in self.case.buses:
            bus_lb.insert(END, bus.name)

        bus_lb.bind("<<ListboxSelect>>", self.on_bus)

        self.bus_params = BusProperties(master)

        return bus_lb

def solve(self, solver_klass=None):
        # Start the clock.
        t0 = time()

        # Build an OPF model with variables and constraints.
        om = self._construct_opf_model(self.case)
        if om is None:
            return {"converged": False, "output": {"message": "No Ref Bus."}}

        # Call the specific solver.
#        if self.opt["verbose"]:
#            print '\nPYLON Version %s, %s', "0.4.2", "April 2010"
        if solver_klass is not None:
            result = solver_klass(om, opt=self.opt).solve()
        elif self.dc:
#            if self.opt["verbose"]:
#                print ' -- DC Optimal Power Flow\n'
            result = DCOPFSolver(om, opt=self.opt).solve()
        else:
#            if self.opt["verbose"]:
#                print ' -- AC Optimal Power Flow\n'
            result = PIPSSolver(om, opt=self.opt).solve()

        result["elapsed"] = time() - t0

        if self.opt.has_key("verbose"):
            if self.opt["verbose"]:
                logger.info("OPF completed in %.3fs." % result["elapsed"])

        return result

def _construct_opf_model(self, case):
        # Zero the case result attributes.
        self.case.reset()

        base_mva = case.base_mva

        # Check for one reference bus.
        oneref, refs = self._ref_check(case)
        if not oneref: #return {"status": "error"}
            None

        # Remove isolated components.
        bs, ln, gn = self._remove_isolated(case)

        # Update bus indexes.
        self.case.index_buses(bs)

        # Convert single-block piecewise-linear costs into linear polynomial.
        gn = self._pwl1_to_poly(gn)

        # Set-up initial problem variables.
        Va = self._get_voltage_angle_var(refs, bs)
        Pg = self._get_pgen_var(gn, base_mva)

        if self.dc: # DC model.
            # Get the susceptance matrices and phase shift injection vectors.
            B, Bf, Pbusinj, Pfinj = self.case.makeBdc(bs, ln)

            # Power mismatch constraints (B*Va + Pg = Pd).
            Pmis = self._power_mismatch_dc(bs, gn, B, Pbusinj, base_mva)

            # Branch flow limit constraints.
            Pf, Pt = self._branch_flow_dc(ln, Bf, Pfinj, base_mva)
        else:
            # Set-up additional AC-OPF problem variables.
            Vm = self._get_voltage_magnitude_var(bs, gn)
            Qg = self._get_qgen_var(gn, base_mva)

            Pmis, Qmis, Sf, St = self._nln_constraints(len(bs), len(ln))

            vl = self._const_pf_constraints(gn, base_mva)

            # TODO: Generator PQ capability curve constraints.
#            PQh, PQl = self._pq_capability_curve_constraints(gn)

        # Branch voltage angle difference limits.
        ang = self._voltage_angle_diff_limit(bs, ln)

        if self.dc:
            vars = [Va, Pg]
            constraints = [Pmis, Pf, Pt, ang]
        else:
            vars = [Va, Vm, Pg, Qg]
            constraints = [Pmis, Qmis, Sf, St, #PQh, PQL,
                           vl, ang]

        # Piece-wise linear generator cost constraints.
        y, ycon = self._pwl_gen_costs(gn, base_mva)

        if ycon is not None:
            vars.append(y)
            constraints.append(ycon)

        # Add variables and constraints to the OPF model object.
        opf = OPFModel(case)
        opf.add_vars(vars)
        opf.add_constraints(constraints)

        if self.dc: # user data
            opf._Bf = Bf
            opf._Pfinj = Pfinj

        return opf

def _ref_check(self, case):
        refs = [bus._i for bus in case.buses if bus.type == REFERENCE]

        if len(refs) == 1:
            return True, refs
        else:
            logger.error("OPF requires a single reference bus.")
            return False, refs

def _remove_isolated(self, case):
#        case.deactivate_isolated()
        buses = case.connected_buses
        branches = case.online_branches
        gens = case.online_generators

        return buses, branches, gens

def _pwl1_to_poly(self, generators):
        for g in generators:
            if (g.pcost_model == PW_LINEAR) and (len(g.p_cost) == 2):
                g.pwl_to_poly()

        return generators

def _get_voltage_angle_var(self, refs, buses):
        Va = array([b.v_angle * (pi / 180.0) for b in buses])

        Vau = Inf * ones(len(buses))
        Val = -Vau
        Vau[refs] = Va[refs]
        Val[refs] = Va[refs]

        return Variable("Va", len(buses), Va, Val, Vau)

def _get_voltage_magnitude_var(self, buses, generators):
        Vm = array([b.v_magnitude for b in buses])

        # For buses with generators initialise Vm from gen data.
        for g in generators:
            Vm[g.bus._i] = g.v_magnitude

        Vmin = array([b.v_min for b in buses])
        Vmax = array([b.v_max for b in buses])

        return Variable("Vm", len(buses), Vm, Vmin, Vmax)

def _get_pgen_var(self, generators, base_mva):
        Pg = array([g.p / base_mva for g in generators])

        Pmin = array([g.p_min / base_mva for g in generators])
        Pmax = array([g.p_max / base_mva for g in generators])

        return Variable("Pg", len(generators), Pg, Pmin, Pmax)

def _get_qgen_var(self, generators, base_mva):
        Qg = array([g.q / base_mva for g in generators])

        Qmin = array([g.q_min / base_mva for g in generators])
        Qmax = array([g.q_max / base_mva for g in generators])

        return Variable("Qg", len(generators), Qg, Qmin, Qmax)

def _nln_constraints(self, nb, nl):
        Pmis = NonLinearConstraint("Pmis", nb)
        Qmis = NonLinearConstraint("Qmis", nb)
        Sf = NonLinearConstraint("Sf", nl)
        St = NonLinearConstraint("St", nl)

        return Pmis, Qmis, Sf, St

def _const_pf_constraints(self, gn, base_mva):
        ivl = array([i for i, g in enumerate(gn)
                     if g.is_load and (g.q_min != 0.0 or g.q_max != 0.0)])
        vl = [gn[i] for i in ivl]
        nvl = len(vl)

        ng = len(gn)
        Pg = array([g.p for g in vl]) / base_mva
        Qg = array([g.q for g in vl]) / base_mva
        Pmin = array([g.p_min for g in vl]) / base_mva
        Qmin = array([g.q_min for g in vl]) / base_mva
        Qmax = array([g.q_max for g in vl]) / base_mva

        # At least one of the Q limits must be zero (corresponding to Pmax==0).
        for g in vl:
            if g.qmin != 0.0 and g.q_max != 0.0:
                logger.error("Either Qmin or Qmax must be equal to zero for "
                "each dispatchable load.")

        # Initial values of PG and QG must be consistent with specified power
        # factor. This is to prevent a user from unknowingly using a case file

        # which would have defined a different power factor constraint under a

        # previous version which used PG and QG to define the power factor.
        Qlim = (Qmin == 0.0) * Qmax + (Qmax == 0.0) * Qmin
        if any( abs(Qg - Pg * Qlim / Pmin) > 1e-6 ):
            logger.error("For a dispatchable load, PG and QG must be "

                         "consistent with the power factor defined by "
                         "PMIN and the Q limits.")

        # Make Avl, lvl, uvl, for lvl <= Avl * r_[Pg, Qg] <= uvl
        if nvl > 0:
            xx = Pmin
            yy = Qlim
            pftheta = arctan2(yy, xx)
            pc = sin(pftheta)
            qc = -cos(pftheta)
            ii = array([range(nvl), range(nvl)])
            jj = r_[ivl, ivl + ng]
            Avl = csr_matrix(r_[pc, qc], (ii, jj), (nvl, 2 * ng))
            lvl = zeros(nvl)
            uvl = lvl
        else:
            Avl = zeros((0, 2 * ng))
            lvl = array([])
            uvl = array([])

        return LinearConstraint("vl", Avl, lvl, uvl, ["Pg", "Qg"])

def _voltage_angle_diff_limit(self, buses, branches):
        nb = len(buses)

        if not self.ignore_ang_lim:
            iang = [i for i, b in enumerate(branches)
                    if (b.ang_min and (b.ang_min > -360.0))
                    or (b.ang_max and (b.ang_max < 360.0))]
            iangl = array([i for i, b in enumerate(branches)
                     if b.ang_min is not None])[iang]
            iangh = array([i for i, b in enumerate(branches)
                           if b.ang_max is not None])[iang]
            nang = len(iang)

            if nang > 0:
                ii = range(nang) + range(nang)
                jjf = array([b.from_bus._i for b in branches])[iang]
                jjt = array([b.to_bus._i for b in branches])[iang]
                jj = r_[jjf, jjt]
                Aang = csr_matrix(r_[ones(nang), -ones(nang)], (ii, jj))
                uang = Inf * ones(nang)
                lang = -uang
                lang[iangl] = array([b.ang_min * (pi / 180.0)
                                    for b in branches])[iangl]
                uang[iangh] = array([b.ang_max * (pi / 180.0)
                                    for b in branches])[iangh]
            else:
#                Aang = csr_matrix((0, nb), dtype=float64)
#                lang = array([], dtype=float64)
#                uang = array([], dtype=float64)
                Aang = zeros((0, nb))
                lang = array([])
                uang = array([])
        else:
#            Aang = csr_matrix((0, nb), dtype=float64)
#            lang = array([], dtype=float64)
#            uang = array([], dtype=float64)
#            iang = array([], dtype=float64)
            Aang = zeros((0, nb))
            lang = array([])
            uang = array([])

        return LinearConstraint("ang", Aang, lang, uang, ["Va"])

def add_var(self, var):
        if var.name in [v.name for v in self.vars]:
            logger.error("Variable set named '%s' already exists." % var.name)
            return

        var.i1 = self.var_N
        var.iN = self.var_N + var.N - 1
        self.vars.append(var)

def get_var(self, name):
        for var in self.vars:
            if var.name == name:
                return var
        else:
            raise ValueError

def linear_constraints(self):
        if self.lin_N == 0:
            return None, array([]), array([])

        A = lil_matrix((self.lin_N, self.var_N), dtype=float64)
        l = -Inf * ones(self.lin_N)
        u = -l

        for lin in self.lin_constraints:
            if lin.N:                   # non-zero number of rows to add
                Ak = lin.A              # A for kth linear constrain set
                i1 = lin.i1             # starting row index
                iN = lin.iN             # ending row index
                vsl = lin.vs            # var set list
                kN = -1                 # initialize last col of Ak used
                Ai = lil_matrix((lin.N, self.var_N), dtype=float64)
                for v in vsl:
                    var = self.get_var(v)
                    j1 = var.i1         # starting column in A
                    jN = var.iN         # ending column in A
                    k1 = kN + 1         # starting column in Ak
                    kN = kN + var.N     # ending column in Ak

                    if j1 == jN:
                        # FIXME: Single column slicing broken in lil.
                        for i in range(Ai.shape[0]):
                            Ai[i, j1] = Ak[i, k1]
                    else:
                        Ai[:, j1:jN + 1] = Ak[:, k1:kN + 1]

                A[i1:iN + 1, :] = Ai
                l[i1:iN + 1] = lin.l
                u[i1:iN + 1] = lin.u

        return A.tocsr(), l, u

def add_constraint(self, con):
        if isinstance(con, LinearConstraint):
            N, M = con.A.shape
            if con.name in [c.name for c in self.lin_constraints]:
                logger.error("Constraint set named '%s' already exists."
                             % con.name)
                return False
            else:
                con.i1 = self.lin_N# + 1
                con.iN = self.lin_N + N - 1

                nv = 0
                for vs in con.vs:
                    nv = nv + self.get_var_N(vs)
                if M != nv:
                    logger.error("Number of columns of A does not match number"
                        " of variables, A is %d x %d, nv = %d", N, M, nv)
                self.lin_constraints.append(con)
        elif isinstance(con, NonLinearConstraint):
            N = con.N
            if con.name in [c.name for c in self.nln_constraints]:
                logger.error("Constraint set named '%s' already exists."
                             % con.name)
                return False
            else:
                con.i1 = self.nln_N# + 1
                con.iN = self.nln_N + N
                self.nln_constraints.append(con)
        else:
            raise ValueError

        return True

def _solve(self, x0, A, l, u, xmin, xmax):
        # Indexes of constrained lines.
        il = [i for i,ln in enumerate(self._ln) if 0.0 < ln.rate_a < 1e10]
        nl2 = len(il)

        neqnln = 2 * self._nb # no. of non-linear equality constraints
        niqnln = 2 * len(il)  # no. of lines with constraints

        user_data = {"A": A, "neqnln": neqnln, "niqnln": niqnln}

        self._f(x0)
        Jdata = self._dg(x0, False, user_data)
#        Hdata = self._h(x0, ones(neqnln + niqnln), None, False, user_data)

        lmbda = {"eqnonlin": ones(neqnln),
                 "ineqnonlin": ones(niqnln)}
        H = tril(self._hessfcn(x0, lmbda), format="coo")
        self._Hrow, self._Hcol = H.row, H.col

        n = len(x0) # the number of variables
        xl = xmin
        xu = xmax
        gl = r_[zeros(2 * self._nb), -Inf * ones(2 * nl2), l]
        gu = r_[zeros(2 * self._nb),       zeros(2 * nl2), u]
        m = len(gl) # the number of constraints
        nnzj = len(Jdata) # the number of nonzeros in Jacobian matrix
        nnzh = 0#len(H.data) # the number of non-zeros in Hessian matrix

        f_fcn, df_fcn, g_fcn, dg_fcn, h_fcn = \
            self._f, self._df, self._g, self._dg, self._h

        nlp = pyipopt.create(n, xl, xu, m, gl, gu, nnzj, nnzh,
                             f_fcn, df_fcn, g_fcn, dg_fcn)#, h_fcn)

#        print dir(nlp)
#        nlp.str_option("print_options_documentation", "yes")
#        nlp.int_option("max_iter", 10)

#        x, zl, zu, obj = nlp.solve(x0)
        success = nlp.solve(x0, user_data)
        nlp.close()

def doOutages(self):
        assert len(self.branchOutages) == len(self.market.case.branches)

        weights = [[(False, r), (True, 1 - (r))] for r in self.branchOutages]

        for i, ln in enumerate(self.market.case.branches):
            ln.online = weighted_choice(weights[i])
            if ln.online == False:
                print "Branch outage [%s] in period %d." %(ln.name,self.stepid)

def reset_case(self):
        for bus in self.market.case.buses:
            bus.p_demand = self.pdemand[bus]
        for task in self.tasks:
            for g in task.env.generators:
                g.p = task.env._g0[g]["p"]
                g.p_max = task.env._g0[g]["p_max"]
                g.p_min = task.env._g0[g]["p_min"]
                g.q = task.env._g0[g]["q"]
                g.q_max = task.env._g0[g]["q_max"]
                g.q_min = task.env._g0[g]["q_min"]
                g.p_cost = task.env._g0[g]["p_cost"]
                g.pcost_model = task.env._g0[g]["pcost_model"]
                g.q_cost = task.env._g0[g]["q_cost"]
                g.qcost_model = task.env._g0[g]["qcost_model"]
                g.c_startup = task.env._g0[g]["startup"]
                g.c_shutdown = task.env._g0[g]["shutdown"]

def doEpisodes(self, number=1):
        for episode in range(number):
            print "Starting episode %d." % episode

            # Initialise the profile cycle.
            if len(self.profile.shape) == 1: # 1D array
                self._pcycle = cycle(self.profile)
            else:
                assert self.profile.shape[0] >= number
                self._pcycle = cycle(self.profile[episode, :])

            # Scale the initial load.
            c = self._pcycle.next()
            for bus in self.market.case.buses:
                bus.p_demand = self.pdemand[bus] * c

            # Initialise agents and their tasks.
            for task, agent in zip(self.tasks, self.agents):
                agent.newEpisode()
                task.reset()

            while False in [task.isFinished() for task in self.tasks]:
                if True in [task.isFinished() for task in self.tasks]:
                    raise ValueError
                self._oneInteraction()

        self.reset_case()

def reset(self):
        self.stepid = 0

        for task, agent in zip(self.tasks, self.agents):
            task.reset()

            agent.module.reset()
            agent.history.reset()

def _updatePropensities(self, lastState, lastAction, reward):
        phi = self.recency

        for action in range(self.module.numActions):
            carryOver = (1 - phi) * self.module.getValue(lastState, action)
            experience = self._experience(lastState, action, lastAction,reward)

            self.module.updateValue(lastState, action, carryOver + experience)

def _forwardImplementation(self, inbuf, outbuf):
        assert self.module

        propensities = self.module.getActionValues(0)

        summedProps = sum(propensities)
        probabilities = propensities / summedProps

        action = eventGenerator(probabilities)
#        action = drawIndex(probabilities)

        outbuf[:] = scipy.array([action])

def write(self, file_or_filename):
        self.book = Workbook()
        self._write_data(None)
        self.book.save(file_or_filename)

def write_bus_data(self, file):
        bus_sheet = self.book.add_sheet("Buses")

        for i, bus in enumerate(self.case.buses):
            for j, attr in enumerate(BUS_ATTRS):
                bus_sheet.write(i, j, getattr(bus, attr))

def write_branch_data(self, file):
        branch_sheet = self.book.add_sheet("Branches")

        for i, branch in enumerate(self.case.branches):
            for j, attr in enumerate(BRANCH_ATTRS):
                branch_sheet.write(i, j, getattr(branch, attr))

def write_generator_data(self, file):
        generator_sheet = self.book.add_sheet("Generators")

        for j, generator in enumerate(self.case.generators):
            i = generator.bus._i
            for k, attr in enumerate(GENERATOR_ATTRS):
                generator_sheet.write(j, 0, i)

def write(self, file_or_filename):
        if isinstance(file_or_filename, basestring):
            file = open(file_or_filename, "wb")
        else:
            file = file_or_filename

        self.writer = csv.writer(file)

        super(CSVWriter, self).write(file)

def write_case_data(self, file):
        writer = self._get_writer(file)
        writer.writerow(["Name", "base_mva"])
        writer.writerow([self.case.name, self.case.base_mva])

def write_bus_data(self, file):
        writer = self._get_writer(file)
        writer.writerow(BUS_ATTRS)
        for bus in self.case.buses:
            writer.writerow([getattr(bus, attr) for attr in BUS_ATTRS])

def write_branch_data(self, file):
        writer = self._get_writer(file)
        writer.writerow(BRANCH_ATTRS)
        for branch in self.case.branches:
            writer.writerow([getattr(branch, a) for a in BRANCH_ATTRS])

def write_generator_data(self, file):
        writer = self._get_writer(file)
        writer.writerow(["bus"] + GENERATOR_ATTRS)

        for g in self.case.generators:
            i = g.bus._i
            writer.writerow([i] + [getattr(g,a) for a in GENERATOR_ATTRS])

def run(self):
        # Start the clock.
        t0 = time.time()

        # Manage reactive power offers/bids.
        haveQ = self._isReactiveMarket()

        # Withhold offers/bids outwith optional price limits.
        self._withholdOffbids()

        # Convert offers/bids to pwl functions and update limits.
        self._offbidToCase()

        # Compute dispatch points and LMPs using OPF.
        success = self._runOPF()

        if success:
            # Get nodal marginal prices from OPF.
            gteeOfferPrice, gteeBidPrice = self._nodalPrices(haveQ)
            # Determine quantity and price for each offer/bid.
            self._runAuction(gteeOfferPrice, gteeBidPrice, haveQ)

            logger.info("SmartMarket cleared in %.3fs" % (time.time() - t0))
        else:
            for offbid in self.offers + self.bids:
                offbid.clearedQuantity = 0.0
                offbid.clearedPrice = 0.0
                offbid.accepted = False

                offbid.generator.p = 0.0

            logger.error("Non-convergent market OPF. Blackout!")

        return self.offers, self.bids

def _runOPF(self):
        if self.decommit:
            solver = UDOPF(self.case, dc=(self.locationalAdjustment == "dc"))
        elif self.locationalAdjustment == "dc":
            solver = OPF(self.case, dc=True)
        else:
            solver = OPF(self.case, dc=False, opt={"verbose": True})

        self._solution = solver.solve()

#        for ob in self.offers + self.bids:
#            ob.f = solution["f"]

        return self._solution["converged"]

def encode(self, o):
        # This doesn't pass the iterator directly to ''.join() because it
        # sucks at reporting exceptions.  It's going to do this internally
        # anyway because it uses PySequence_Fast or similar.
        chunks = list(self.iterencode(o))
        return ''.join(chunks)

def compute_file_metrics(processors, language, key, token_list):
        This methods load the IWNLP.Lemmatizer json file and creates a dictionary
         of lowercased forms which maps each form to its possible lemmas.
            integer and given to the super class.
            environment.
    Try make a GET request with an HTTP client against a certain path and
    return once any response has been received, ignoring any errors.

    :param ContainerHttpClient client:
        The HTTP client to use to connect to the container.
    :param timeout:
        Timeout value in seconds.
    :param path:
        HTTP path to request.
    :param int expected_status_code:
        If set, wait until a response with this status code is received. If not
        set, the status code will not be checked.
    :raises TimeoutError:
        If a request fails to be made within the timeout period.
        Make a request against a container.

        :param method:
            The HTTP method to use.
        :param list path:
            The HTTP path (either absolute or relative).
        :param dict url_kwargs:
            Parameters to override in the generated URL. See `~hyperlink.URL`.
        :param kwargs:
            Any other parameters to pass to Requests.
        Sends an OPTIONS request.

        :param path:
            The HTTP path (either absolute or relative).
        :param url_kwargs:
            Parameters to override in the generated URL. See `~hyperlink.URL`.
        :param **kwargs:
            Optional arguments that ``request`` takes.
        :return: response object
        Sends a HEAD request.

        :param path:
            The HTTP path (either absolute or relative).
        :param url_kwargs:
            Parameters to override in the generated URL. See `~hyperlink.URL`.
        :param **kwargs:
            Optional arguments that ``request`` takes.
        :return: response object
        Sends a POST request.

        :param path:
            The HTTP path (either absolute or relative).
        :param url_kwargs:
            Parameters to override in the generated URL. See `~hyperlink.URL`.
        :param **kwargs:
            Optional arguments that ``request`` takes.
        :return: response object
    This function serves as a handler for the different implementations of the IUWT decomposition. It allows the
    different methods to be used almost interchangeably.

    INPUTS:

    in1                 (no default):       Array on which the decomposition is to be performed.

    scale_count         (no default):       Maximum scale to be considered.

    scale_adjust        (default=0):        Adjustment to scale value if first scales are of no interest.

    mode                (default='ser'):    Implementation of the IUWT to be used - 'ser', 'mp' or 'gpu'.

    core_count          (default=1):        Additional option for multiprocessing - specifies core count.

    store_smoothed      (default=False):    Boolean specifier for whether the smoothed image is stored or not.

    store_on_gpu        (default=False):    Boolean specifier for whether the decomposition is stored on the gpu or not.

    OUTPUTS:
    Returns the decomposition with the additional smoothed coefficients if specified.
    This function serves as a handler for the different implementations of the IUWT recomposition. It allows the
    different methods to be used almost interchangeably.

    INPUTS:

    in1                 (no default):       Array on which the decomposition is to be performed.

    scale_adjust        (no default):       Number of omitted scales.

    mode                (default='ser')     Implementation of the IUWT to be used - 'ser', 'mp' or 'gpu'.

    core_count          (default=1)         Additional option for multiprocessing - specifies core count.

    store_on_gpu        (default=False):    Boolean specifier for whether the decomposition is stored on the gpu or not.

    OUTPUTS:
    Returns the recomposition.
    This function calls the a trous algorithm code to decompose the input into its wavelet coefficients. This is
    the isotropic undecimated wavelet transform implemented for a single CPU core.

    INPUTS:

    in1                 (no default):   Array on which the decomposition is to be performed.

    scale_count         (no default):   Maximum scale to be considered.

    scale_adjust        (default=0):    Adjustment to scale value if first scales are of no interest.

    store_smoothed      (default=False):Boolean specifier for whether the smoothed image is stored or not.

    OUTPUTS:
    detail_coeffs                       Array containing the detail coefficients.
    C0                  (optional):     Array containing the smoothest version of the input.
    This function calls the a trous algorithm code to recompose the input into a single array. This is the
    implementation of the isotropic undecimated wavelet transform recomposition for a single CPU core.

    INPUTS:

    in1             (no default):   Array containing wavelet coefficients.

    scale_adjust    (no default):   Indicates the number of truncated array pages.

    smoothed_array  (default=None): For a complete inverse transform, this must be the smoothest approximation.

    OUTPUTS:
    recomposition                   Array containing the reconstructed image.
    This function calls the a trous algorithm code to recompose the input into a single array. This is the
    implementation of the isotropic undecimated wavelet transform recomposition for multiple CPU cores.

    INPUTS:

    in1             (no default):   Array containing wavelet coefficients.

    scale_adjust    (no default):   Indicates the number of omitted array pages.

    core_count      (no default):   Indicates the number of cores to be used.

    smoothed_array  (default=None): For a complete inverse transform, this must be the smoothest approximation.

    OUTPUTS:
    recomposiiton                   Array containing the reconstructed image.
    This function calls the a trous algorithm code to decompose the input into its wavelet coefficients. This is
    the isotropic undecimated wavelet transform implemented for a GPU.

    INPUTS:

    in1                 (no default):   Array on which the decomposition is to be performed.

    scale_count         (no default):   Maximum scale to be considered.

    scale_adjust        (no default):   Adjustment to scale value if first scales are of no interest.

    store_smoothed      (no default):   Boolean specifier for whether the smoothed image is stored or not.

    store_on_gpu        (no default):   Boolean specifier for whether the decomposition is stored on the gpu or not.

    OUTPUTS:
    detail_coeffs                       Array containing the detail coefficients.
    C0                  (optional):     Array containing the smoothest version of the input.
                        __global__ void gpu_store_detail_coeffs(float *in1, float *in2, float* out1, int *scale, int *adjust)
                        {
                            const int len = gridDim.x*blockDim.x;
                            const int i = (blockDim.x * blockIdx.x + threadIdx.x);
                            const int j = (blockDim.y * blockIdx.y + threadIdx.y)*len;
                            const int k = (blockDim.z * blockIdx.z + threadIdx.z)*(len*len);
                            const int tid2 = i + j;
                            const int tid3 = i + j + k;

                            if ((blockIdx.z + adjust[0])==scale[0])
                                { out1[tid3] = in1[tid2] - in2[tid2]; }

                        }
    This function calls the a trous algorithm code to recompose the input into a single array. This is the
    implementation of the isotropic undecimated wavelet transform recomposition for a GPU.

    INPUTS:

    in1             (no default):   Array containing wavelet coefficients.

    scale_adjust    (no default):   Indicates the number of omitted array pages.

    store_on_gpu    (no default):   Boolean specifier for whether the decomposition is stored on the gpu or not.

    OUTPUTS:
    recomposiiton                   Array containing the reconstructed array.
    logout and remove all session data
    display some user info to show we have authenticated successfully
    Check to see if we already have an access_key stored,
    if we do then we have already gone through
    OAuth. If not then we haven't and we probably need to.
    Iterate over items in a streaming response from the Docker client within
    a timeout.

    :param ~docker.types.daemon.CancellableStream stream:
        Stream from the Docker client to consume items from.
    :param timeout:
        Timeout value in seconds.
    :param timeout_msg:
        Message to raise in the exception when a timeout occurs.
            Get per-program state.
        Return object for given name registered in System namespace.
            Recursively cancel all threaded background processes of this Callable.
            This is called automatically for actions if program deactivates.
            Give string representation of the callable.
        Prepares the request and catches common errors and returns tuple of data and the request response.

        Read more about error codes: https://docs.polrproject.org/en/latest/developer-guide/api/#http-error-codes

        :param endpoint: full endpoint url
        :type endpoint: str
        :param params: parameters for the given endpoint
        :type params: dict
        :return: Tuple of response data, and the response instance
        :rtype: dict, requests.Response
        Creates a short url if valid

        :param str long_url: The url to shorten.
        :param custom_ending: The custom url to create if available.
        :type custom_ending: str or None
        :param bool is_secret: if not public, it's secret
        :return: a short link
        :rtype: str
        Returns the short url ending from a short url or an short url ending.

        Example:
         - Given `<your Polr server>/5N3f8`, return `5N3f8`.
         - Given `5N3f8`, return `5N3f8`.

        :param lookup_url: A short url or short url ending
        :type lookup_url: str
        :return: The url ending
        :rtype: str
        Looks up the url_ending to obtain information about the short url.

        If it exists, the API will return a dictionary with information, including
        the long_url that is the destination of the given short url URL.


        The lookup object looks like something like this:

        .. code-block:: python

            {
                'clicks': 42,
                'created_at':
                    {
                        'date': '2017-12-03 00:40:45.000000',
                        'timezone': 'UTC',
                        'timezone_type': 3
                    },
                'long_url': 'https://stackoverflow.com/questions/tagged/python',
                'updated_at':
                    {
                        'date': '2017-12-24 13:37:00.000000',
                        'timezone': 'UTC',
                        'timezone_type': 3
                    }
            }

        :param str lookup_url: An url ending or full short url address
        :param url_key: optional URL ending key for lookups against secret URLs
        :type url_key: str or None
        :return: Lookup dictionary containing, among others things, the long url; or None if not existing
        :rtype: dict or None
    Setup argparse arguments.

    :return: The parser which :class:`MypolrCli` expects parsed arguments from.
    :rtype: argparse.ArgumentParser
    This function estimates the noise using the MAD estimator.

    INPUTS:

    in1             (no default):   The array from which the noise is estimated

    OUTPUTS:
    out1                            An array of per-scale noise estimates.
    Convenience function for allocating work to cpu or gpu, depending on the selected mode.

    INPUTS:

    in1         (no default):   Array containing the wavelet decomposition.

    tolerance   (no default):   Percentage of maximum coefficient at which objects are deemed significant.

    mode        (default="cpu"):Mode of operation - either "gpu" or "cpu".

    OUTPUTS:
    Array containing the significant wavelet coefficients of extracted sources.
    The following function determines connectivity within a given wavelet decomposition. These connected and labelled
    structures are thresholded to within some tolerance of the maximum coefficient at the scale. This determines
    whether on not an object is to be considered as significant. Significant objects are extracted and factored into
    a mask which is finally multiplied by the wavelet coefficients to return only wavelet coefficients belonging to
    significant objects across all scales.

    INPUTS:

    in1         (no default):   Array containing the wavelet decomposition.

    tolerance   (no default):   Percentage of maximum coefficient at which objects are deemed significant.

    OUTPUTS:
    objects*in1                 The wavelet coefficients of the significant structures.
    objects                     The mask of the significant structures.
    The following function simply calculates the signal to noise ratio between two signals.

    INPUTS:

    in1         (no default):   Array containing values for signal 1.

    in2         (no default):   Array containing values for signal 2.

    OUTPUTS:
    out1                        The ratio of the signal to noise ratios of two signals.
        Wait for the RabbitMQ process to be come up.
        Execute a ``rabbitmqctl`` command inside a running container.

        :param command: the command to run
        :param args: a list of args for the command
        :param rabbitmqctl_opts:
            a list of extra options to pass to ``rabbitmqctl``
        :returns: a tuple of the command exit code and output
        Execute a ``rabbitmqctl`` command to list the given resources.

        :param resources: the resources to list, e.g. ``'vhosts'``
        :param args: a list of args for the command
        :param rabbitmqctl_opts:
            a list of extra options to pass to ``rabbitmqctl``
        :returns: a tuple of the command exit code and output
        Run the ``list_users`` command and return a list of tuples describing
        the users.

        :return:
            A list of 2-element tuples. The first element is the username, the
            second a list of tags for the user.
        return 'amqp://{}:{}@{}/{}'.format(
            self.user, self.password, self.name, self.vhost)

def exec_pg_success(self, cmd):
        result = self.inner().exec_run(cmd, user='postgres')
        assert result.exit_code == 0, result.output.decode('utf-8')
        return result

def clean(self):
        self.exec_pg_success(['dropdb', '-U', self.user, self.database])
        self.exec_pg_success(['createdb', '-U', self.user, self.database])

def exec_psql(self, command, psql_opts=['-qtA']):
        cmd = ['psql'] + psql_opts + [
            '--dbname', self.database,
            '-U', self.user,
            '-c', command,
        ]
        return self.inner().exec_run(cmd, user='postgres')

def list_databases(self):
        lines = output_lines(self.exec_psql('\\list'))
        return [line.split('|') for line in lines]

def list_tables(self):
        lines = output_lines(self.exec_psql('\\dt'))
        return [line.split('|') for line in lines]

def list_users(self):
        lines = output_lines(self.exec_psql('\\du'))
        return [line.split('|') for line in lines]

def database_url(self):
        return 'postgres://{}:{}@{}/{}'.format(
            self.user, self.password, self.name, self.database)

def from_config(config):
    matrix = {}
    variables = config.keys()
    for entries in product(*config.values()):
        combination = dict(zip(variables, entries))
        include = True
        for value in combination.values():
            for reducer in value.reducers:
                if reducer.pattern == '-':
                    match = not combination[reducer.variable].value
                else:
                    match = fnmatch(combination[reducer.variable].value, reducer.pattern)
                if match if reducer.is_exclude else not match:
                    include = False
        if include:
            key = '-'.join(entry.alias for entry in entries if entry.alias)
            data = dict(
                zip(variables, (entry.value for entry in entries))
            )
            if key in matrix and data != matrix[key]:
                raise DuplicateEnvironment(key, data, matrix[key])
            matrix[key] = data
    return matrix

def flush(self):
        self.logger.debug('Flush joining')
        self.queue.join()
        self.logger.debug('Flush joining ready')

def output_lines(output, encoding='utf-8', error_exc=None):
    if isinstance(output, ExecResult):
        exit_code, output = output
        if exit_code != 0 and error_exc is not None:
            raise error_exc(output.decode(encoding))

    return output.decode(encoding).splitlines()

def get_file(self, fid):
        url = self.get_file_url(fid)
        return self.conn.get_raw_data(url)

def get_file_url(self, fid, public=None):
        try:
            volume_id, rest = fid.strip().split(",")
        except ValueError:
            raise BadFidFormat(
                "fid must be in format: <volume_id>,<file_name_hash>")
        file_location = self.get_file_location(volume_id)
        if public is None:
            public = self.use_public_url
        volume_url = file_location.public_url if public else file_location.url
        url = "http://{volume_url}/{fid}".format(
            volume_url=volume_url, fid=fid)
        return url

def get_file_location(self, volume_id):
        url = ("http://{master_addr}:{master_port}/"
               "dir/lookup?volumeId={volume_id}").format(
            master_addr=self.master_addr,
            master_port=self.master_port,
            volume_id=volume_id)
        data = json.loads(self.conn.get_data(url))
        _file_location = random.choice(data['locations'])
        FileLocation = namedtuple('FileLocation', "public_url url")
        return FileLocation(_file_location['publicUrl'], _file_location['url'])

def get_file_size(self, fid):
        url = self.get_file_url(fid)
        res = self.conn.head(url)
        if res is not None:
            size = res.headers.get("content-length", None)
            if size is not None:
                return int(size)
        return None

def file_exists(self, fid):
        res = self.get_file_size(fid)
        if res is not None:
            return True
        return False

def delete_file(self, fid):
        url = self.get_file_url(fid)
        return self.conn.delete_data(url)

def upload_file(self, path=None, stream=None, name=None, **kwargs):
        params = "&".join(["%s=%s" % (k, v) for k, v in kwargs.items()])
        url = "http://{master_addr}:{master_port}/dir/assign{params}".format(
            master_addr=self.master_addr,
            master_port=self.master_port,
            params="?" + params if params else ''
        )
        data = json.loads(self.conn.get_data(url))
        if data.get("error") is not None:
            return None
        post_url = "http://{url}/{fid}".format(
            url=data['publicUrl' if self.use_public_url else 'url'],
            fid=data['fid']
        )

        if path is not None:
            filename = os.path.basename(path)
            with open(path, "rb") as file_stream:
                res = self.conn.post_file(post_url, filename, file_stream)
        # we have file like object and filename
        elif stream is not None and name is not None:
            res = self.conn.post_file(post_url, name, stream)
        else:
            raise ValueError(
                "If `path` is None then *both* `stream` and `name` must not"
                " be None ")
        response_data = json.loads(res)
        if "size" in response_data:
            return data.get('fid')
        return None

def vacuum(self, threshold=0.3):
        '''
        url = ("http://{master_addr}:{master_port}/"
               "vol/vacuum?garbageThreshold={threshold}").format(
            master_addr=self.master_addr,
            master_port=self.master_port,
            threshold=threshold)
        res = self.conn.get_data(url)
        if res is not None:
            return True
        return False

def version(self):
        '''
        url = "http://{master_addr}:{master_port}/dir/status".format(
            master_addr=self.master_addr,
            master_port=self.master_port)
        data = self.conn.get_data(url)
        response_data = json.loads(data)
        return response_data.get("Version")

def fft_convolve(in1, in2, conv_device="cpu", conv_mode="linear", store_on_gpu=False):

    # NOTE: Circular convolution assumes a periodic repetition of the input. This can cause edge effects. Linear
    # convolution pads the input with zeros to avoid this problem but is consequently heavier on computation and
    # memory.

    if conv_device=='gpu':

        if conv_mode=="linear":
            fft_in1 = pad_array(in1)
            fft_in1 = gpu_r2c_fft(fft_in1, store_on_gpu=True)
            fft_in2 = in2

            conv_in1_in2 = fft_in1*fft_in2

            conv_in1_in2 = contiguous_slice(fft_shift(gpu_c2r_ifft(conv_in1_in2, is_gpuarray=True, store_on_gpu=True)))

            if store_on_gpu:
                return conv_in1_in2
            else:
                return conv_in1_in2.get()

        elif conv_mode=="circular":
            fft_in1 = gpu_r2c_fft(in1, store_on_gpu=True)
            fft_in2 = in2

            conv_in1_in2 = fft_in1*fft_in2

            conv_in1_in2 = fft_shift(gpu_c2r_ifft(conv_in1_in2, is_gpuarray=True, store_on_gpu=True))

            if store_on_gpu:
                return conv_in1_in2
            else:
                return conv_in1_in2.get()
    else:

        if conv_mode=="linear":
            fft_in1 = pad_array(in1)
            fft_in2 = in2

            out1_slice = tuple(slice(0.5*sz,1.5*sz) for sz in in1.shape)

            return np.require(np.fft.fftshift(np.fft.irfft2(fft_in2*np.fft.rfft2(fft_in1)))[out1_slice], np.float32, 'C')

        elif conv_mode=="circular":
            return np.fft.fftshift(np.fft.irfft2(in2*np.fft.rfft2(in1)))

def gpu_r2c_fft(in1, is_gpuarray=False, store_on_gpu=False):

    if is_gpuarray:
        gpu_in1 = in1
    else:
        gpu_in1 = gpuarray.to_gpu_async(in1.astype(np.float32))

    output_size = np.array(in1.shape)
    output_size[1] = 0.5*output_size[1] + 1

    gpu_out1 = gpuarray.empty([output_size[0], output_size[1]], np.complex64)
    gpu_plan = Plan(gpu_in1.shape, np.float32, np.complex64)
    fft(gpu_in1, gpu_out1, gpu_plan)

    if store_on_gpu:
        return gpu_out1
    else:
        return gpu_out1.get()

def gpu_c2r_ifft(in1, is_gpuarray=False, store_on_gpu=False):

    if is_gpuarray:
        gpu_in1 = in1
    else:
        gpu_in1 = gpuarray.to_gpu_async(in1.astype(np.complex64))

    output_size = np.array(in1.shape)
    output_size[1] = 2*(output_size[1]-1)

    gpu_out1 = gpuarray.empty([output_size[0],output_size[1]], np.float32)
    gpu_plan = Plan(output_size, np.complex64, np.float32)
    ifft(gpu_in1, gpu_out1, gpu_plan)
    scale_fft(gpu_out1)

    if store_on_gpu:
        return gpu_out1
    else:
        return gpu_out1.get()

def pad_array(in1):

    padded_size = 2*np.array(in1.shape)

    out1 = np.zeros([padded_size[0],padded_size[1]])
    out1[padded_size[0]/4:3*padded_size[0]/4,padded_size[1]/4:3*padded_size[1]/4] = in1

    return out1

def is_dragon(host, timeout=1):
        try:
            r = requests.get('http://{}/'.format(host), timeout=timeout)
            if r.status_code == 200:
                if '<title>DragonMint</title>' in r.text or \
                        '<title>AsicMiner</title>' in r.text:
                    return True
        except requests.exceptions.RequestException:
            pass
        return False

def updatePools(self,
                    pool1,
                    username1,
                    password1,
                    pool2=None,
                    username2=None,
                    password2=None,
                    pool3=None,
                    username3=None,
                    password3=None):
        return self.__post('/api/updatePassword',
                           data={
                               'user': user,
                               'currentPassword': currentPassword,
                               'newPassword': newPassword
                           })

def updateNetwork(self,
                      dhcp='dhcp',
                      ipaddress=None,
                      netmask=None,
                      gateway=None,
                      dns=None):
        files = {'upfile': open(file, 'rb')}
        return self.__post_files('/upgrade/upload',
                                 files=files)

def is_program(self):
        from automate.callables import Empty
        return not (isinstance(self.on_activate, Empty)
                    and isinstance(self.on_deactivate, Empty)
                    and isinstance(self.on_update, Empty))

def get_as_datadict(self):
        d = super().get_as_datadict()
        d.update(dict(status=self.status, data_type=self.data_type, editable=self.editable))
        return d

def _do_change_status(self, status, force=False):
        self.system.worker_thread.put(DummyStatusWorkerTask(self._request_status_change_in_queue, status, force=force))

def activate_program(self, program):
        self.logger.debug("activate_program %s", program)
        if program in self.program_stack:
            return

        with self._program_lock:
            self.logger.debug("activate_program got through %s", program)
            self.program_stack.append(program)
            self._update_program_stack()

def deactivate_program(self, program):
        self.logger.debug("deactivate_program %s", program)

        with self._program_lock:
            self.logger.debug("deactivate_program got through %s", program)
            if program not in self.program_stack:
                import ipdb
                ipdb.set_trace()
            self.program_stack.remove(program)
            if program in self.program_status:
                del self.program_status[program]
            self._update_program_stack()

def stream_logs(container, timeout=10.0, **logs_kwargs):
    stream = container.logs(stream=True, **logs_kwargs)
    return stream_timeout(
        stream, timeout, 'Timeout waiting for container logs.')

def fetch_image(client, name):
    try:
        image = client.images.get(name)
    except docker.errors.ImageNotFound:
        name, tag = _parse_image_tag(name)
        tag = 'latest' if tag is None else tag

        log.info("Pulling tag '{}' for image '{}'...".format(tag, name))
        image = client.images.pull(name, tag=tag)

    log.debug("Found image '{}' for tag '{}'".format(image.id, name))
    return image

def _get_id_and_model(self, id_or_model):
        if isinstance(id_or_model, self.collection.model):
            model = id_or_model
        elif isinstance(id_or_model, str):
            # Assume we have an ID string
            model = self.collection.get(id_or_model)
        else:
            raise TypeError('Unexpected type {}, expected {} or {}'.format(
                type(id_or_model), str, self.collection.model))

        return model.id, model

def create(self, name, *args, **kwargs):
        resource_name = self._resource_name(name)
        log.info(
            "Creating {} '{}'...".format(self._model_name, resource_name))
        resource = self.collection.create(*args, name=resource_name, **kwargs)
        self._ids.add(resource.id)
        return resource

def remove(self, resource, **kwargs):
        log.info(
            "Removing {} '{}'...".format(self._model_name, resource.name))
        resource.remove(**kwargs)
        self._ids.remove(resource.id)

def remove(self, container, force=True, volumes=True):
        super().remove(container, force=force, v=volumes)

def get_default(self, create=True):

        if self._default_network is None and create:

            log.debug("Creating default network...")

            self._default_network = self.create('default', driver='bridge')


        return self._default_network

def _helper_for_model(self, model_type):
        if model_type is models.containers.Container:
            return self.containers
        if model_type is models.images.Image:
            return self.images
        if model_type is models.networks.Network:
            return self.networks
        if model_type is models.volumes.Volume:
            return self.volumes

        raise ValueError('Unknown model type {}'.format(model_type))

def teardown(self):
        self.containers._teardown()
        self.networks._teardown()
        self.volumes._teardown()

        # We need to close the underlying APIClient explicitly to avoid
        # ResourceWarnings from unclosed HTTP connections.
        self._client.api.close()

def exec_redis_cli(self, command, args=[], db=0, redis_cli_opts=[]):
        cli_opts = ['-n', str(db)] + redis_cli_opts
        cmd = ['redis-cli'] + cli_opts + [command] + [str(a) for a in args]
        return self.inner().exec_run(cmd)

def list_keys(self, pattern='*', db=0):
        lines = output_lines(self.exec_redis_cli('KEYS', [pattern], db=db))
        return [] if lines == [''] else lines

def threaded(system, func, *args, **kwargs):
        Return ``True`` if the expected matchers are matched in the expected
        order, otherwise ``False``.
        Return ``True`` if the expected matchers are matched in any order,
        otherwise ``False``.
        Extension of the MORESANE algorithm. This takes a scale-by-scale approach, attempting to remove all sources
        at the lower scales before moving onto the higher ones. At each step the algorithm may return to previous
        scales to remove the sources uncovered by the deconvolution.

        INPUTS:

        start_scale         (default=1)         The first scale which is to be considered.

        stop_scale          (default=20)        The maximum scale which is to be considered. Optional.

        subregion           (default=None):     Size, in pixels, of the central region to be analyzed and deconvolved.

        sigma_level         (default=4)         Number of sigma at which thresholding is to be performed.

        loop_gain           (default=0.1):      Loop gain for the deconvolution.

        tolerance           (default=0.75):     Tolerance level for object extraction. Significant objects contain
                                                wavelet coefficients greater than the tolerance multiplied by the
                                                maximum wavelet coefficient in the scale under consideration.

        accuracy            (default=1e-6):     Threshold on the standard deviation of the residual noise. Exit main
                                                loop when this threshold is reached.

        major_loop_miter    (default=100):      Maximum number of iterations allowed in the major loop. Exit
                                                condition.

        minor_loop_miter    (default=30):       Maximum number of iterations allowed in the minor loop. Serves as an
                                                exit condition when the SNR does not reach a maximum.

        all_on_gpu          (default=False):    Boolean specifier to toggle all gpu modes on.

        decom_mode          (default='ser'):    Specifier for decomposition mode - serial, multiprocessing, or gpu.

        core_count          (default=1):        In the event that multiprocessing, specifies the number of cores.

        conv_device         (default='cpu'):    Specifier for device to be used - cpu or gpu.

        conv_mode           (default='linear'): Specifier for convolution mode - linear or circular.

        extraction_mode     (default='cpu'):    Specifier for mode to be used - cpu or gpu.

        enforce_positivity  (default=False):    Boolean specifier for whether or not a model must be strictly positive.

        edge_suppression    (default=False):    Boolean specifier for whether or not the edges are to be suprressed.

        edge_offset         (default=0):        Numeric value for an additional user-specified number of edge pixels
                                                to be ignored. This is added to the minimum suppression.

        OUTPUTS:

        self.model          (no default):       Model extracted by the algorithm.

        self.residual       (no default):       Residual signal after deconvolution.
        This method constructs the restoring beam and then adds the convolution to the residual.
        This method tries to ensure that the input data has the correct dimensions.

        INPUTS:

        input_hdr   (no default)    Header from which data shape is to be extracted.
        This method simply saves the model components and the residual.

        INPUTS:

        data    (no default)    Data which is to be saved.

        name    (no default)    File name for new .fits file. Will overwrite.
        Convenience function which creates a logger for the module.

        INPUTS:

        level   (default="INFO"):   Minimum log level for logged/streamed messages.

        OUTPUTS:
        logger                      Logger for the function. NOTE: Must be bound to variable named logger.
            Start Text UI main loop

        Return dict of header to be used in requests.

        Args:
            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            Headers dict. Key and values are string

        on provided url

        Returns content under the provided url as text

        Args:
            **url**: address of the wanted data

            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            string


        Returns content under the provided url as bytes
        ie. for binary data

        Args:
            **url**: address of the wanted data

            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            bytes


        Returns contents as text

        Args:
            **url**: address where to upload file

            **filename**: Name of the uploaded file

            **file_stream**: file like object to upload

            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            string

        Returns status as boolean.

        Args:
            **url**: address of file to be deleted

            .. versionadded:: 0.3.2
                **additional_headers**: (optional) Additional headers
                to be used with request

        Returns:
            Boolean. True if request was successful. False if not.
    Given a collection of Unicode diacritics, return a function that takes a
    string and returns the string without those diacritics.
    Recursively merge all input dicts into a single dict.

        Create an instance of this resource definition.

        Only one instance may exist at any given time.

        Remove an instance of this resource definition.
        Setup this resource so that is ready to be used in a test. If the
        resource has already been created, this call does nothing.

        For most resources, this just involves creating the resource in Docker.

        :param helper:
            The resource helper to use, if one was not provided when this

            resource definition was created.
        :param **create_kwargs: Keyword arguments passed to :meth:`.create`.

        :returns:

            This definition instance. Useful for creating and setting up a
            resource in a single step::

                volume = VolumeDefinition('volly').setup(helper=docker_helper)
        A decorator to inject this container into a function as a test fixture.
        Creates the container, starts it, and waits for it to completely start.

        :param helper:
            The resource helper to use, if one was not provided when this

            container definition was created.
        :param **run_kwargs: Keyword arguments passed to :meth:`.run`.

        :returns:

            This container definition instance. Useful for creating and setting
            up a container in a single step::

                con = ContainerDefinition('conny', 'nginx').setup(helper=dh)
        Stop and remove the container if it exists.
        Get the container's current status from Docker.

        If the container does not exist (before creation and after removal),
        the status is ``None``.
        Stop the container. The container must have been created.

        :param timeout:
            Timeout in seconds to wait for the container to stop before sending

            a ``SIGKILL``. Default: 5 (half the Docker default)
        Create the container and start it. Similar to ``docker run``.

        :param fetch_image:
            Whether to try pull the image if it's not found. The behaviour here

            is similar to ``docker run`` and this parameter defaults to
            ``True``.
        :param **kwargs: Keyword arguments passed to :meth:`.create`.
        Wait for the container to start.


        By default this will wait for the log lines matching the patterns
        passed in the ``wait_patterns`` parameter of the constructor using an
        UnorderedMatcher. For more advanced checks for container startup, this
        method should be overridden.
        Get container logs.

        This method does not support streaming, use :meth:`stream_logs` for
        that.
        Stream container output.
        Wait for logs matching the given matcher.
        Construct an HTTP client for this container.

    # Extract the arguments needed from the handler.
    args = self.argument_transform(object, trait_name, old, new)

    # Send a description of the event to the change event tracer.
    if tnotifier._pre_change_event_tracer is not None:
        tnotifier._pre_change_event_tracer(object, trait_name, old, new, handler)

    # Dispatch the event to the listener.
    from automate.common import SystemNotReady
    try:
        self.dispatch(handler, *args)
    except SystemNotReady:
        pass
    except Exception as e:
        if tnotifier._post_change_event_tracer is not None:
            tnotifier._post_change_event_tracer(object, trait_name, old, new,
                                                handler, exception=e)
        # This call needs to be made inside the `except` block in case
        # the handler wants to re-raise the exception.
        tnotifier.handle_exception(object, trait_name, old, new)
    else:
        if tnotifier._post_change_event_tracer is not None:
            tnotifier._post_change_event_tracer(object, trait_name, old, new,
                                                handler, exception=None)

def split(value, precision=1):
    '''
    negative = False
    digits = precision + 1

    if value < 0.:
        value = -value
        negative = True
    elif value == 0.:
        return 0., 0

    expof10 = int(math.log10(value))
    if expof10 > 0:
        expof10 = (expof10 // 3) * 3
    else:
        expof10 = (-expof10 + 3) // 3 * (-3)

    value *= 10 ** (-expof10)

    if value >= 1000.:
        value /= 1000.0
        expof10 += 3
    elif value >= 100.0:
        digits -= 2
    elif value >= 10.0:
        digits -= 1

    if negative:
        value *= -1

    return value, int(expof10)

def si_format(value, precision=1, format_str=u'{value} {prefix}',
              exp_format_str=u'{value}e{expof10}'):
    '''
    svalue, expof10 = split(value, precision)
    value_format = u'%%.%df' % precision
    value_str = value_format % svalue
    try:
        return format_str.format(value=value_str,
                                 prefix=prefix(expof10).strip())
    except ValueError:
        sign = ''
        if expof10 > 0:
            sign = "+"
        return exp_format_str.format(value=value_str,
                                     expof10=''.join([sign, str(expof10)]))

def si_parse(value):
    '''
    CRE_10E_NUMBER = re.compile(r'^\s*(?P<integer>[\+\-]?\d+)?'
                                r'(?P<fraction>.\d+)?\s*([eE]\s*'
                                r'(?P<expof10>[\+\-]?\d+))?$')
    CRE_SI_NUMBER = re.compile(r'^\s*(?P<number>(?P<integer>[\+\-]?\d+)?'
                               r'(?P<fraction>.\d+)?)\s*'
                               u'(?P<si_unit>[%s])?\s*$' % SI_PREFIX_UNITS)
    match = CRE_10E_NUMBER.match(value)
    if match:
        # Can be parse using `float`.
        assert(match.group('integer') is not None or
               match.group('fraction') is not None)
        return float(value)
    match = CRE_SI_NUMBER.match(value)
    assert(match.group('integer') is not None or
           match.group('fraction') is not None)
    d = match.groupdict()
    si_unit = d['si_unit'] if d['si_unit'] else ' '
    prefix_levels = (len(SI_PREFIX_UNITS) - 1) // 2
    scale = 10 ** (3 * (SI_PREFIX_UNITS.index(si_unit) - prefix_levels))
    return float(d['number']) * scale

def set_status(self, name, status):
        getattr(self.system, name).status = status
        return True

def toggle_object_status(self, objname):
        o = getattr(self.system, objname)
        o.status = not o.status
        self.system.flush()
        return o.status

def log(self):
        logserv = self.system.request_service('LogStoreService')
        return logserv.lastlog(html=False)

def load_or_create(cls, filename=None, no_input=False, create_new=False, **kwargs):
        parser = argparse.ArgumentParser()
        parser.add_argument('--no_input', action='store_true')
        parser.add_argument('--create_new', action='store_true')
        args = parser.parse_args()

        if args.no_input:
            print('Parameter --no_input was given')
            no_input = True
        if args.create_new:
            print('Parameter --create_new was given')
            create_new = True
            no_input = True


        def savefile_more_recent():
            time_savefile = os.path.getmtime(filename)
            time_program = os.path.getmtime(sys.argv[0])
            return time_savefile > time_program


        def load_pickle():
            with open(filename, 'rb') as of:
                statefile_version, data = pickle.load(of)

            if statefile_version != STATEFILE_VERSION:
                raise RuntimeError(f'Wrong statefile version, please remove state file {filename}')
            return data


        def load():
            print('Loading %s' % filename)
            obj_list, config = load_pickle()
            system = System(load_state=obj_list, filename=filename, **kwargs)

            return system


        def create():
            print('Creating new system')
            config = None
            if filename:
                try:
                    obj_list, config = load_pickle()
                except FileNotFoundError:
                    config = None
            return cls(filename=filename, load_config=config, **kwargs)

        if filename and os.path.isfile(filename):
            if savefile_more_recent() and not create_new:
                return load()
            else:
                if no_input:
                    print('Program file more recent. Loading that instead.')
                    return create()
                while True:
                    answer = input('Program file more recent. Do you want to load it? (y/n) ')
                    if answer == 'y':
                        return create()
                    elif answer == 'n':
                        return load()
        else:
            return create()

def cmd_namespace(self):
        import automate
        ns = dict(list(automate.__dict__.items()) + list(self.namespace.items()))
        return ns

def services_by_name(self):

        srvs = defaultdict(list)
        for i in self.services:
            srvs[i.__class__.__name__].append(i)
        return srvs

def name_to_system_object(self, name):
        if isinstance(name, str):
            if self.allow_name_referencing:
                name = name
            else:
                raise NameError('System.allow_name_referencing is set to False, cannot convert string to name')
        elif isinstance(name, Object):
            name = str(name)
        return self.namespace.get(name, None)

def register_service_functions(self, *funcs):
        for func in funcs:
            self.namespace[func.__name__] = func

def register_service(self, service):
        if service not in self.services:
            self.services.append(service)

def cleanup(self):

        self.pre_exit_trigger = True

        self.logger.info("Shutting down %s, please wait a moment.", self.name)
        for t in threading.enumerate():
            if isinstance(t, TimerClass):
                t.cancel()
        self.logger.debug('Timers cancelled')

        for i in self.objects:
            i.cleanup()

        self.logger.debug('Sensors etc cleanups done')

        for ser in (i for i in self.services if isinstance(i, AbstractUserService)):
            ser.cleanup_system()
        self.logger.debug('User services cleaned up')
        if self.worker_thread.is_alive():
            self.worker_thread.stop()
        self.logger.debug('Worker thread really stopped')

        for ser in (i for i in self.services if isinstance(i, AbstractSystemService)):
            ser.cleanup_system()
        self.logger.debug('System services cleaned up')
        threads = list(t.name for t in threading.enumerate() if t.is_alive() and not t.daemon)
        if threads:
            self.logger.info('After cleanup, we have still the following threads '
                             'running: %s', ', '.join(threads))

def cmd_exec(self, cmd):

        if not cmd:
            return
        ns = self.cmd_namespace
        import copy
        rval = True
        nscopy = copy.copy(ns)
        try:
            r = eval(cmd, ns)
            if isinstance(r, SystemObject) and not r.system:
                r.setup_system(self)
            if callable(r):
                r = r()
                cmd += "()"
            self.logger.info("Eval: %s", cmd)
            self.logger.info("Result: %s", r)
        except SyntaxError:
            r = {}
            try:
                exec (cmd, ns)
                self.logger.info("Exec: %s", cmd)
            except ExitException:
                raise
            except Exception as e:
                self.logger.info("Failed to exec cmd %s: %s.", cmd, e)
                rval = False
            for key, value in list(ns.items()):
                if key not in nscopy or not value is nscopy[key]:
                    if key in self.namespace:
                        del self.namespace[key]
                    self.namespace[key] = value
                    r[key] = value
            self.logger.info("Set items in namespace: %s", r)
        except ExitException:
            raise
        except Exception as e:
            self.logger.info("Failed to eval cmd %s: %s", cmd, e)
            return False

        return rval

def write_puml(self, filename=''):

        def get_type(o):
            type = 'program'
            if isinstance(o, AbstractSensor):
                type = 'sensor'
            elif isinstance(o, AbstractActuator):
                type = 'actuator'
            return type

        if filename:
            s = open(filename, 'w')
        else:
            s = io.StringIO()
        s.write('@startuml\n')
        s.write('skinparam state {\n')
        for k, v in list(self.background_colors.items()):
            s.write('BackGroundColor<<%s>> %s\n' % (k, v))
        s.write('}\n')

        for o in self.system.objects:
            if isinstance(o, DefaultProgram) or o.hide_in_uml:
                continue

            if isinstance(o, ProgrammableSystemObject):
                s.write('state "%s" as %s <<%s>>\n' % (o, o, get_type(o)))

                s.write('%s: %s\n' % (o, o.class_name))
                if isinstance(o, AbstractActuator):
                    for p in reversed(o.program_stack):
                        s.write('%s: %s :: %s\n' % (o, p, o.program_status.get(p, '-')))
                elif hasattr(o, 'status'):
                    s.write('%s: Status: %s\n' % (o, o.status))
                if getattr(o, 'is_program', False):
                    s.write('%s: Priority: %s\n' % (o, o.priority))

                for t in o.actual_triggers:
                    if isinstance(t, DefaultProgram) or t.hide_in_uml:
                        continue
                    s.write('%s -[%s]-> %s\n' % (t, self.arrow_colors['trigger'], o))
                for t in o.actual_targets:
                    if t.hide_in_uml:
                        continue
                    if o.active:
                        color = 'active_target'
                    else:
                        color = 'inactive_target'
                    if getattr(t, 'program', None) == o:
                        color = 'controlled_target'

                    s.write('%s -[%s]-> %s\n' % (o, self.arrow_colors[color], t))
        s.write('@enduml\n')
        if filename:
            s.close()
        else:
            return s.getvalue()

def write_svg(self):
        import plantuml
        puml = self.write_puml()
        server = plantuml.PlantUML(url=self.url)
        svg = server.processes(puml)
        return svg

def median_kneighbour_distance(X, k=5):
    N_all = X.shape[0]
    k = min(k, N_all)
    N_subset = min(N_all, 2000)
    sample_idx_train = np.random.permutation(N_all)[:N_subset]
    nn = neighbors.NearestNeighbors(k)
    nn.fit(X[sample_idx_train, :])
    d, idx = nn.kneighbors(X[sample_idx_train, :])
    return np.median(d[:, -1])

def pair_distance_centile(X, centile, max_pairs=5000):
    N = X.shape[0]
    n_pairs = min(max_pairs, N**2)
    # randorder1 = np.random.permutation(N)
    # randorder2 = np.random.permutation(N)

    dists = np.zeros(n_pairs)

    for i in range(n_pairs):
        pair = np.random.randint(0, N, 2)
        pairdiff = X[pair[0], :]-X[pair[1], :]
        dists[i] = np.dot(pairdiff, pairdiff.T)
    dists.sort()

    out = dists[int(n_pairs*centile/100.)]
    return np.sqrt(out)

def fit(self, X, y=None):
        N = X.shape[0]

        if y is None:
            y = np.zeros(N)

        self.classes = list(set(y))
        self.classes.sort()
        self.n_classes = len(self.classes)


        # If no kernel parameters specified, try to choose some defaults
        if not self.sigma:
            self.sigma = median_kneighbour_distance(X)
            self.gamma = self.sigma**-2

        if not self.gamma:
            self.gamma = self.sigma**-2

        if not self.rho:
            self.rho = 0.1

        # choose kernel basis centres
        if self.kernel_pos is None:
            B = min(self.n_kernels_max, N)
            kernel_idx = np.random.permutation(N)
            self.kernel_pos = X[kernel_idx[:B]]
        else:
            B = self.kernel_pos.shape[0]

        # fit coefficients
        Phi = metrics.pairwise.rbf_kernel(X, self.kernel_pos, self.gamma)
        theta = {}
        Phi_PhiT = np.dot(Phi.T, Phi)
        inverse_term = np.linalg.inv(Phi_PhiT + self.rho*np.eye(B))
        for c in self.classes:
            m = (y == c).astype(int)
            theta[c] = np.dot(inverse_term, np.dot(Phi.T, m))

        self.theta = theta

def predict(self, X):
        predictions_proba = self.predict_proba(X)
        predictions = []
        allclasses = copy.copy(self.classes)
        allclasses.append('anomaly')
        for i in range(X.shape[0]):
            predictions.append(allclasses[predictions_proba[i, :].argmax()])
        return predictions

def predict_proba(self, X):
        Phi = metrics.pairwise.rbf_kernel(X, self.kernel_pos, self.gamma)
        N = X.shape[0]
        predictions = np.zeros((N, self.n_classes+1))
        for i in range(N):
            post = np.zeros(self.n_classes)
            for c in range(self.n_classes):
                post[c] = max(0,
                              np.dot(self.theta[self.classes[c]].T, Phi[i, :]))
                post[c] = min(post[c], 1.)
            predictions[i, :-1] = post
            predictions[i, -1] = max(0, 1-sum(post))

        return predictions

def decision_function(self, X):
        predictions = self.predict_proba(X)
        out = np.zeros((predictions.shape[0], 1))
        out[:, 0] = 1 - predictions[:, -1]
        return out

def score(self, X, y):
        predictions = self.predict(X)
        true = 0.0
        total = 0.0
        for i in range(len(predictions)):
            total += 1
            if predictions[i] == y[i]:
                true += 1
        return true/total

def predict_sequence(self, X, A, pi, inference='smoothing'):
        obsll = self.predict_proba(X)
        T, S = obsll.shape
        alpha = np.zeros((T, S))

        alpha[0, :] = pi
        for t in range(1, T):
            alpha[t, :] = np.dot(alpha[t-1, :], A)
            for s in range(S):
                alpha[t, s] *= obsll[t, s]
            alpha[t, :] = alpha[t, :]/sum(alpha[t, :])

        if inference == 'filtering':
            return alpha
        else:
            beta = np.zeros((T, S))
            gamma = np.zeros((T, S))
            beta[T-1, :] = np.ones(S)
            for t in range(T-2, -1, -1):
                for i in range(S):
                    for j in range(S):
                        beta[t, i] += A[i, j]*obsll[t+1, j]*beta[t+1, j]
                beta[t, :] = beta[t, :]/sum(beta[t, :])

            for t in range(T):
                gamma[t, :] = alpha[t, :]*beta[t, :]
                gamma[t, :] = gamma[t, :]/sum(gamma[t, :])

            return gamma

def toggle_sensor(request, sensorname):
    if service.read_only:
        service.logger.warning("Could not perform operation: read only mode enabled")
        raise Http404
    source = request.GET.get('source', 'main')
    sensor = service.system.namespace[sensorname]
    sensor.status = not sensor.status
    service.system.flush()
    return HttpResponseRedirect(reverse(source))

def toggle_value(request, name):
    obj = service.system.namespace.get(name, None)
    if not obj or service.read_only:
        raise Http404
    new_status = obj.status = not obj.status
    if service.redirect_from_setters:
        return HttpResponseRedirect(reverse('set_ready', args=(name, new_status)))
    else:
        return set_ready(request, name, new_status)

def set_value(request, name, value):
    obj = service.system.namespace.get(name, None)
    if not obj or service.read_only:
        raise Http404
    obj.status = value
    if service.redirect_from_setters:
        return HttpResponseRedirect(reverse('set_ready', args=(name, value)))
    else:
        return set_ready(request, name, value)

def object_type(self):

        from .statusobject import AbstractSensor, AbstractActuator
        from .program import Program
        if isinstance(self, AbstractSensor):
            return 'sensor'
        elif isinstance(self, AbstractActuator):
            return 'actuator'
        elif isinstance(self, Program):
            return 'program'
        else:
            return 'other'

def get_as_datadict(self):
        return dict(type=self.__class__.__name__, tags=list(self.tags))

def setup_system(self, system, name_from_system='', **kwargs):

        if not self.system:
            self.system = system
        name, traits = self._passed_arguments
        new_name = self.system.get_unique_name(self, name, name_from_system)
        if not self in self.system.reverse:
            self.name = new_name
        self.logger = self.system.logger.getChild('%s.%s' % (self.__class__.__name__, self.name))
        self.logger.setLevel(self.log_level)
        if name is None and 'name' in traits:  # Only __setstate__ sets name to None. Default is ''.
            del traits['name']

        for cname in self.callables:
            if cname in traits:
                c = self._postponed_callables[cname] = traits.pop(cname)
                c.setup_callable_system(self.system)
            getattr(self, cname).setup_callable_system(self.system)

        if not self.traits_inited():
            super().__init__(**traits)
        self.name_changed_event = True
        self.setup()

def setup_callables(self):

        defaults = self.get_default_callables()

        for key, value in list(defaults.items()):

            self._postponed_callables.setdefault(key, value)
        for key in self.callables:
            value = self._postponed_callables.pop(key)
            value.setup_callable_system(self.system, init=True)
            setattr(self, key, value)

def grab_keyfile(cert_url):

    key_cache = caches[getattr(settings, 'BOUNCY_KEY_CACHE', 'default')]

    pemfile = key_cache.get(cert_url)
    if not pemfile:
        response = urlopen(cert_url)
        pemfile = response.read()
        # Extract the first certificate in the file and confirm it's a valid
        # PEM certificate
        certificates = pem.parse(smart_bytes(pemfile))

        # A proper certificate file will contain 1 certificate
        if len(certificates) != 1:
            logger.error('Invalid Certificate File: URL %s', cert_url)
            raise ValueError('Invalid Certificate File')

        key_cache.set(cert_url, pemfile)
    return pemfile

def verify_notification(data):
    pemfile = grab_keyfile(data['SigningCertURL'])
    cert = crypto.load_certificate(crypto.FILETYPE_PEM, pemfile)
    signature = base64.decodestring(six.b(data['Signature']))

    if data['Type'] == "Notification":
        hash_format = NOTIFICATION_HASH_FORMAT
    else:
        hash_format = SUBSCRIPTION_HASH_FORMAT

    try:
        crypto.verify(
            cert, signature, six.b(hash_format.format(**data)), 'sha1')
    except crypto.Error:
        return False
    return True

def approve_subscription(data):
    url = data['SubscribeURL']

    domain = urlparse(url).netloc
    pattern = getattr(
        settings,
        'BOUNCY_SUBSCRIBE_DOMAIN_REGEX',
        r"sns.[a-z0-9\-]+.amazonaws.com$"
    )
    if not re.search(pattern, domain):
        logger.error('Invalid Subscription Domain %s', url)
        return HttpResponseBadRequest('Improper Subscription Domain')

    try:
        result = urlopen(url).read()
        logger.info('Subscription Request Sent %s', url)
    except urllib.HTTPError as error:
        result = error.read()
        logger.warning('HTTP Error Creating Subscription %s', str(result))

    signals.subscription.send(
        sender='bouncy_approve_subscription',
        result=result,
        notification=data
    )

    # Return a 200 Status Code
    return HttpResponse(six.u(result))

def clean_time(time_string):
    for a given model class.

        This is a wrapper of the to_html method of the pandas dataframe.

        :param bool index: do not include the index
        :param bool escape: do not escape special characters
        :param bool header: include header
        :param bool collapse_table: long tables are shorten with a scroll bar
        :param kargs: any parameter accepted by
            :meth:`pandas.DataFrame.to_html`


        :param colname:
        :param cmap: a colormap (matplotlib) or created using
            colormap package (from pypi).
        :param mode: type of normalisation in 'absmax', 'max', 'clip'
            (see details below)
        :param threshold: used if mode is set to 'clip'

        Colormap have values between 0 and 1 so we need to normalised the data
        between 0 and 1. There are 3 mode to normalise the data so far.

        If mode is set to 'absmax', negatives and positives values are
        expected to be found in a range from -inf to inf. Values are
        scaled in between [0,1] X' = (X / M +1) /2. where m is the absolute
        maximum. Ideally a colormap should be made of 3 colors, the first
        color used for negative values, the second for zeros and third color
        for positive values.

        If mode is set to 'clip', values are clipped to a max value (parameter
        *threshold* and values are normalised by that same threshold.

        If mode is set to 'max', values are normalised by the max.

        extra_context = extra_context or {}
        if 'object' in request.GET.keys():
            value = request.GET['object'].split(':')
            content_type = get_object_or_404(
                ContentType,
                id=value[0],
            )
            tracked_object = get_object_or_404(
                content_type.model_class(),
                id=value[1],
            )
            extra_context['tracked_object'] = tracked_object
            extra_context['tracked_object_opts'] = tracked_object._meta
        return super(TrackingEventAdmin, self).changelist_view(
            request, extra_context)

def list(self, filter=None, type=None, sort=None, limit=None, page=None): # pylint: disable=redefined-builtin
        schema = self.LIST_SCHEMA
        resp = self.service.list(self.base, filter, type, sort, limit, page)
        cs, l = self.service.decode(schema, resp, many=True, links=True)
        return Page(cs, l)

def iter_list(self, *args, **kwargs):
        return self.service.iter_list(self.list, *args, **kwargs)

def get_plaintext(self, id): # pylint: disable=invalid-name,redefined-builtin
        return self.service.get_id(self.base, id, params={'format': 'text'}).text

def create(self, resource):
        schema = self.CREATE_SCHEMA
        json = self.service.encode(schema, resource)

        schema = self.GET_SCHEMA
        resp = self.service.create(self.base, json)
        return self.service.decode(schema, resp)

def edit(self, resource):
        schema = self.EDIT_SCHEMA
        json = self.service.encode(schema, resource)

        schema = self.GET_SCHEMA
        resp = self.service.edit(self.base, resource.id, json)
        return self.service.decode(schema, resp)

def edit_shares(self, id, user_ids): # pylint: disable=invalid-name,redefined-builtin
        return self.service.edit_shares(self.base, id, user_ids)

def check_config(self, contents):
        schema = CheckConfigSchema()
        resp = self.service.post(self.base,
                                 params={'process': 'check'}, json={'contents': contents})
        return self.service.decode(schema, resp)

def upgrade_config(self, contents):
        schema = UpgradeConfigSchema()
        resp = self.service.post(self.base,
                                 params={'process': 'upgrade'}, json={'contents': contents})
        return self.service.decode(schema, resp)

def get_networks(self, contents):
        schema = NetworksSchema()
        resp = self.service.post(self.base,
                                 params={'process': 'networks'}, json={'contents': contents})
        return self.service.decode(schema, resp)

def bulk_copy(self, ids):
        schema = self.GET_SCHEMA
        return self.service.bulk_copy(self.base, self.RESOURCE, ids, schema)

def bulk_edit(self, _fields, ids=None, filter=None, type=None, all=False, testvars=None): # pylint: disable=redefined-builtin
        schema = self.EDIT_SCHEMA
        _fields = self.service.encode(schema, _fields, skip_none=True)
        return self.service.bulk_edit(self.base, self.RESOURCE,
                                      _fields, ids=ids, filter=filter, type=type, all=all, testvars=testvars)

def bulk_delete(self, ids=None, filter=None, type=None, all=False): # pylint: disable=redefined-builtin
        return self.service.bulk_delete(self.base, self.RESOURCE,
                                        ids=ids, filter=filter, type=type, all=all)

def response_token_setter(remote, resp):
    if resp is None:
        raise OAuthRejectedRequestError('User rejected request.', remote, resp)
    else:
        if 'access_token' in resp:
            return oauth2_token_setter(remote, resp)
        elif 'oauth_token' in resp and 'oauth_token_secret' in resp:
            return oauth1_token_setter(remote, resp)
        elif 'error' in resp:
            # Only OAuth2 specifies how to send error messages
            raise OAuthClientError(
                'Authorization with remote service failed.', remote, resp,
            )
    raise OAuthResponseError('Bad OAuth authorized request', remote, resp)

def oauth1_token_setter(remote, resp, token_type='', extra_data=None):
    return token_setter(
        remote,
        resp['oauth_token'],
        secret=resp['oauth_token_secret'],
        extra_data=extra_data,
        token_type=token_type,
    )

def oauth2_token_setter(remote, resp, token_type='', extra_data=None):
    return token_setter(
        remote,
        resp['access_token'],
        secret='',
        token_type=token_type,
        extra_data=extra_data,
    )

def token_setter(remote, token, secret='', token_type='', extra_data=None,
                 user=None):
    session[token_session_key(remote.name)] = (token, secret)
    user = user or current_user

    # Save token if user is not anonymous (user exists but can be not active at
    # this moment)
    if not user.is_anonymous:
        uid = user.id
        cid = remote.consumer_key

        # Check for already existing token
        t = RemoteToken.get(uid, cid, token_type=token_type)

        if t:
            t.update_token(token, secret)
        else:
            t = RemoteToken.create(
                uid, cid, token, secret,
                token_type=token_type, extra_data=extra_data
            )
        return t
    return None

def token_getter(remote, token=''):
    session_key = token_session_key(remote.name)

    if session_key not in session and current_user.is_authenticated:
        # Fetch key from token store if user is authenticated, and the key
        # isn't already cached in the session.
        remote_token = RemoteToken.get(
            current_user.get_id(),
            remote.consumer_key,
            token_type=token,
        )

        if remote_token is None:
            return None

        # Store token and secret in session
        session[session_key] = remote_token.token()

    return session.get(session_key, None)

def token_delete(remote, token=''):
    session_key = token_session_key(remote.name)
    return session.pop(session_key, None)

def oauth_error_handler(f):

    Default authorized handler.

    :param remote: The remote application.
    :param resp: The response.
    :returns: Redirect response.

    :param remote: The remote application.
    :returns: Redirect response or the template rendered.
    oauth = current_app.extensions['oauthlib.client']
    for remote in oauth.remote_apps.values():
        token_delete(remote)
    db.session.commit()

def make_handler(f, remote, with_response=True):
    if isinstance(f, six.string_types):
        f = import_string(f)

    @wraps(f)

    def inner(*args, **kwargs):
        if with_response:
            return f(args[0], remote, *args[1:], **kwargs)
        else:
            return f(remote, *args, **kwargs)
    return inner

def _enable_lock(func):

    @functools.wraps(func)

    def wrapper(*args, **kwargs):
        self = args[0]
        if self.is_concurrent:
            only_read = kwargs.get('only_read')
            if only_read is None or only_read:
                with self._rwlock:
                    return func(*args, **kwargs)
            else:
                self._rwlock.acquire_writer()
                try:
                    return func(*args, **kwargs)
                finally:
                    self._rwlock.release()
        else:
            return func(*args, **kwargs)

    return wrapper

def _enable_cleanup(func):

    @functools.wraps(func)

    def wrapper(*args, **kwargs):
        self = args[0]
        result = func(*args, **kwargs)
        self.cleanup(self)
        return result

    return wrapper

def _enable_thread_pool(func):

    @functools.wraps(func)

    def wrapper(*args, **kwargs):
        self = args[0]
        if self.enable_thread_pool and hasattr(self, 'thread_pool'):
            future = self.thread_pool.submit(func, *args, **kwargs)
            is_async = kwargs.get('is_async')
            if is_async is None or not is_async:
                timeout = kwargs.get('timeout')
                if timeout is None:
                    timeout = 2
                try:
                    result = future.result(timeout=timeout)
                except TimeoutError as e:
                    self.logger.exception(e)
                    result = None
                return result
            return future
        else:
            return func(*args, **kwargs)

    return wrapper

def statistic_record(self, desc=True, timeout=3, is_async=False, only_read=True, *keys):
        if len(keys) == 0:
            records = self._generate_statistic_records()
        else:
            records = self._generate_statistic_records_by_keys(keys)
        return sorted(records, key=lambda t: t['hit_counts'], reverse=desc)

def signature_unsafe(m, sk, pk, hash_func=H):
    h = hash_func(sk)
    a = 2 ** (b - 2) + sum(2 ** i * bit(h, i) for i in range(3, b - 2))
    r = Hint(bytearray([h[j] for j in range(b // 8, b // 4)]) + m)
    R = scalarmult_B(r)
    S = (r + Hint(encodepoint(R) + pk + m) * a) % l
    return bytes(encodepoint(R) + encodeint(S))

def checkvalid(s, m, pk):
    if len(s) != b // 4:
        raise ValueError("signature length is wrong")

    if len(pk) != b // 8:
        raise ValueError("public-key length is wrong")

    s = bytearray(s)
    m = bytearray(m)
    pk = bytearray(pk)

    R = decodepoint(s[: b // 8])
    A = decodepoint(pk)
    S = decodeint(s[b // 8 : b // 4])
    h = Hint(encodepoint(R) + pk + m)

    (x1, y1, z1, t1) = P = scalarmult_B(S)
    (x2, y2, z2, t2) = Q = edwards_add(R, scalarmult(A, h))

    if (
        not isoncurve(P)
        or not isoncurve(Q)
        or (x1 * z2 - x2 * z1) % q != 0
        or (y1 * z2 - y2 * z1) % q != 0
    ):
        raise SignatureMismatch("signature does not pass verification")

def dict():

    default = defaultdict(list)
    for key, value in entries():

        default[key].append(value)

    return default

def symbols():
        Connects input ``Pipers`` to "datas" input data  in the correct order 
        determined, by the ``Piper.ornament`` attribute and the ``Dagger._cmp`` 
        function.

        It is assumed that the input data is in the form of an iterator and
        that all inputs have the same number of input items. A pipeline will
        **deadlock** otherwise. 
        
        Arguments:
        
          - datas (sequence of sequences) An ordered sequence of inputs for 
            all input ``Pipers``.
        
        Given the pipeline topology starts ``Pipers`` in the order input -> 
        output. See ``Piper.start``. ``Pipers`` instances are started in two 
        stages, which allows them to share ``NuMaps``.
        
        Stops the ``Pipers`` according to pipeline topology.
        
        Removes a ``Piper`` from the ``Dagger`` instance.

        Arguments:
        
          - piper(``Piper``  or id(``Piper``)) ``Piper`` instance or ``Piper`` 
            instance id.

          - forced(bool) [default: ``False``] If "forced" is ``True``, will not 
            raise a ``DaggerError`` if the ``Piper`` hase outgoing pipes and 
            will also remove it.
            
        Starts the pipeline by connecting the input ``Pipers`` of the pipeline 
        to the input data, connecting the pipeline and starting the ``NuMap``
        instances.
        
        The order of items in the "datas" argument sequence should correspond 

        to the order of the input ``Pipers`` defined by ``Dagger._cmp`` and 
        ``Piper.ornament``.

        Arguments:
        
          - datas(sequence) A sequence of external input data in the form of 
            sequences or iterators.

        Pauses a running pipeline. This will stop retrieving results from the 
        pipeline. Parallel parts of the pipeline will stop after the ``NuMap`` 
        buffer is has been filled. A paused pipeline can be run or stopped. 
        
        Stops a paused pipeline. This will a trigger a ``StopIteration`` in the
        inputs of the pipeline. And retrieve the buffered results. This will
        stop all ``Pipers`` and ``NuMaps``. Python will not terminate cleanly 
        if a pipeline is running or paused.
        
        Returns the next sequence of results, given stride and n.
        
        Returns the next result from the chained iterables given ``"stride"``.
        

        :param filter: (optional) Filters to apply as a string list.
        :param type: (optional) `union` or `inter` as string.
        :param sort: (optional) Sort fields to apply as string list.
        :param limit: (optional) Limit returned list length.
        :param page: (optional) Page to return.
        :rtype: string

        :param id: Result ID as an int.
        :param update_id: (optional) Update ID as an int.
        :return: :class:`results.Update <results.Update>` object
        :rtype: results.Update


        :param id: Result ID as an int.
        :param when: Must be string `end-of-test` or `end-of-loop`.

        :param id: Result ID as an int.

        :param id: Result ID as an int.
        :param exclude_captures: If bool `True`, don't export capture files
        :rtype: tuple `(io.BytesIO, 'filename')`

        :param ids: Int list of result IDs.
        :rtype: tuple `(io.BytesIO, 'filename')`

        :param ids: Int list of result IDs.
        :return: :class:`results.Result <results.Result>` list

        :return: :class:`results.AllStats <results.AllStats>` object
        :rtype: results.AllStats

        :param id: Result IDs as int list.
        :return: :class:`results.SetStats <results.SetStats>` object
        :rtype: results.SetStats

        :param id: Result IDs as int list.
        :return: :class:`results.DiffStats <results.DiffStats>` object
        :rtype: results.DiffStats

        :param id: Result ID as an int.
        :return: :class:`results.SingleStats <results.SingleStats>` object
        :rtype: results.SingleStats

        :param id: Result ID as an int.
        :return: :class:`results.Progress <results.Progress>` object
        :rtype: results.Progress

        :param id: Result ID as an int.
        :return: :class:`results.SummaryStats <results.SummaryStats>` object
        :rtype: results.SummaryStats

        :param id: Result ID as an int.
        :param filter: Filter to apply as string.
        :param sort: Sort field to apply as string.
        :return: :class:`results.LogDirFile <results.LogDirFile>` list

        :param id: Result ID as an int.
        :param filename: Logdir filename as string.
        :rtype: tuple `(io.BytesIO, 'filename')`

        :param id: Result ID as an int.
        :param format: (optional) Format to download, must be string `zip` or `tgz`.
        :param exclude_captures: If bool `True`, don't include capture files
        :rtype: tuple `(io.BytesIO, 'filename')`
    logout_url = REMOTE_APP['logout_url']

    apps = current_app.config.get('OAUTHCLIENT_REMOTE_APPS')
    if apps:
        cern_app = apps.get('cern', REMOTE_APP)
        logout_url = cern_app['logout_url']

    return redirect(logout_url, code=302)

def find_remote_by_client_id(client_id):

    :param groups: The complete list of groups.
    :returns: A filtered list of groups.
    person_id = resource.get('PersonID', [None])[0]
    identity_class = resource.get('IdentityClass', [None])[0]
    department = resource.get('Department', [None])[0]

    return dict(
        person_id=person_id,
        identity_class=identity_class,
        department=department
    )

def account_groups_and_extra_data(account, resource,
                                  refresh_timedelta=None):
    provides = set([UserNeed(current_user.email)] + [
        RoleNeed('{0}@cern.ch'.format(name)) for name in groups
    ])
    identity.provides |= provides
    session[OAUTHCLIENT_CERN_SESSION_KEY] = provides

def get_dict_from_response(response):
    cached_resource = session.pop('cern_resource', None)
    if cached_resource:
        return cached_resource

    response = remote.get(REMOTE_APP_RESOURCE_API_URL)
    dict_response = get_dict_from_response(response)
    session['cern_resource'] = dict_response
    return dict_response

def on_identity_changed(sender, identity):
    if isinstance(identity, AnonymousIdentity):
        return

    client_id = current_app.config['CERN_APP_CREDENTIALS']['consumer_key']
    account = RemoteAccount.get(
        user_id=current_user.get_id(),
        client_id=client_id,
    )
    groups = []
    if account:
        remote = find_remote_by_client_id(client_id)
        resource = get_resource(remote)
        refresh = current_app.config.get(
            'OAUTHCLIENT_CERN_REFRESH_TIMEDELTA',
            OAUTHCLIENT_CERN_REFRESH_TIMEDELTA
        )

        groups.extend(
            account_groups_and_extra_data(account, resource,
                                          refresh_timedelta=refresh)
        )

    extend_identity(identity, groups)

def get(cls, user_id, client_id):
        return cls.query.filter_by(
            user_id=user_id,
            client_id=client_id,
        ).first()

def create(cls, user_id, client_id, extra_data):
        with db.session.begin_nested():
            account = cls(
                user_id=user_id,
                client_id=client_id,
                extra_data=extra_data or dict()
            )
            db.session.add(account)
        return account

def update_token(self, token, secret):
        if self.access_token != token or self.secret != secret:
            with db.session.begin_nested():
                self.access_token = token
                self.secret = secret
                db.session.add(self)

def get(cls, user_id, client_id, token_type='', access_token=None):
        args = [
            RemoteAccount.id == RemoteToken.id_remote_account,
            RemoteAccount.user_id == user_id,
            RemoteAccount.client_id == client_id,
            RemoteToken.token_type == token_type,
        ]

        if access_token:
            args.append(RemoteToken.access_token == access_token)

        return cls.query.options(
            db.joinedload('remote_account')
        ).filter(*args).first()

def get_by_token(cls, client_id, access_token, token_type=''):
        return cls.query.options(db.joinedload('remote_account')).filter(
            RemoteAccount.id == RemoteToken.id_remote_account,
            RemoteAccount.client_id == client_id,
            RemoteToken.token_type == token_type,
            RemoteToken.access_token == access_token,
        ).first()

def bulk_export(self, config_ids=None, device_ids=None, package_ids=None, result_ids=None, exclude_captures=False):
        if config_ids is None:
            config_ids = []
        if device_ids is None:
            device_ids = []
        if package_ids is None:
            package_ids = []
        if result_ids is None:
            result_ids = []
        json = {
            'configs': map(int, config_ids),
            'devices': map(int, device_ids),
            'packages': map(int, package_ids),
            'results': map(int, result_ids),
            'options': {'exclude_captures': exclude_captures}
        }
        resp = self.service.post(self.base, json=json, stream=True)
        b = io.BytesIO()
        stream.stream_response_to_file(resp, path=b)
        resp.close()
        b.seek(0)
        return (b, self.service.filename(resp))

def _init_report(self):
        import datetime
        import getpass
        username = getpass.getuser()
        # this is not working on some systems: os.environ["USERNAME"]
        timenow = str(datetime.datetime.now())
        timenow = timenow.split('.')[0]
        msg = '<div class="date">Created on ' + timenow
        msg += " by " + username +'</div>'
        return msg

def _track_class_related_field(cls, field):
    if '__' in field:
        _track_class_related_field(cls, field)
        return
    # Will raise FieldDoesNotExist if there is an error
    cls._meta.get_field(field)
    # Detect m2m fields changes
    if isinstance(cls._meta.get_field(field), ManyToManyField):
        m2m_changed.connect(
            tracking_m2m,
            sender=getattr(cls, field).through,
            dispatch_uid=repr(cls),
        )

def _track_class(cls, fields):

    def get_tracking_url(self):
       Decorator used to track changes on Model's fields.

       :Example:
       >>> @track('name')
       ... class Human(models.Model):
       ...     name = models.CharField(max_length=30)
    Indent a value by `n` `character`s

    :param value: string to indent
    :param n: number of characters to indent by
    :param character: character to indent with

        Purpose: This decorator function is used by all functions within
              | the Jaide class that interact with a device to ensure the
              | proper session type is in use. If it is not, it will
              | attempt to migrate _session to that type before moving
              | to the originally requested function.
              | > **NOTE:** This function is a decorator, and should not be
              | >  used directly. All other methods in this class that touch
              | >  the Junos device are wrapped by this function to ensure the
              | >  proper connection type is used.

        @param function: the function that is being wrapped around
        @type function: function

        @returns: the originally requested function
        @rtype: function

        Purpose: Executes a commit operation. All parameters are optional.
               | commit confirm and commit at are mutually exclusive. All
               | the others can be used with each other and commit confirm/at.

        @param commands: A string or list of multiple commands
                       | that the device will compare with.
                       | If a string, it can be a single command,
                       | multiple commands separated by commas, or
                       | a filepath location of a file with multiple
                       | commands, each on its own line.
        @type commands: str or list
        @param confirmed: integer value of the number of **seconds** to
                             | confirm the commit for, if requested.
        @type confirmed: int
        @param comment: string that the user wants to comment the commit
                      | with. Will show up in the 'show system commit' log.
        @type comment: str
        @param at_time: string designating the time at which the commit
                      | should happen. Can be in one of two Junos approved
                      | formats.
        @type comment: str
        @param synchronize: boolean set to true if desiring a commit
                          | synchronize operation.
        @type synchronize: bool
        @param req_format: string to specify the response format. Accepts
                         | either 'text' or 'xml'
        @type req_format: str

        @returns: The reply from the device.
        @rtype: str

        Purpose: This method will take in string of multiple commands,
               | and perform and 'commit check' on the device to ensure
               | the commands are syntactically correct. The response can
               | be formatted as text or as xml.

        @param commands: A string, filepath, or list of multiple commands
                       | that the device will compare with.
        @type commands: str or list

        @param req_format: The desired format of the response, defaults to
                         | 'text', but also accepts 'xml'
        @type req_format: str

        @returns: The reply from the device.
        @rtype: str

        Purpose: This method will take in string of multiple commands,
               | and perform and 'show | compare' on the device to show the
               | differences between the active running configuration and
               | the changes proposed by the passed commands parameter.

        @param commands: A string, filepath, or list of multiple commands
                       | that the device will compare with.
        @type commands: str or list

        @param req_format: The desired format of the response, defaults to
                         | 'text', but also accepts 'xml'
        @type req_format: str

        @returns: The reply from the device.
        @rtype: str

        Purpose: This method is used to make a connection to the junos
               | device. The internal property conn_type is what
               | determines the type of connection we make to the device.
               | - 'paramiko' is used for operational commands (to allow
               |            pipes in commands)
               | - 'scp' is used for copying files
               | - 'shell' is used for to send shell commands
               | - 'root' is used when logging into the device as root, and
               |            wanting to send operational commands
               | - 'ncclient' is used for the rest (commit, compare_config,
               |            commit_check)

        @returns: None
        @rtype: None

        Purpose: Callback function for an SCP operation. Used to show
               | the progress of an actively running copy. This directly
               | prints to stdout, one line for each file as it's copied.
               | The parameters received by this function are those received
               | from the scp.put or scp.get function, as explained in the
               | python scp module docs.

        @param filename: The filename of file being copied.
        @type filename: str
        @param size: The total size of the current file being copied.
        @type size: str or float
        @param sent: The amount of data sent for the current file being copied.
        @type sent: str or float

        @returns: None

        Purpose: This function grabs the hostname, model, running version, and
               | serial number of the device.

        @returns: The output that should be shown to the user.
        @rtype: str
                <junos-version>15.1</junos-version>
                <package-information>
                    <name>junos-version</name>
                    <comment>Junos: 14.2R4</comment>
                </package-information>
                <package-information>
                    <name>junos</name>
                    <comment>JUNOS Base OS boot [12.3R5]</comment>
                </package-information>

        Purpose: Open a second ncclient.manager.Manager with second_host, and
               | and pull the configuration from it. We then use difflib to
               | get the delta between the two, and yield the results.

        @param second_host: the IP or hostname of the second device to
                          | compare against.
        @type second_host: str
        @param mode: string to signify 'set' mode or 'stanza' mode.
        @type mode: str

        @returns: iterable of strings
        @rtype: str

        Purpose: Takes the xml output of 'show interfaces extensive' for a
               | given interface and yields the error types that have a
               | significant number of errors.

        @param interface: The xml output of the 'sh int ext' command for
                        | the desired interface.
        @type interface: lxml.etree._Element object
        @param face: The direction of the errors we're wanting. Either 'input'
                   | or 'output' is accepted.
        @type face: str

        @returns: Yields each error that has a significant number
        @rtype: iterable of strings.

        Purpose: Grab the cpu/mem usage, system/chassis alarms, top 5
               | processes, and states if the primary/backup partitions are on
               | different versions.

        @returns: The output that should be shown to the user.
        @rtype: str

        Purpose: This function is called for the -e flag. It will let the user
               | know if there are any interfaces with errors, and what those
               | interfaces are.

        @returns: The output that should be shown to the user.
        @rtype: str
        if isinstance(self._session, manager.Manager):
            self._session.lock()

def op_cmd(self, command, req_format='text', xpath_expr=""):
        if not command:
            raise InvalidCommandError("Parameter 'command' cannot be empty")
        if req_format.lower() == 'xml' or xpath_expr:
            command = command.strip() + ' | display xml'
        command = command.strip() + ' | no-more\n'
        out = ''
        # when logging in as root, we use _shell to get the response.
        if self.username == 'root':
            self._shell.send(command)
            time.sleep(3)
            while self._shell.recv_ready():
                out += self._shell.recv(999999)
                time.sleep(.75)
            # take off the command being sent and the prompt at the end.
            out = '\n'.join(out.split('\n')[1:-2])
        # not logging in as root, and can grab the output as normal.
        else:
            stdin, stdout, stderr = self._session.exec_command(command=command,
                                           timeout=float(self.session_timeout))
            stdin.close()
            # read normal output
            while not stdout.channel.exit_status_ready():
                out += stdout.read()
            stdout.close()
            # read errors
            while not stderr.channel.exit_status_ready():
                out += stderr.read()
            stderr.close()
        return out if not xpath_expr else xpath(out, xpath_expr)

def unlock(self):
        if isinstance(self._session, manager.Manager):
            self._session.unlock()

def intercept(obj, methodname, wrapper):
    original = getattr(obj, methodname)


    def replacement(*args, **kwargs):
        wrapfn = wrapper(*args, **kwargs)
        wrapfn.send(None)
        result = original(*args, **kwargs)
        try:
            wrapfn.send(result)
        except StopIteration:
            return result
        else:
            raise AssertionError('Generator did not stop')


    def unwrap():
        setattr(obj, methodname, original)

    replacement.unwrap = unwrap

    setattr(obj, methodname, replacement)

def next(self):
        # need new iterable?
        if self.r == self.repeats:
            self.i = (self.i + 1) % self.lenght
            self.r = 0

        self.r += 1
        if self.stopping and self.i == 0 and self.r == 1:
            self.stopped = True
        if self.i == 0 and self.stopped:
            raise StopIteration
        else:
            iterator = self.iterators[self.i]
            return iterator.next()

def next(self):
        return self.iterator.next(task=self.task, timeout=self.timeout,
                                                    block=self.block)

def write_template(fn, lang="python"):
    with open(fn, "wb") as fh:
        if lang == "python":
            fh.write(PY_TEMPLATE)
        elif lang == "bash":
            fh.write(SH_TEMPLATE)

def script(inbox, cfg):
    script_name = cfg["id"]
    script_id = str(abs(hash((cfg["id"],) + tuple(inbox[0].values()))))[0:8]
    # LOG.log(mp.DEFAULT, "@papy;script %s:%s started" % (script_name, script_id))
    # LOG.log(mp.SUBDEFAULT, "@papy;%s:%s received: %s" % (script_name, script_id, inbox))
    args = {}
    args["params"] = dict(cfg["params"])
    args["in"] = {}
    for in_port in cfg["in"]:
        for inin_ports in inbox:
            in_path = inin_ports.get(in_port, None)
            if (in_path is not None):
                # first matching input-output (including type) port is linked remaining ignored
                args["in"][in_port] = in_path
                break
    # check that all input ports are connected
    if len(args["in"]) < len(cfg["in"]):
        raise Exception("not all in_ports connected, got: %s" % (args["in"],))
    # create output file for out_ports
    args["out"] = {}
    out = {}
    for i, (out_port, out_ext) in enumerate(cfg["out"]):
        if cfg["in"] == tuple(out_port_ for out_port_, _ in cfg["out"]):
            pfx = args["in"][cfg["in"][i]].split("/")[-1].split(".")[0] + "_"
            base = cfg["id"]
        else:
            pfx = args["in"][cfg["in"][0]].split("/")[-1].split(".")[0] + "_"
            base = cfg["id"] + "-" + out_port 
        if out_ext:
            out_path = cfg["dir"] + "/" + pfx + base + "." + out_ext
        else:
            out_path = cfg["dir"] + "/" + pfx + base
        args["out"][out_port] = out_path
        out[out_port] = out_path
    # evaluate and check for errors
    ret = _eval_script(cfg["evaluator"], cfg["preamble"], cfg["dir"], cfg["executable"], cfg["script"], args)
    if ret[0] != 0:
        # LOG.error("@papy;%s:%s %s:%s:%s" % (script_name, script_id, ret[0], 
        #                                                              ret[1].replace("\n", "<br>"), 
        #                                                              ret[2].replace("\n", "<br>")))
        raise Exception(ret[0], cfg["script"], ret[1], ret[2])
    #LOG.log(mp.SUBDEFAULT, "@papy;%s:%s produced:%s" % (script_name, script_id, out))
    #LOG.log(mp.DEFAULT, "@papy;script %s:%s finished" % (script_name, script_id))
    return out

def edit(self, resource):
        schema = JobSchema(exclude=('id', 'status', 'options', 'package_name', 'config_name', 'device_name', 'result_id', 'user_id', 'created', 'updated', 'automatic', 'run_at'))
        json = self.service.encode(schema, resource)

        schema = JobSchema()
        resp = self.service.edit(self.base, resource.name, json)
        return self.service.decode(schema, resp)

def launch(self, resource):
        schema = JobSchema(exclude=('id', 'status', 'package_name', 'config_name', 'device_name', 'result_id', 'user_id', 'created', 'updated', 'automatic'))
        json = self.service.encode(schema, resource)

        schema = JobSchema()
        resp = self.service.create(self.base, json)
        return self.service.decode(schema, resp)

def bulk_launch(self, jobs=None, filter=None, all=False): # pylint: disable=redefined-builtin
        json = None
        if jobs is not None:
            schema = JobSchema(exclude=('id', 'status', 'package_name', 'config_name', 'device_name', 'result_id', 'user_id', 'created', 'updated', 'automatic'))
            jobs_json = self.service.encode(schema, jobs, many=True)
            json = {self.RESOURCE: jobs_json}

        schema = JobSchema()
        resp = self.service.post(self.base,
                                 params={'bulk': 'launch', 'filter': filter, 'all': all}, json=json)
        return self.service.decode(schema, resp, many=True)

def get(self, id, seq, line): # pylint: disable=invalid-name,redefined-builtin
        schema = HighlightSchema()
        resp = self.service.get_id(self._base(id, seq), line)
        return self.service.decode(schema, resp)

def create_or_edit(self, id, seq, resource): # pylint: disable=invalid-name,redefined-builtin
        schema = HighlightSchema(exclude=('id', 'seq'))
        json = self.service.encode(schema, resource)

        schema = HighlightSchema()
        resp = self.service.edit(self._base(id, seq), resource.line, json)
        return self.service.decode(schema, resp)

def create(self, id, seq, resource): # pylint: disable=invalid-name,redefined-builtin
        return self.create_or_edit(id, seq, resource)

def edit(self, id, seq, resource): # pylint: disable=invalid-name,redefined-builtin
        return self.create_or_edit(id, seq, resource)

def delete(self, id, seq, line): # pylint: disable=invalid-name,redefined-builtin
        return self.service.delete_id(self._base(id, seq), line)

def post_ext_init(state):
    oauth = current_app.extensions['oauthlib.client']

    if remote_app not in oauth.remote_apps:
        return abort(404)

    # Get redirect target in safe manner.
    next_param = get_safe_redirect_target(arg='next')

    # Redirect URI - must be registered in the remote service.
    callback_url = url_for(
        '.authorized',
        remote_app=remote_app,
        _external=True,
    )

    # Create a JSON Web Token that expires after OAUTHCLIENT_STATE_EXPIRES
    # seconds.
    state_token = serializer.dumps({
        'app': remote_app,
        'next': next_param,
        'sid': _create_identifier(),
    })

    return oauth.remote_apps[remote_app].authorize(
        callback=callback_url,
        state=state_token,
    )

def authorized(remote_app=None):
    if remote_app not in current_oauthclient.signup_handlers:
        return abort(404)
    res = current_oauthclient.signup_handlers[remote_app]['view']()
    return abort(404) if res is None else res

def disconnect(remote_app):
    if remote_app not in current_oauthclient.disconnect_handlers:
        return abort(404)

    ret = current_oauthclient.disconnect_handlers[remote_app]()
    db.session.commit()
    return ret

def address_checksum(address):
    address_bytes = address
    h = blake2b(digest_size=5)
    h.update(address_bytes)
    checksum = bytearray(h.digest())
    checksum.reverse()
    return checksum

def keypair_from_seed(seed, index=0):

    h = blake2b(digest_size=32)
    h.update(seed + struct.pack(">L", index))
    priv_key = h.digest()
    pub_key = private_to_public_key(priv_key)
    return {'private': priv_key, 'public': pub_key}

def verify_signature(message, signature, public_key):

    try:
        ed25519_blake2.checkvalid(signature, message, public_key)
    except ed25519_blake2.SignatureMismatch:
        return False
    return True

def sign_message(message, private_key, public_key=None):

    if public_key is None:
        public_key = private_to_public_key(private_key)

    return ed25519_blake2.signature_unsafe(message, private_key, public_key)

def check_for_lounge_upgrade(self, email, password):
        schema = ReleaseSchema()
        resp = self.service.post(self.base+'lounge/check/',
                                 json={'email': email, 'password': password})
        return self.service.decode(schema, resp)

def lounge_upgrade(self, email, password, release_id):
        schema = UpgradeSchema()
        resp = self.service.post(self.base+'lounge/upgrade/',
                                 json={'email': email, 'password': password, 'release': {'id': int(release_id)}})
        return self.service.decode(schema, resp)

def lounge_update_license(self):
        schema = UpgradeSchema()
        resp = self.service.post(self.base+'license/')
        return self.service.decode(schema, resp)

def manual_update_license(self, fd, filename='cdrouter.lic'):
        schema = UpgradeSchema()
        resp = self.service.post(self.base+'license/',
                                 files={'file': (filename, fd)})
        return self.service.decode(schema, resp)

def space(self):
        schema = SpaceSchema()
        resp = self.service.get(self.base+'space/')
        return self.service.decode(schema, resp)

def interfaces(self, addresses=False):
        schema = InterfaceSchema()
        resp = self.service.get(self.base+'interfaces/', params={'addresses': addresses})
        return self.service.decode(schema, resp, many=True)

def _set_original_fields(instance):
    original_fields = {}


    def _set_original_field(instance, field):
        if instance.pk is None:
            original_fields[field] = None
        else:
            if isinstance(instance._meta.get_field(field), ForeignKey):
                # Only get the PK, we don't want to get the object
                # (which would make an additional request)
                original_fields[field] = getattr(instance,
                                                 '{0}_id'.format(field))
            else:
                original_fields[field] = getattr(instance, field)

    for field in getattr(instance, '_tracked_fields', []):
        _set_original_field(instance, field)
    for field in getattr(instance, '_tracked_related_fields', {}).keys():
        _set_original_field(instance, field)

    instance._original_fields = original_fields
    # Include pk to detect the creation of an object
    instance._original_fields['pk'] = instance.pk

def _has_changed(instance):
    for field, value in instance._original_fields.items():
        if field != 'pk' and \
           not isinstance(instance._meta.get_field(field), ManyToManyField):
            try:
                if field in getattr(instance, '_tracked_fields', []):
                    if isinstance(instance._meta.get_field(field), ForeignKey):
                        if getattr(instance, '{0}_id'.format(field)) != value:
                            return True
                    else:
                        if getattr(instance, field) != value:
                            return True
            except TypeError:
                # Can't compare old and new value, should be different.
                return True
    return False

def _has_changed_related(instance):
    tracked_related_fields = getattr(
        instance,
        '_tracked_related_fields',
        {}
    ).keys()
    for field, value in instance._original_fields.items():
        if field != 'pk' and \
           not isinstance(instance._meta.get_field(field), ManyToManyField):
            if field in tracked_related_fields:
                if isinstance(instance._meta.get_field(field), ForeignKey):
                    if getattr(instance, '{0}_id'.format(field)) != value:
                        return True
                else:
                    if getattr(instance, field) != value:
                        return True
    return False

def _create_event(instance, action):
    user = None
    user_repr = repr(user)
    if CUSER:
        user = CuserMiddleware.get_user()
        user_repr = repr(user)
        if user is not None and user.is_anonymous:
            user = None
    return TrackingEvent.objects.create(
        action=action,
        object=instance,
        object_repr=repr(instance),
        user=user,
        user_repr=user_repr,
    )

def _create_tracked_field(event, instance, field, fieldname=None):
    fieldname = fieldname or field
    if isinstance(instance._meta.get_field(field), ForeignKey):
        # We only have the pk, we need to get the actual object
        model = instance._meta.get_field(field).remote_field.model
        pk = instance._original_fields[field]
        try:
            old_value = model.objects.get(pk=pk)
        except model.DoesNotExist:
            old_value = None
    else:
        old_value = instance._original_fields[field]
    return TrackedFieldModification.objects.create(
        event=event,
        field=fieldname,
        old_value=_serialize_field(old_value),
        new_value=_serialize_field(getattr(instance, field))
    )

def _create_create_tracking_event(instance):
    event = _create_event(instance, CREATE)
    for field in instance._tracked_fields:
        if not isinstance(instance._meta.get_field(field), ManyToManyField):
            _create_tracked_field(event, instance, field)

def _create_update_tracking_event(instance):
    event = _create_event(instance, UPDATE)
    for field in instance._tracked_fields:
        if not isinstance(instance._meta.get_field(field), ManyToManyField):
            try:
                if isinstance(instance._meta.get_field(field), ForeignKey):
                    # Compare pk
                    value = getattr(instance, '{0}_id'.format(field))
                else:
                    value = getattr(instance, field)
                if instance._original_fields[field] != value:
                    _create_tracked_field(event, instance, field)
            except TypeError:
                # Can't compare old and new value, should be different.
                _create_tracked_field(event, instance, field)

def _create_update_tracking_related_event(instance):
    events = {}
    # Create a dict mapping related model field to modified fields
    for field, related_fields in instance._tracked_related_fields.items():
        if not isinstance(instance._meta.get_field(field), ManyToManyField):
            if isinstance(instance._meta.get_field(field), ForeignKey):
                # Compare pk
                value = getattr(instance, '{0}_id'.format(field))
            else:
                value = getattr(instance, field)
            if instance._original_fields[field] != value:
                for related_field in related_fields:

                    events.setdefault(related_field, []).append(field)

    # Create the events from the events dict
    for related_field, fields in events.items():
        try:
            related_instances = getattr(instance, related_field[1])
        except ObjectDoesNotExist:
            continue

        # FIXME: isinstance(related_instances, RelatedManager ?)
        if hasattr(related_instances, 'all'):
            related_instances = related_instances.all()
        else:
            related_instances = [related_instances]
        for related_instance in related_instances:
            event = _create_event(related_instance, UPDATE)
            for field in fields:
                fieldname = '{0}__{1}'.format(related_field[0], field)
                _create_tracked_field(
                    event, instance, field, fieldname=fieldname
                )

def _get_m2m_field(model, sender):
    for field in getattr(model, '_tracked_fields', []):
        if isinstance(model._meta.get_field(field), ManyToManyField):
            if getattr(model, field).through == sender:
                return field
    for field in getattr(model, '_tracked_related_fields', {}).keys():
        if isinstance(model._meta.get_field(field), ManyToManyField):
            if getattr(model, field).through == sender:
                return field

def tracking_save(sender, instance, raw, using, update_fields, **kwargs):
    if _has_changed(instance):
        if instance._original_fields['pk'] is None:
            # Create
            _create_create_tracking_event(instance)
        else:
            # Update
            _create_update_tracking_event(instance)
    if _has_changed_related(instance):
        # Because an object need to be saved before being related,
        # it can only be an update
        _create_update_tracking_related_event(instance)
    if _has_changed(instance) or _has_changed_related(instance):
        _set_original_fields(instance)

def from_entry_dict(cls, entry_dict):
        # Debug helper
        # https://circleci.com/gh/andresriancho/w3af-api-docker/30
        try:
            _type = entry_dict['type']
            _id = entry_dict['id']
            _time = entry_dict['time']
            message = entry_dict['message']
            severity = entry_dict['severity']
        except KeyError:
            msg = ('Missing expected log entry attribute. Log entry'
                   ' object is:\n\n%s')
            raise APIException(msg % json.dumps(entry_dict, indent=4))

        return cls(_type, message, _time, severity, _id)

def list(self, id, seq): # pylint: disable=invalid-name,redefined-builtin
        schema = CaptureSchema(exclude=('id', 'seq'))
        resp = self.service.list(self._base(id, seq))
        return self.service.decode(schema, resp, many=True)

def get(self, id, seq, intf): # pylint: disable=invalid-name,redefined-builtin
        schema = CaptureSchema()
        resp = self.service.get_id(self._base(id, seq), intf)
        return self.service.decode(schema, resp)

def download(self, id, seq, intf, inline=False): # pylint: disable=invalid-name,redefined-builtin
        resp = self.service.get_id(self._base(id, seq), intf, params={'format': 'cap', 'inline': inline}, stream=True)
        b = io.BytesIO()
        stream.stream_response_to_file(resp, path=b)
        resp.close()
        b.seek(0)
        return (b, self.service.filename(resp))

def summary(self, id, seq, intf, filter=None, inline=False): # pylint: disable=invalid-name,redefined-builtin
        schema = SummarySchema()
        resp = self.service.get(self._base(id, seq)+str(intf)+'/summary/',
                                params={'filter': filter, 'inline': inline})
        return self.service.decode(schema, resp)

def decode(self, id, seq, intf, filter=None, frame=None, inline=False): # pylint: disable=invalid-name,redefined-builtin
        schema = DecodeSchema()
        resp = self.service.get(self._base(id, seq)+str(intf)+'/decode/',
                                params={'filter': filter, 'frame': frame, 'inline': inline})
        return self.service.decode(schema, resp)

def send_to_cloudshark(self, id, seq, intf, inline=False): # pylint: disable=invalid-name,redefined-builtin
        schema = CloudSharkSchema()
        resp = self.service.post(self._base(id, seq)+str(intf)+'/cloudshark/', params={'inline': inline})
        return self.service.decode(schema, resp)

def get_dict_from_response(response):

    See the docs here for v2/oauth/userinfo:
    https://docs.globus.org/api/auth/reference/

    A Globus ID is a UUID that can uniquely identify a Globus user. See the
    docs here for v2/api/identities
    https://docs.globus.org/api/auth/reference/
    Return the signature string of the specified function.


    >>> def foo(name): pass
    >>> get_function_signature(foo)
    'foo(name)'
    >>> something = 'Hello'
    >>> get_function_signature(something)
    Traceback (most recent call last):
        ...
    TypeError: The argument must be a function object: None type is <class 'str'>
        Acquire a read lock, several threads can hold this type of lock.
        Acquire a write lock, only one thread can hold this lock
        and only when no read locks are also held.

        :param filter: (optional) Filters to apply as a string list.
        :param type: (optional) `union` or `inter` as string.
        :param sort: (optional) Sort fields to apply as string list.
        :param limit: (optional) Limit returned list length.
        :param page: (optional) Page to return.
        :return: :class:`packages.Page <packages.Page>` object

        :param id: Package ID as an int.
        :return: :class:`packages.Package <packages.Package>` object
        :rtype: packages.Package

        :param resource: :class:`packages.Package <packages.Package>` object
        :return: :class:`packages.Package <packages.Package>` object
        :rtype: packages.Package

        :param id: Package ID as an int.
        :return: :class:`packages.Analysis <packages.Analysis>` object
        :rtype: packages.Analysis

        :param ids: Int list of package IDs.
        :return: :class:`packages.Package <packages.Package>` list

        :param _fields: :class:`packages.Package <packages.Package>` object
        :param ids: (optional) Int list of package IDs.
        :param filter: (optional) String list of filters.
        :param type: (optional) `union` or `inter` as string.
        :param all: (optional) Apply to all if bool `True`.

    Purpose: This function is a generator that will read in either a
           | plain text file of strings(IP list, command list, etc), a
           | comma separated string of strings, or a list of strings. It
           | will crop out any comments or blank lines, and yield
           | individual strings.
           |
           | Only strings that do not start with a comment '#', or are not
           | entirely whitespace will be yielded. This allows a file with
           | comments and blank lines for formatting neatness to be used
           | without a problem.

    @param commands: This can be either a string that is a file
                   | location, a comma separated string of strings
                   | ('x,y,z,1,2,3'), or a python list of strings.
    @type commands: str or list

    @returns: Yields each command in order
    @rtype: iterable of str

    Purpose: This function applies an Xpath expression to the XML
           | supplied by source_xml. Returns a string subtree or
           | subtrees that match the Xpath expression. It can also return
           | an xml object if desired.

    @param source_xml: Plain text XML that will be filtered
    @type source_xml: str or lxml.etree.ElementTree.Element object
    @param xpath_expr: Xpath expression that we will filter the XML by.
    @type xpath_expr: str
    @param req_format: the desired format of the response, accepts string or
                     | xml.
    @type req_format: str

    @returns: The filtered XML if filtering was successful. Otherwise,
            | an empty string.
    @rtype: str or ElementTree
        Set the value for the key in the key-value store.

        Setting a value on a key increments the revision
        of the key-value store and generates one event in
        the event history.

        :param key: key is the key, in bytes, to put into
            the key-value store.
        :type key: bytes

        :param value: value is the value, in bytes, to
            associate with the key in the key-value store.
        :key value: bytes

        :param lease: Lease to associate the key in the
            key-value store with.
        :type lease: instance of :class:`txaioetcd.Lease` or None

        :param return_previous: If set, return the previous key-value.
        :type return_previous: bool or None

        :param timeout: Request timeout in seconds.
        :type timeout: int

        :returns: Revision info
        :rtype: instance of :class:`txaioetcd.Revision`
        Watch one or more keys or key sets and invoke a callback.

        Watch watches for events happening or that have happened. The entire event history
        can be watched starting from the last compaction revision.

        :param keys: Watch these keys / key sets.
        :type keys: list of bytes or list of instance of :class:`txaioetcd.KeySet`

        :param on_watch: The callback to invoke upon receiving
            a watch event.
        :type on_watch: callable

        :param filters: Any filters to apply.

        :param start_revision: start_revision is an optional
            revision to watch from (inclusive). No start_revision is "now".
        :type start_revision: int

        :param return_previous: Flag to request returning previous values.


        :returns: A deferred that just fires when watching has started successfully,
            or which fires with an error in case the watching could not be started.
        :rtype: twisted.internet.Deferred
        Creates a lease which expires if the server does not
        receive a keep alive within a given time to live period.

        All keys attached to the lease will be expired and deleted if
        the lease expires.

        Each expired key generates a delete event in the event history.

        :param time_to_live: TTL is the advisory time-to-live in seconds.
        :type time_to_live: int

        :param lease_id: ID is the requested ID for the lease.
            If ID is None, the lessor (etcd) chooses an ID.
        :type lease_id: int or None

        :param timeout: Request timeout in seconds.
        :type timeout: int

        :returns: A lease object representing the created lease. This
            can be used for refreshing or revoking the least etc.
        :rtype: instance of :class:`txaioetcd.Lease`

        :param fd: File-like object to upload.
        :param filename: (optional) Filename to use for import as string.
        :return: :class:`imports.Import <imports.Import>` object

        :param filepath: Local filesystem path as string.
        :return: :class:`imports.Import <imports.Import>` object

        :param url: URL to import as string.
        :param token: (optional) API token to use as string (may be required if importing from a CDRouter 10+ system).
        :param username: (optional) API username to use as string (may be required if importing from a CDRouter 10+ system).
        :param password: (optional) API password to use as string (may be required if importing from a CDRouter 10+ system).
        :param insecure: (optional) Allow insecure HTTPS connections if bool `True`.
        :return: :class:`imports.Import <imports.Import>` object

        :param id: Staged import ID as an int.
        :return: :class:`imports.Request <imports.Request>` object
        :rtype: imports.Request

        :param id: Staged import ID as an int.
        :param impreq: :class:`imports.Request <imports.Request>` object
        :return: :class:`imports.Request <imports.Request>` object
        :rtype: imports.Request
    
    Screenshots will be saved in subdirectories under this directory by

        :param id: User ID as an int.
        :param new: New password as string.
        :param old: (optional) Old password as string (required if performing action as non-admin).
        :param change_token: (optional) If bool `True`, also generate a new API token for user.
        :return: :class:`users.User <users.User>` object
        :rtype: users.User

        :param id: User ID as an int.
        :return: :class:`users.User <users.User>` object
        :rtype: users.User

        :param ids: Int list of user IDs.
        :return: :class:`users.User <users.User>` list

        :param id: Device ID as an int.
        :param filter: (optional) Filters to apply as a string list.
        :param type: (optional) `union` or `inter` as string.
        :param sort: (optional) Sort fields to apply as string list.
        :param limit: (optional) Limit returned list length.
        :param page: (optional) Page to return.
        :return: :class:`attachments.Page <attachments.Page>` object
        of attachments according to its ``limit`` and ``page``
        arguments, ``iter_list`` returns all attachments by internally
        making successive calls to ``list``.

        :param id: Device ID as an int.
        :param args: Arguments that ``list`` takes.
        :param kwargs: Optional arguments that ``list`` takes.
        :return: :class:`attachments.Attachment <attachments.Attachment>` list


        :param id: Device ID as an int.
        :param attid: Attachment ID as an int.
        :return: :class:`attachments.Attachment <attachments.Attachment>` object
        :rtype: attachments.Attachment

        :param id: Device ID as an int.
        :param fd: File-like object to upload.
        :param filename: (optional) Name to use for new attachment as a string.
        :return: :class:`attachments.Attachment <attachments.Attachment>` object
        :rtype: attachments.Attachment

        :param id: Device ID as an int.
        :param attid: Attachment ID as an int.
        :rtype: tuple `(io.BytesIO, 'filename')`

        :param resource: :class:`attachments.Attachment <attachments.Attachment>` object
        :return: :class:`attachments.Attachment <attachments.Attachment>` object
        :rtype: attachments.Attachment

        :param id: Device ID as an int.
        :param attid: Attachment ID as an int.

        :param name: Device name as string.
        :return: :class:`devices.Device <devices.Device>` object
        :rtype: devices.Device

        :param resource: :class:`devices.Device <devices.Device>` object
        :return: :class:`devices.Device <devices.Device>` object
        :rtype: devices.Device

        :param id: Device ID as an int.
        :return: :class:`devices.Connection <devices.Connection>` object
        :rtype: devices.Connection

        :param id: Device ID as an int.
        :return: :class:`devices.Connection <devices.Connection>` object
        :rtype: devices.Connection

        :param id: Device ID as an int.

        :param id: Device ID as an int.
        :return: :class:`devices.PowerCmd <devices.PowerCmd>` object
        :rtype: devices.PowerCmd

        :param ids: Int list of device IDs.
        :return: :class:`devices.Device <devices.Device>` list
    if value is None:
        return []
    return value if isinstance(value, list) else [value]

def color(out_string, color='grn'):
    c = {
        'blk': Fore.BLACK,
        'blu': Fore.BLUE,
        'cyn': Fore.CYAN,
        'grn': Fore.GREEN,
        'mag': Fore.MAGENTA,
        'red': Fore.RED,
        'wht': Fore.WHITE,
        'yel': Fore.YELLOW,
    }
    try:
        init()
        return (c[color] + Style.BRIGHT + out_string + Fore.RESET + Style.NORMAL)
    except AttributeError:
        return out_string

def color_diffs(string):
    string = string.replace('--- ', color('--- ', 'red'))
    string = string.replace('\n+++ ', color('\n+++ '))
    string = string.replace('\n-', color('\n-', 'red'))
    string = string.replace('\n+', color('\n+'))
    string = string.replace('\n@@ ', color('\n@@ ', 'yel'))
    return string

def index():
    label = XPathSelector(browser,
                          unicode('//label[contains(., "%s")]' % label))
    if not label:
        return False
    return label.get_attribute('for')

def find_field(browser, field, value):
    return find_field_by_id(browser, field, value) + \
        find_field_by_name(browser, field, value) + \
        find_field_by_label(browser, field, value)

def find_any_field(browser, field_types, field_name):

    return reduce(
        operator.add,
        (find_field(browser, field_type, field_name)
         for field_type in field_types)
    )

def find_field_by_label(browser, field, label):

    return XPathSelector(browser,
                         field_xpath(field, 'id', escape=False) %
                         u'//label[contains(., "{0}")]/@for'.format(label))

def wait_for(func):


    def wrapped(*args, **kwargs):
        timeout = kwargs.pop('timeout', 15)

        start = time()
        result = None

        while time() - start < timeout:
            result = func(*args, **kwargs)
            if result:
                break
            sleep(0.2)

        return result

    return wrapped

def get_defaults():
    DEFAULTS = {}
    # Determine the run-time pipe read/write buffer.
    if 'PC_PIPE_BUF' in os.pathconf_names:
        # unix
        x, y = os.pipe()
        DEFAULTS['PIPE_BUF'] = os.fpathconf(x, "PC_PIPE_BUF")
    else:
        # in Jython 16384
        # on windows 512
        # in jython in windows 512
        DEFAULTS['PIPE_BUF'] = 512

    # Determine the run-time socket buffers.
    # Note that this number is determine on the papy server
    # and inherited by the clients.
    tcp_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    DEFAULTS['TCP_SNDBUF'] = tcp_sock.getsockopt(socket.SOL_SOCKET, \
                                                 socket.SO_SNDBUF)
    DEFAULTS['TCP_RCVBUF'] = tcp_sock.getsockopt(socket.SOL_SOCKET, \
                                                 socket.SO_RCVBUF)
    udp_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    DEFAULTS['UDP_SNDBUF'] = udp_sock.getsockopt(socket.SOL_SOCKET, \
                                                 socket.SO_SNDBUF)
    DEFAULTS['UDP_RCVBUF'] = udp_sock.getsockopt(socket.SOL_SOCKET, \
                                                 socket.SO_RCVBUF)

    # check the ip visible from the world.
    DEFAULTS['WHATS_MYIP_URL'] = \
    'http://www.whatismyip.com/automation/n09230945.asp'
    return DEFAULTS

def site_url(url):
    base_url = 'http://%s' % socket.gethostname()

    if server.port is not 80:
        base_url += ':%d' % server.port

    return urlparse.urljoin(base_url, url)

def _get_external_id(account_info):

    Uses either the access token or extracted account information to retrieve
    the user object.

    :param client_id: The client id.
    :param account_info: The dictionary with the account info.
        (Default: ``None``)
    :param access_token: The access token. (Default: ``None``)
    :returns: A :class:`invenio_accounts.models.User` instance or ``None``.

    :param client_id: The client id.
    :param user: A user instance.
    :param require_existing_link: If ``True``, check if remote account exists.
        (Default: ``False``)
    :returns: ``True`` if the user is successfully authenticated.

    :param form: A form instance.
    :returns: A :class:`invenio_accounts.models.User` instance.

    :param user: A :class:`invenio_accounts.models.User` instance.
    :param external_id: The external id associated with the user.
        (Default: ``None``)
    :raises invenio_oauthclient.errors.AlreadyLinkedError: Raised if already
        exists a link.

    :param external_id: The external id associated with the user.
    class RegistrationForm(_security.confirm_register_form):
        password = None
        recaptcha = None
    return RegistrationForm(*args, **kwargs)

def fill_form(form, data):
    for (key, value) in data.items():
        if hasattr(form, key):
            if isinstance(value, dict):
                fill_form(getattr(form, key), value)
            else:
                getattr(form, key).data = value
    return form

def _get_csrf_disabled_param():
    import flask_wtf
    from pkg_resources import parse_version
    supports_meta = parse_version(flask_wtf.__version__) >= parse_version(
        "0.14.0")
    return dict(meta={'csrf': False}) if supports_meta else \
        dict(csrf_enabled=False)

def run(self):
        try:

            self.loader.find_and_load_step_definitions()
        except StepLoadingError, e:

            print "Error loading step definitions:\n", e
            return

        results = []
        if self.explicit_features:
            features_files = self.explicit_features
        else:
            features_files = self.loader.find_feature_files()
        if self.random:
            random.shuffle(features_files)

        if not features_files:
            self.output.print_no_features_found(self.loader.base_dir)
            return

        processes = Pool(processes=self.parallelization)
        test_results_it = processes.imap_unordered(
            worker_process, [(self, filename) for filename in features_files]
        )
        
        all_total = ParallelTotalResult()
        for result in test_results_it:
            all_total += result['total']
            sys.stdout.write(result['stdout'])
            sys.stderr.write(result['stderr'])

        return all_total

def open_connection(ip, username, password, function, args, write=False,
                    conn_timeout=5, sess_timeout=300, port=22):
    # start with the header line on the output.
    output = color('=' * 50 + '\nResults from device: %s\n' % ip, 'yel')
    try:
        # create the Jaide session object for the device.
        conn = Jaide(ip, username, password, connect_timeout=conn_timeout,
                     session_timeout=sess_timeout, port=port)
        if write is not False:
            return write, output + function(conn, *args)
        else:
            return output + function(conn, *args)
    except errors.SSHError:
        output += color('Unable to connect to port %s on device: %s\n' %
                        (str(port), ip), 'red')
    except errors.AuthenticationError:  # NCClient auth failure
        output += color('Authentication failed for device: %s' % ip, 'red')
    except AuthenticationException:  # Paramiko auth failure
        output += color('Authentication failed for device: %s' % ip, 'red')
    except SSHException as e:
        output += color('Error connecting to device: %s\nError: %s' %
                        (ip, str(e)), 'red')
    except socket.timeout:
        output += color('Timeout exceeded connecting to device: %s' % ip, 'red')
    except socket.gaierror:
        output += color('No route to host, or invalid hostname: %s' % ip, 'red')
    except socket.error:
        output += color('The device refused the connection on port %s, or '
                        'no route to host.' % port, 'red')
    if write is not False:
        return write, output
    else:
        return output

def command(jaide, commands, format="text", xpath=False):
    output = ""
    for cmd in clean_lines(commands):
        expression = ""
        output += color('> ' + cmd + '\n', 'yel')
        # Get xpath expression from the command, if it is there.
        # If there is an xpath expr, the output will be xml,
        # overriding the req_format parameter
        #
        # Example command forcing xpath: show route % //rt-entry
        if len(cmd.split('%')) == 2:
            expression = cmd.split('%')[1].strip()
            cmd = cmd.split('%')[0] + '\n'
        elif xpath is not False:
            expression = xpath
        if expression:
            try:
                output += jaide.op_cmd(command=cmd, req_format='xml',
                                       xpath_expr=expression) + '\n'
            except lxml.etree.XMLSyntaxError:
                output += color('Xpath expression resulted in no response.\n',
                                'red')
        else:
            output += jaide.op_cmd(cmd, req_format=format) + '\n'
    return output

def shell(jaide, commands):
    out = ""
    for cmd in clean_lines(commands):
        out += color('> %s\n' % cmd, 'yel')
        out += jaide.shell_cmd(cmd) + '\n'
    return out

def get_all_keys(reactor, key_type, value_type, etcd_address):
    etcd = Client(reactor, etcd_address)
    result = yield etcd.get(b'\x00', range_end=b'\x00')

    res = {}
    for item in result.kvs:
        if key_type == u'utf8':
            key = item.key.decode('utf8')
        elif key_type == u'binary':
            key = binascii.b2a_base64(item.key).decode().strip()
        else:
            raise Exception('logic error')

        if value_type == u'json':
            value = json.loads(item.value.decode('utf8'))
        elif value_type == u'binary':
            value = binascii.b2a_base64(item.value).decode().strip()
        elif value_type == u'utf8':
            value = item.value.decode('utf8')
        else:
            raise Exception('logic error')

        res[key] = value

    returnValue(res)

def stop(self, timeout=None):
        assert self.scan_id is not None, 'No scan_id has been set'

        #
        #   Simple stop
        #
        if timeout is None:
            url = '/scans/%s/stop' % self.scan_id
            self.conn.send_request(url, method='GET')
            return

        #
        #   Stop with timeout
        #
        self.stop()

        for _ in xrange(timeout):
            time.sleep(1)

            is_running = self.get_status()['is_running']
            if not is_running:
                return

        msg = 'Failed to stop the scan in %s seconds'
        raise ScanStopTimeoutException(msg % timeout)

def xrb_address_to_public_key(address):

    address = bytearray(address, 'ascii')

    if not address.startswith(b'xrb_'):
        raise ValueError('address does not start with xrb_: %s' % address)

    if len(address) != 64:
        raise ValueError('address must be 64 chars long: %s' % address)

    address = bytes(address)
    key_b32xrb = b'1111' + address[4:56]
    key_bytes = b32xrb_decode(key_b32xrb)[3:]
    checksum = address[56:]

    if b32xrb_encode(address_checksum(key_bytes)) != checksum:
        raise ValueError('invalid address, invalid checksum: %s' % address)

    return key_bytes

def generate_account(seed=None, index=0):

    if not seed:
        seed = unhexlify(''.join(random.choice('0123456789ABCDEF') for i in range(64)))

    pair = keypair_from_seed(seed, index=index)
    result = {
        'address': public_key_to_xrb_address(pair['public']),
        'private_key_bytes': pair['private'],
        'public_key_bytes': pair['public'],
    }
    result['private_key_hex'] = hexlify(pair['private'])
    result['public_key_hex'] = hexlify(pair['public'])

    return result

def spasser(inbox, s=None):
    seq = (s or range(len(inbox)))
    return [input_ for i, input_ in enumerate(inbox) if i in seq]

def sjoiner(inbox, s=None, join=""):
    return join.join([input_ for i, input_ in enumerate(inbox) if i in s])

def load_item(inbox, type="string", remove=True, buffer=None):
    is_file, is_fifo, is_socket = False, False, False

    file = inbox[0]
    try:
        file_type = file[0]
    except:
        raise ValueError("invalid inbox item")
    if file_type == "file":
        is_file = os.path.exists(file[1])
    elif file_type == "fifo":
        is_fifo = stat.S_ISFIFO(os.stat(file[1]).st_mode)
    elif file_type == "socket":
        # how to test is valid socket?
        is_socket = True
    else:
        raise ValueError("type: %s not undertood" % file_type)


    if (is_fifo or is_socket) and (type == 'mmap'):
        raise ValueError("mmap is not supported for FIFOs and sockets")
    if (is_fifo or is_socket) and not remove:
        raise ValueError("FIFOs and sockets have to be removed")

    # get a fd and start/stop
    start = 0
    if is_fifo or is_file:
        stop = os.stat(file[1]).st_size - 1
        fd = os.open(file[1], os.O_RDONLY)
        BUFFER = (buffer or PAPY_DEFAULTS['PIPE_BUF'])
    elif is_socket:
        host, port = socket.gethostbyname(file[1]), file[2]
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.connect((host, port))
        stop = -1
        fd = sock.fileno()
        BUFFER = (buffer or PAPY_DEFAULTS['TCP_RCVBUF'])
    else:
        raise ValueError("got unknown inbox: %s" % (repr(inbox)))

    # get the data
    if type == 'mmap':
        offset = start - (start % (getattr(mmap, 'ALLOCATIONGRANULARITY', None)\
                                   or getattr(mmap, 'PAGESIZE')))
        start = start - offset
        stop = stop - offset + 1
        try:
            data = mmap.mmap(fd, stop, access=mmap.ACCESS_READ, offset=offset)
        except TypeError:
            # we're on Python 2.5
            data = mmap.mmap(fd, stop, access=mmap.ACCESS_READ)
        data.seek(start)

    elif type == 'string':
        data = []
        if stop == -1:
            while True:
                buffer_ = os.read(fd, BUFFER)
                if not buffer_:
                    break
                data.append(buffer_)
            data = "".join(data)
            # data = sock.recv(socket.MSG_WAITALL) 
            # this would read all the data from a socket
        else:
            os.lseek(fd, start, 0)
            data = os.read(fd, stop - start + 1)
    else:
        raise ValueError('type: %s not understood.' % type)

    # remove the file or close the socket
    if remove:
        if is_socket:
            # closes client socket
            sock.close()
        else:
            # pipes and files are just removed
            os.close(fd)
            os.unlink(file[1])
    else:
        os.close(fd)
    # returns a string or mmap
    return data

def pickle_dumps(inbox):
    # http://bugs.python.org/issue4074
    gc.disable()
    str_ = cPickle.dumps(inbox[0], cPickle.HIGHEST_PROTOCOL)
    gc.enable()
    return str_

def pickle_loads(inbox):
    gc.disable()
    obj = cPickle.loads(inbox[0])
    gc.enable()
    return obj

def json_dumps(inbox):
    gc.disable()
    str_ = json.dumps(inbox[0])
    gc.enable()
    return str_

def json_loads(inbox):
    gc.disable()
    obj = json.loads(inbox[0])
    gc.enable()
    return obj

def at_time_validate(ctx, param, value):
    # if they are doing commit_at, ensure the input is formatted correctly.
    if value is not None:
        if (re.search(r'([0-2]\d)(:[0-5]\d){1,2}', value) is None and
            re.search(r'\d{4}-[01]\d-[0-3]\d [0-2]\d:[0-5]\d(:[0-5]\d)?',
                      value) is None):
            raise click.BadParameter("A commit at time must be in one of the "
                                     "two formats: 'hh:mm[:ss]' or "
                                     "'yyyy-mm-dd hh:mm[:ss]' (seconds are "
                                     "optional).")
    ctx.obj['at_time'] = value
    return value

def write_validate(ctx, param, value):

    if value != ("default", "default"):
        try:
            mode, dest_file = (value[0], value[1])
        except IndexError:
            raise click.BadParameter('Expecting two arguments, one for how to '
                                     'output (s, single, m, multiple), and '
                                     'the second is a filepath where to put'
                                     ' the output.')
        if mode.lower() not in ['s', 'single', 'm', 'multiple']:
            raise click.BadParameter('The first argument of the -w/--write '
                                     'option must specifies whether to write'
                                     ' to one file per device, or all device'
                                     ' output to a single file. Valid options'
                                     ' are "s", "single", "m", and "multiple"')
        # we've passed the checks, so set the 'out' context variable to our
        # tuple of the mode, and the destination file.
        ctx.obj['out'] = (mode.lower(), dest_file)
    else:  # they didn't use -w, so set the context variable accordingly.
        ctx.obj['out'] = None

def write_out(input):
    # peel off the to_file metadata from the output.
    to_file, output = input
    if to_file != "quiet":
        try:
            # split the to_file metadata into it's separate parts.
            mode, dest_file = to_file
        except TypeError:
            # just dump the output if we had an internal problem with getting
            # the metadata.
            click.echo(output)
        else:
            ip = output.split('device: ')[1].split('\n')[0].strip()
            if mode in ['m', 'multiple']:
                # put the IP in front of the filename if we're writing each
                # device to its own file.
                dest_file = path.join(path.split(dest_file)[0], ip + "_" +
                                      path.split(dest_file)[1])
            try:
                out_file = open(dest_file, 'a+b')
            except IOError as e:
                print(color("Could not open output file '%s' for writing. "
                            "Output would have been:\n%s" %
                            (dest_file, output), 'red'))
                print(color('Here is the error for opening the output file:' +
                            str(e), 'red'))
            else:
                click.echo(output, nl=False, file=out_file)
                print(color('%s output appended to: %s' % (ip, dest_file)))
                out_file.close()

def main(ctx, host, password, port, quiet, session_timeout, connect_timeout,
         username):
    # build the list of hosts
    ctx.obj['hosts'] = [ip for ip in clean_lines(host)]
    # set the connection parameters
    ctx.obj['conn'] = {
        "username": username,
        "password": password,
        "port": port,
        "session_timeout": session_timeout,
        "connect_timeout": connect_timeout
    }
    if quiet:
        ctx.obj['out'] = "quiet"

def compare(ctx, commands):
    mp_pool = multiprocessing.Pool(multiprocessing.cpu_count() * 2)
    for ip in ctx.obj['hosts']:
        mp_pool.apply_async(wrap.open_connection, args=(ip,
                            ctx.obj['conn']['username'],
                            ctx.obj['conn']['password'],
                            wrap.compare, [commands],
                            ctx.obj['out'],
                            ctx.obj['conn']['connect_timeout'],
                            ctx.obj['conn']['session_timeout'],
                            ctx.obj['conn']['port']), callback=write_out)
    mp_pool.close()
    mp_pool.join()

def diff_config(ctx, second_host, mode):
    mp_pool = multiprocessing.Pool(multiprocessing.cpu_count() * 2)
    for ip in ctx.obj['hosts']:
        mp_pool.apply_async(wrap.open_connection, args=(ip,
                            ctx.obj['conn']['username'],
                            ctx.obj['conn']['password'],
                            wrap.diff_config, [second_host, mode],
                            ctx.obj['out'],
                            ctx.obj['conn']['connect_timeout'],
                            ctx.obj['conn']['session_timeout'],
                            ctx.obj['conn']['port']), callback=write_out)
    mp_pool.close()
    mp_pool.join()

def get_command(self, ctx, cmd_name):
    Converts a value from `from_unit` units to `to_unit` units

    :param value: value to convert
    :type value: int or str or decimal.Decimal

    :param from_unit: unit to convert from
    :type from_unit: str

    :param to_unit: unit to convert to
    :type to_unit: str

    >>> convert(value='1.5', from_unit='xrb', to_unit='krai')
    Decimal('0.0015')
    # pylint: disable=too-many-return-statements,too-many-branches

    # In order to 'hide' the endpoint, all non-POST requests should return

    # the site's default HTTP404
    if request.method != 'POST':
        raise Http404

    # If necessary, check that the topic is correct
    if hasattr(settings, 'BOUNCY_TOPIC_ARN'):
        # Confirm that the proper topic header was sent
        if 'HTTP_X_AMZ_SNS_TOPIC_ARN' not in request.META:
            return HttpResponseBadRequest('No TopicArn Header')

        # Check to see if the topic is in the settings
        # Because you can have bounces and complaints coming from multiple
        # topics, BOUNCY_TOPIC_ARN is a list
        if (not request.META['HTTP_X_AMZ_SNS_TOPIC_ARN']
                in settings.BOUNCY_TOPIC_ARN):
            return HttpResponseBadRequest('Bad Topic')

    # Load the JSON POST Body
    if isinstance(request.body, str):
        # requests return str in python 2.7
        request_body = request.body
    else:
        # and return bytes in python 3.4
        request_body = request.body.decode()
    try:
        data = json.loads(request_body)
    except ValueError:
        logger.warning('Notification Not Valid JSON: {}'.format(request_body))
        return HttpResponseBadRequest('Not Valid JSON')

    # Ensure that the JSON we're provided contains all the keys we expect
    # Comparison code from http://stackoverflow.com/questions/1285911/
    if not set(VITAL_NOTIFICATION_FIELDS) <= set(data):
        logger.warning('Request Missing Necessary Keys')
        return HttpResponseBadRequest('Request Missing Necessary Keys')

    # Ensure that the type of notification is one we'll accept
    if not data['Type'] in ALLOWED_TYPES:
        logger.info('Notification Type Not Known %s', data['Type'])
        return HttpResponseBadRequest('Unknown Notification Type')

    # Confirm that the signing certificate is hosted on a correct domain

    # AWS by default uses sns.{region}.amazonaws.com
    # On the off chance you need this to be a different domain, allow the
    # regex to be overridden in settings
    domain = urlparse(data['SigningCertURL']).netloc
    pattern = getattr(
        settings, 'BOUNCY_CERT_DOMAIN_REGEX', r"sns.[a-z0-9\-]+.amazonaws.com$"
    )
    if not re.search(pattern, domain):
        logger.warning(
            'Improper Certificate Location %s', data['SigningCertURL'])
        return HttpResponseBadRequest('Improper Certificate Location')

    # Verify that the notification is signed by Amazon
    if (getattr(settings, 'BOUNCY_VERIFY_CERTIFICATE', True)
            and not verify_notification(data)):
        logger.error('Verification Failure %s', )
        return HttpResponseBadRequest('Improper Signature')

    # Send a signal to say a valid notification has been received
    signals.notification.send(
        sender='bouncy_endpoint', notification=data, request=request)

    # Handle subscription-based messages.
    if data['Type'] == 'SubscriptionConfirmation':
        # Allow the disabling of the auto-subscription feature
        if not getattr(settings, 'BOUNCY_AUTO_SUBSCRIBE', True):
            raise Http404
        return approve_subscription(data)
    elif data['Type'] == 'UnsubscribeConfirmation':
        # We won't handle unsubscribe requests here. Return a 200 status code
        # so Amazon won't redeliver the request. If you want to remove this
        # endpoint, remove it either via the API or the AWS Console
        logger.info('UnsubscribeConfirmation Not Handled')
        return HttpResponse('UnsubscribeConfirmation Not Handled')

    try:
        message = json.loads(data['Message'])
    except ValueError:
        # This message is not JSON. But we need to return a 200 status code
        # so that Amazon doesn't attempt to deliver the message again
        logger.info('Non-Valid JSON Message Received')
        return HttpResponse('Message is not valid JSON')

    return process_message(message, data)

def process_message(message, notification):
    # Confirm that there are 'notificationType' and 'mail' fields in our
    # message
    if not set(VITAL_MESSAGE_FIELDS) <= set(message):
        # At this point we're sure that it's Amazon sending the message
        # If we don't return a 200 status code, Amazon will attempt to send us
        # this same message a few seconds later.
        logger.info('JSON Message Missing Vital Fields')
        return HttpResponse('Missing Vital Fields')

    if message['notificationType'] == 'Complaint':
        return process_complaint(message, notification)
    if message['notificationType'] == 'Bounce':
        return process_bounce(message, notification)
    if message['notificationType'] == 'Delivery':
        return process_delivery(message, notification)
    else:
        return HttpResponse('Unknown Notification Type')

def process_bounce(message, notification):
    mail = message['mail']
    complaint = message['complaint']

    if 'arrivalDate' in complaint:
        arrival_date = clean_time(complaint['arrivalDate'])
    else:
        arrival_date = None

    complaints = []
    for recipient in complaint['complainedRecipients']:
        # Create each Complaint. Save in a list for reference later.
        complaints += [Complaint.objects.create(
            sns_topic=notification['TopicArn'],
            sns_messageid=notification['MessageId'],
            mail_timestamp=clean_time(mail['timestamp']),
            mail_id=mail['messageId'],
            mail_from=mail['source'],
            address=recipient['emailAddress'],
            feedback_id=complaint['feedbackId'],
            feedback_timestamp=clean_time(complaint['timestamp']),
            useragent=complaint.get('userAgent'),
            feedback_type=complaint.get('complaintFeedbackType'),
            arrival_date=arrival_date
        )]

    # Send signals for each complaint.
    for complaint in complaints:
        signals.feedback.send(
            sender=Complaint,
            instance=complaint,
            message=message,
            notification=notification
        )

    logger.info('Logged %s Complaint(s)', str(len(complaints)))

    return HttpResponse('Complaint Processed')

def process_delivery(message, notification):
    Click on a label
    Check if the element is focused
    Check if the element is not focused
    Check that the form input element has given value.
    Submit the form having given id.
    Submit the form having given action URL.
    Check the alert text
    Check that the page title matches the given one.

        :param name: Tag name as string.
        :return: :class:`tags.Tag <tags.Tag>` object
        :rtype: tags.Tag

        :param resource: :class:`tags.Tag <tags.Tag>` object
        :return: :class:`tags.Tag <tags.Tag>` object
        :rtype: tags.Tag
        Get the remaining time-to-live of this lease.

        :returns: TTL in seconds.
        :rtype: int
        Revokes a lease. All keys attached to the lease will expire
        and be deleted.

        :returns: Response header.
        :rtype: instance of :class:`txaioetcd.Header`
        Keeps the lease alive by streaming keep alive requests from the client
        to the server and streaming keep alive responses from the server to
        the client.

        :returns: Response header.
        :rtype: instance of :class:`txaioetcd.Header`
        Makes an RPC call to the server and returns the json response

        :param action: RPC method to call
        :type action: str

        :param params: Dict of arguments to send with RPC call
        :type params: dict

        :raises: :py:exc:`nano.rpc.RPCException`
        :raises: :py:exc:`requests.exceptions.RequestException`

        >>> rpc.call(
        ...     action='account_balance',
        ...     params={
        ...         'account': 'xrb_3t6k35gi95xu6tergt6p69ck76ogmitsa8mnijtpxm9fkcm736xtoncuohr3'
        ...     })
        {'balance': '325586539664609129644855132177',
         'pending': '2309370940000000000000000000000000'}

        Process a value that will be sent to backend

        :param value: the value to return

        :param type: hint for what sort of value this is
        :type type: str

        Returns the account containing block

        :param hash: Hash of the block to return account for
        :type hash: str

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.block_account(
        ...     hash="000D1BAEC8EC208142C99059B393051BAC8380F9B5A2E6B2489A277D81789F3F"
        ... )
        "xrb_3e3j5tkog48pnny9dmfzj1r16pg8t1e76dz5tmac6iq689wyjfpi00000000"

        Reports the number of blocks in the ledger and unchecked synchronizing
        blocks

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.block_count()
        {
          "count": 1000,
          "unchecked": 10
        }

        Divide a raw amount down by the Mrai ratio.

        :param amount: Amount in raw to convert to Mrai
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.mrai_from_raw(amount=1000000000000000000000000000000)
        1

        Multiply an Mrai amount by the Mrai ratio.

        :param amount: Amount in Mrai to convert to raw
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.mrai_to_raw(amount=1)
        1000000000000000000000000000000

        Divide a raw amount down by the krai ratio.

        :param amount: Amount in raw to convert to krai
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.krai_from_raw(amount=1000000000000000000000000000)
        1
        Multiply an krai amount by the krai ratio.

        :param amount: Amount in krai to convert to raw
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.krai_to_raw(amount=1)
        1000000000000000000000000000

        Divide a raw amount down by the rai ratio.

        :param amount: Amount in raw to convert to rai
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.rai_from_raw(amount=1000000000000000000000000)
        1

        Multiply an rai amount by the rai ratio.

        :param amount: Amount in rai to convert to raw
        :type amount: int

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.rai_to_raw(amount=1)
        1000000000000000000000000

        Begin a new payment session. Searches wallet for an account that's
        marked as available and has a 0 balance. If one is found, the account
        number is returned and is marked as unavailable. If no account is
        found, a new account is created, placed in the wallet, and returned.

        :param wallet: Wallet to begin payment in
        :type wallet: str

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.payment_begin(
        ... wallet="000D1BAEC8EC208142C99059B393051BAC8380F9B5A2E6B2489A277D81789F3F"
        ... )
        "xrb_3e3j5tkog48pnny9dmfzj1r16pg8t1e76dz5tmac6iq689wyjfpi00000000"

        Marks all accounts in wallet as available for being used as a payment
        session.

        :param wallet: Wallet to init payment in
        :type wallet: str

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.payment_init(
        ...     wallet="000D1BAEC8EC208142C99059B393051BAC8380F9B5A2E6B2489A277D81789F3F"
        ... )
        True
        End a payment session.  Marks the account as available for use in a
        payment session.

        :param account: Account to mark available
        :type account: str

        :param wallet: Wallet to end payment session for
        :type wallet: str

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.payment_end(
        ...     account="xrb_3e3j5tkog48pnny9dmfzj1r16pg8t1e76dz5tmac6iq689wyjfpi00000000",
        ...     wallet="FFFD1BAEC8EC20814BBB9059B393051AAA8380F9B5A2E6B2489A277D81789EEE"
        ... )
        True
        Returns a list of pairs of representative and its voting weight

        :param count: Max amount of representatives to return
        :type count: int

        :param sorting: If true, sorts by weight
        :type sorting: bool

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.representatives()
        {
            "xrb_1111111111111111111111111111111111111111111111111117353trpda":
                3822372327060170000000000000000000000,
            "xrb_1111111111111111111111111111111111111111111111111awsq94gtecn":
                30999999999999999999999999000000,
            "xrb_114nk4rwjctu6n6tr6g6ps61g1w3hdpjxfas4xj1tq6i8jyomc5d858xr1xi":
                0
        }


        Returns the node's RPC version

        :raises: :py:exc:`nano.rpc.RPCException`

        >>> rpc.version()
        {
            "rpc_version": 1,
            "store_version": 10,
            "node_vendor": "RaiBlocks 9.0"
        }

    return next(
        (x.email for x in gh.emails() if x.verified and x.primary), None)

def authorized(resp, remote):
    if resp and 'error' in resp:
        if resp['error'] == 'bad_verification_code':
            # See https://developer.github.com/v3/oauth/#bad-verification-code
            # which recommends starting auth flow again.
            return redirect(url_for('invenio_oauthclient.login',
                                    remote_app='github'))
        elif resp['error'] in ['incorrect_client_credentials',
                               'redirect_uri_mismatch']:
            raise OAuthResponseError(
                'Application mis-configuration in GitHub', remote, resp
            )

    return authorized_signup_handler(resp, remote)

def initial_variant_sequences_from_reads(
        variant_reads,
        max_nucleotides_before_variant=None,
        max_nucleotides_after_variant=None):
    unique_sequence_groups = group_unique_sequences(
        variant_reads,
        max_prefix_size=max_nucleotides_before_variant,
        max_suffix_size=max_nucleotides_after_variant)

    return [
        VariantSequence(
            prefix=prefix,
            alt=alt,
            suffix=suffix,
            reads=reads)
        for ((prefix, alt, suffix), reads)
        in unique_sequence_groups.items()
    ]

def trim_variant_sequences(variant_sequences, min_variant_sequence_coverage):
    n_total = len(variant_sequences)
    trimmed_variant_sequences = [
        variant_sequence.trim_by_coverage(min_variant_sequence_coverage)
        for variant_sequence in variant_sequences
    ]
    collapsed_variant_sequences = collapse_substrings(trimmed_variant_sequences)
    n_after_trimming = len(collapsed_variant_sequences)
    logger.info(
        "Kept %d/%d variant sequences after read coverage trimming to >=%dx",
        n_after_trimming,
        n_total,
        min_variant_sequence_coverage)
    return collapsed_variant_sequences

def filter_variant_sequences(
        variant_sequences,
        preferred_sequence_length,
        min_variant_sequence_coverage=MIN_VARIANT_SEQUENCE_COVERAGE,):
    variant_sequences = trim_variant_sequences(
        variant_sequences, min_variant_sequence_coverage)

    return filter_variant_sequences_by_length(
        variant_sequences=variant_sequences,
        preferred_sequence_length=preferred_sequence_length)

def reads_generator_to_sequences_generator(
        variant_and_reads_generator,
        min_alt_rna_reads=MIN_ALT_RNA_READS,
        min_variant_sequence_coverage=MIN_VARIANT_SEQUENCE_COVERAGE,
        preferred_sequence_length=VARIANT_SEQUENCE_LENGTH,
        variant_sequence_assembly=VARIANT_SEQUENCE_ASSEMBLY):
    for variant, variant_reads in variant_and_reads_generator:
        variant_sequences = reads_to_variant_sequences(
            variant=variant,
            reads=variant_reads,
            min_alt_rna_reads=min_alt_rna_reads,
            min_variant_sequence_coverage=min_variant_sequence_coverage,
            preferred_sequence_length=preferred_sequence_length,
            variant_sequence_assembly=variant_sequence_assembly)
        yield variant, variant_sequences

def contains(self, other):
        return (self.alt == other.alt and
                self.prefix.endswith(other.prefix) and
                self.suffix.startswith(other.suffix))

def left_overlaps(self, other, min_overlap_size=1):

        if self.alt != other.alt:
            # allele must match!
            return False

        if len(other.prefix) > len(self.prefix):
            # only consider strings that overlap like:
            #   self: ppppAssss
            #   other:   ppAsssssss
            # which excludes cases where the other sequence has a longer
            # prefix
            return False
        elif len(other.suffix) < len(self.suffix):
            # similarly, we throw away cases where the other sequence is shorter
            # after the alt nucleotides than this sequence
            return False

        # is the other sequence a prefix of this sequence?
        # Example:
        # p1 a1 s1 = XXXXXXXX Y ZZZZZZ
        # p2 a2 s2 =       XX Y ZZZZZZZZZ
        # ...
        # then we can combine them into a longer sequence
        sequence_overlaps = (
            self.prefix.endswith(other.prefix) and
            other.suffix.startswith(self.suffix)
        )
        prefix_overlap_size = min(len(self.prefix), len(other.prefix))
        suffix_overlap_size = min(len(other.suffix), len(self.suffix))
        overlap_size = (
            prefix_overlap_size + suffix_overlap_size + len(self.alt))

        return sequence_overlaps and overlap_size >= min_overlap_size

def add_reads(self, reads):
        if len(reads) == 0:
            return self
        new_reads = self.reads.union(reads)
        if len(new_reads) > len(self.reads):
            return VariantSequence(
                prefix=self.prefix,
                alt=self.alt,
                suffix=self.suffix,
                reads=new_reads)
        else:
            return self

def variant_indices(self):
        variant_start_index = len(self.prefix)
        variant_len = len(self.alt)
        variant_end_index = variant_start_index + variant_len
        return variant_start_index, variant_end_index

def coverage(self):
        variant_start_index, variant_end_index = self.variant_indices()
        n_nucleotides = len(self)
        coverage_array = np.zeros(n_nucleotides, dtype="int32")
        for read in self.reads:
            coverage_array[
                max(0, variant_start_index - len(read.prefix)):
                min(n_nucleotides, variant_end_index + len(read.suffix))] += 1
        return coverage_array

def trim_by_coverage(self, min_reads):
        read_count_array = self.coverage()
        logger.info("Coverage: %s (len=%d)" % (
            read_count_array, len(read_count_array)))
        sufficient_coverage_mask = read_count_array >= min_reads
        sufficient_coverage_indices = np.argwhere(sufficient_coverage_mask)
        if len(sufficient_coverage_indices) == 0:
            logger.debug("No bases in %s have coverage >= %d" % (self, min_reads))
            return VariantSequence(prefix="", alt="", suffix="", reads=self.reads)
        variant_start_index, variant_end_index = self.variant_indices()
        # assuming that coverage drops off monotonically away from
        # variant nucleotides
        first_covered_index = sufficient_coverage_indices.min()
        last_covered_index = sufficient_coverage_indices.max()
        # adding 1 to last_covered_index since it's an inclusive index
        # whereas variant_end_index is the end of a half-open interval
        if (first_covered_index > variant_start_index or
                last_covered_index + 1 < variant_end_index):
            # Example:
            #   Nucleotide sequence:
            #       ACCCTTTT|AA|GGCGCGCC
            #   Coverage:
            #       12222333|44|33333211
            # Then the mask for bases covered >= 4x would be:
            #       ________|**|________
            # with indices:
            #       first_covered_index = 9
            #       last_covered_index = 10
            #       variant_start_index = 9
            #       variant_end_index = 11
            logger.debug("Some variant bases in %s don't have coverage >= %d" % (
                self, min_reads))
            return VariantSequence(prefix="", alt="", suffix="", reads=self.reads)
        return VariantSequence(
            prefix=self.prefix[first_covered_index:],
            alt=self.alt,
            suffix=self.suffix[:last_covered_index - variant_end_index + 1],
            reads=self.reads)

def trim_N_nucleotides(prefix, suffix):
    if 'N' in prefix:
        # trim prefix to exclude all occurrences of N
        rightmost_index = prefix.rfind('N')
        logger.debug(
            "Trimming %d nucleotides from read prefix '%s'",
            rightmost_index + 1, prefix)
        prefix = prefix[rightmost_index + 1:]

    if 'N' in suffix:
        leftmost_index = suffix.find('N')
        logger.debug(
            "Trimming %d nucleotides from read suffix '%s'",
            len(suffix) - leftmost_index,
            suffix)
        suffix = suffix[:leftmost_index]

    return prefix, suffix

def convert_from_bytes_if_necessary(prefix, suffix):
    if isinstance(prefix, bytes):
        prefix = prefix.decode('ascii')

    if isinstance(suffix, bytes):
        suffix = suffix.decode('ascii')

    return prefix, suffix

def publish(self, data, **kwargs):

        The caller is responsible for ensuring that the record has already been
        committed to the database. If a newer version of a record has already
        been indexed then the provided record will not be indexed. This
        behavior can be controlled by providing a different ``version_type``
        when initializing ``RecordIndexer``.

        :param record: Record instance.

        :param dict es_bulk_kwargs: Passed to
            :func:`elasticsearch:elasticsearch.helpers.bulk`.

        :param record_id_iterator: Iterator that yields record UUIDs.
        :param op_type: Indexing operation (one of ``index``, ``create``,
            ``delete`` or ``update``).
        :param index: The Elasticsearch index. (Default: ``None``)
        :param doc_type: The Elasticsearch doc_type. (Default: ``None``)

        :param message_iterator: Iterator yielding messages from a queue.

        :param payload: Decoded message body.

        :returns: Dictionary defining an Elasticsearch bulk 'delete' action.

        :param payload: Decoded message body.

        :returns: Dictionary defining an Elasticsearch bulk 'index' action.

        :param record: The record to prepare.
        :param index: The Elasticsearch index.
        :param doc_type: The Elasticsearch document type.
        :returns: The record metadata.
    Returns a list of merged VariantSequence objects, and True if any
    were successfully merged.
    Greedily merge overlapping sequences into longer sequences.

    Accepts a collection of VariantSequence objects and returns another
    collection of elongated variant sequences. The reads field of the
    returned VariantSequence object will contain reads which
    only partially overlap the full sequence.
    Combine shorter sequences which are fully contained in longer sequences.

    Parameters
    ----------
    variant_sequences : list
       List of VariantSequence objects

    Returns a (potentially shorter) list without any contained subsequences.
    Assembles longer sequences from reads centered on a variant by
    between merging all pairs of overlapping sequences and collapsing
    shorter sequences onto every longer sequence which contains them.

    Returns a list of variant sequences, sorted by decreasing read support.
    Group elements of the list `xs` by keys generated from calling `key_fn`.

    Returns a dictionary which maps keys to sub-lists of `xs`.


    The orthonormal basis generated spans the plane defined with `normal` as
    its normal vector.  The handedness of `on1` and `on2` in the returned
    basis is such that:

    .. math::

            \\mathsf{on1} \\times \\mathsf{on2} =
            {\\mathsf{normal} \\over \\left\\| \\mathsf{normal}\\right\\|}

    `normal` must be expressible as a one-dimensional |nparray| of length 3.

    Parameters
    ----------
    normal
        length-3 |npfloat_| --
        The orthonormal basis output will span the plane perpendicular
        to `normal`.

    ref_vec
        length-3 |npfloat_|, optional --
        If specified, `on1` will be the normalized projection of `ref_vec`
        onto the plane perpendicular to `normal`. Default is |None|.

    Returns
    -------
    on1
        length-3 |npfloat_| --

        First vector defining the orthonormal basis in the plane
        normal to `normal`

    on2
        length-3 |npfloat_| --

        Second vector defining the orthonormal basis in the plane
        normal to `normal`

    Raises
    ------
    ~exceptions.ValueError
        If `normal` or `ref_vec` is not expressible as a 1-D vector
        with 3 elements

    ~opan.error.VectorError
        (typecode :attr:`~opan.error.VectorError.NONPRL`)
        If `ref_vec` is specified and it is insufficiently non-
        parallel with respect to `normal`


    If a one-dimensional |nparray| is passed to `a`, it is treated as a single
    column vector, rather than a row matrix of length-one column vectors.

    The matrix `a` does not need to be square, though it must have at least
    as many rows as columns, since orthonormality is only possible in N-space
    with a set of no more than N vectors. (This condition is not directly
    checked.)

    Parameters
    ----------
    a
        R x S |npfloat_| --
        2-D array of column vectors to be checked for orthonormality.

    tol
        |npfloat_|, optional --
        Tolerance for deviation of dot products from one or zero. Default
        value is :data:`opan.const.DEF.ORTHONORM_TOL`.

    report
        |bool|, optional --
        Whether to record and return vectors / vector pairs failing the
        orthonormality condition. Default is |False|.

    Returns
    -------
    o
        |bool| --
        Indicates whether column vectors of `a` are orthonormal to within
        tolerance `tol`.

    n_fail
        |list| of |int|, or |None| --

        If `report` == |True|:

            A list of indices of column vectors
            failing the normality condition, or an empty list if all vectors
            are normalized.

        If `report` == |False|:

            |None|

    o_fail
        |list| of 2-tuples of |int|, or |None| --

        If `report` == |True|:

            A list of 2-tuples of indices of
            column vectors failing the orthogonality condition, or an
            empty list if all vectors are orthogonal.

        If `report` == |False|:

            |None|


    Vectors must be of the same dimension.

    Parameters
    ----------
    vec1
        length-R |npfloat_| --
        First vector to compare

    vec2
        length-R |npfloat_| --
        Second vector to compare

    Returns
    -------
    par
        |bool| --
        |True| if (anti-)parallel to within
        :data:`opan.const.PRM.NON_PARALLEL_TOL` degrees.  |False| otherwise.


    Calculated as:

    .. math::

         \\mathsf{vec\\_onto} * \\frac{\\mathsf{vec}\\cdot\\mathsf{vec\\_onto}}
         {\\mathsf{vec\\_onto}\\cdot\\mathsf{vec\\_onto}}

    Parameters
    ----------
    vec
        length-R |npfloat_| --
        Vector to project

    vec_onto
        length-R |npfloat_| --
        Vector onto which `vec` is to be projected

    Returns
    -------
    proj_vec
        length-R |npfloat_| --
        Projection of `vec` onto `vec_onto`


    Calculated by subtracting from `vec` the projection of `vec` onto
    `vec_onto`:

    .. math::

        \\mathsf{vec} - \\mathrm{proj}\\left(\\mathsf{vec},
        \\ \\mathsf{vec\\_onto}\\right)

    Parameters
    ----------
    vec
        length-R |npfloat_| --
        Vector to reject

    vec_onto
        length-R |npfloat_| --
        Vector onto which `vec` is to be rejected

    Returns
    -------
    rej_vec
        length-R |npfloat_| --
        Rejection of `vec` onto `vec_onto`


    Angle calculated as:

    .. math::

        \\arccos\\left[
        \\frac{\\mathsf{vec1}\cdot\\mathsf{vec2}}
        {\\left\\|\\mathsf{vec1}\\right\\|
            \\left\\|\\mathsf{vec2}\\right\\|}
        \\right]

    Parameters
    ----------
    vec1
        length-R |npfloat_| --
        First vector

    vec2
        length-R |npfloat_| --
        Second vector

    Returns
    -------
    angle
        |npfloat_| --
        Angle between the two vectors in degrees

    Do all of the gruntwork associated with creating a new module.

    Creates a DataFrame containing number of reads supporting the
    ref vs. alt alleles for each variant.

    query = 'CREATE EXTENSION IF NOT EXISTS "%s";'

    with conn.cursor() as cursor:
        cursor.execute(query, (AsIs(extension),))

    installed = check_extension(conn, extension)

    if not installed:
        raise psycopg2.ProgrammingError(
            'Postgres extension failed installation.', extension
        )

def check_extension(conn, extension: str) -> bool:
    if obj is None:

        return default or []
    if isinstance(obj, (compat.string_types, compat.integer_types)):
        return [obj]
    return obj

def iter_documents(self, fileids=None, categories=None, _destroy=False):
        try:
            with open(self._cache_filename, 'wb') as f:
                compat.pickle.dump(self._document_meta, f, 1)
        except (IOError, compat.pickle.PickleError):
            pass

def _load_meta_cache(self):
        Return documents meta information that can
        be used for fast document lookups. Meta information
        consists of documents titles, categories and positions
        in file.
        doc_str = self._get_doc_by_raw_offset(str(doc_id))
        return compat.ElementTree.XML(doc_str.encode('utf8'))

def _get_doc_by_line_offset(self, doc_id):
        bounds = self._get_meta()[str(doc_id)].bounds
        return xml_utils.load_chunk(self.filename, bounds, slow=True)

def _threeDdot_simple(M,a):
    "Return Ma, where M is a 3x3 transformation matrix, for each pixel"

    result = np.empty(a.shape,dtype=a.dtype)

    for i in range(a.shape[0]):
        for j in range(a.shape[1]):
            A = np.array([a[i,j,0],a[i,j,1],a[i,j,2]]).reshape((3,1))
            L = np.dot(M,A)
            result[i,j,0] = L[0]
            result[i,j,1] = L[1]
            result[i,j,2] = L[2]

    return result

def _swaplch(LCH):
    "Reverse the order of an LCH numpy dstack or tuple for analysis."
    try: # Numpy array
        L,C,H = np.dsplit(LCH,3)
        return np.dstack((H,C,L))
    except: # Tuple
        L,C,H = LCH
        return H,C,L

def rgb_to_hsv(self,RGB):
        "linear rgb to hsv"
        gammaRGB = self._gamma_rgb(RGB)
        return self._ABC_to_DEF_by_fn(gammaRGB,rgb_to_hsv)

def hsv_to_rgb(self,HSV):
        "hsv to linear rgb"
        gammaRGB = self._ABC_to_DEF_by_fn(HSV,hsv_to_rgb)
        return self._ungamma_rgb(gammaRGB)

def image2working(self,i):
        return self.colorspace.convert(self.image_space,
                                       self.working_space, i)

def working2analysis(self,r):
        "Transform working space inputs to the analysis color space."
        a = self.colorspace.convert(self.working_space, self.analysis_space, r)
        return self.swap_polar_HSVorder[self.analysis_space](a)

def analysis2working(self,a):
        "Convert back from the analysis color space to the working space."
        a = self.swap_polar_HSVorder[self.analysis_space](a)
        return self.colorspace.convert(self.analysis_space, self.working_space, a)

def load_chunk(filename, bounds, encoding='utf8', slow=False):
    if slow:
        return _load_chunk_slow(filename, bounds, encoding)

    with open(filename, 'rb') as f:
        f.seek(bounds.byte_start)
        size = bounds.byte_end - bounds.byte_start
        return f.read(size).decode(encoding)

def generate_numeric_range(items, lower_bound, upper_bound):

    quantile_grid = create_quantiles(items, lower_bound, upper_bound)
    labels, bounds = (zip(*quantile_grid))
    ranges = ((label, NumericRange(*bound))
              for label, bound in zip(labels, bounds))
    return ranges

def edge_average(a):
    "Return the mean value around the edge of an array."

    if len(np.ravel(a)) < 2:
        return float(a[0])
    else:
        top_edge = a[0]
        bottom_edge = a[-1]
        left_edge = a[1:-1,0]
        right_edge = a[1:-1,-1]

        edge_sum = np.sum(top_edge) + np.sum(bottom_edge) + np.sum(left_edge) + np.sum(right_edge)
        num_values = len(top_edge)+len(bottom_edge)+len(left_edge)+len(right_edge)

        return float(edge_sum)/num_values

def _process_channels(self,p,**params_to_override):
        orig_image = self._image

        for i in range(len(self._channel_data)):
            self._image = self._original_channel_data[i]
            self._channel_data[i] = self._reduced_call(**params_to_override)
        self._image = orig_image
        return self._channel_data

def set_matrix_dimensions(self, *args):
        self._image = None
        super(FileImage, self).set_matrix_dimensions(*args)

def _load_pil_image(self, filename):
        self._channel_data = []
        self._original_channel_data = []

        im = Image.open(filename)
        self._image = ImageOps.grayscale(im)
        im.load()

        file_data = np.asarray(im, float)
        file_data = file_data / file_data.max()

        # if the image has more than one channel, load them
        if( len(file_data.shape) == 3 ):
            num_channels = file_data.shape[2]
            for i in range(num_channels):
                self._channel_data.append( file_data[:, :, i])
                self._original_channel_data.append( file_data[:, :, i] )

def _load_npy(self, filename):
        self._channel_data = []
        self._original_channel_data = []
        file_channel_data = np.load(filename)
        file_channel_data = file_channel_data / file_channel_data.max()

        for i in range(file_channel_data.shape[2]):
            self._channel_data.append(file_channel_data[:, :, i])
            self._original_channel_data.append(file_channel_data[:, :, i])

        self._image = file_channel_data.sum(2) / file_channel_data.shape[2]

def ok_kwarg(val):
    if delayed:
        celery_kwargs = {
            'kwargs': {
                'version_type': version_type,
                'es_bulk_kwargs': {'raise_on_error': raise_on_error},
            }
        }
        click.secho(
            'Starting {0} tasks for indexing records...'.format(concurrency),
            fg='green')
        if queue is not None:
            celery_kwargs.update({'queue': queue})
        for c in range(0, concurrency):
            process_bulk_queue.apply_async(**celery_kwargs)
    else:
        click.secho('Indexing records...', fg='green')
        RecordIndexer(version_type=version_type).process_bulk_queue(
            es_bulk_kwargs={'raise_on_error': raise_on_error})

def reindex(pid_type):
    click.secho('Sending records to indexing queue ...', fg='green')

    query = (x[0] for x in PersistentIdentifier.query.filter_by(
        object_type='rec', status=PIDStatus.REGISTERED
    ).filter(
        PersistentIdentifier.pid_type.in_(pid_type)
    ).values(
        PersistentIdentifier.object_uuid
    ))
    RecordIndexer().bulk_index(query)
    click.secho('Execute "run" command to process the queue!',
                fg='yellow')

def process_actions(actions):

    def action(queue):
        queue.declare()
        click.secho('Indexing queue has been initialized.', fg='green')
        return queue
    return action

def purge_queue():

    def action(queue):
        queue.delete()
        click.secho('Indexing queue has been deleted.', fg='green')
        return queue
    return action

def variant_matches_reference_sequence(variant, ref_seq_on_transcript, strand):
    if strand == "-":
        ref_seq_on_transcript = reverse_complement_dna(ref_seq_on_transcript)
    return ref_seq_on_transcript == variant.ref

def from_variant_and_transcript(
            cls, variant, transcript, context_size):

        full_transcript_sequence = transcript.sequence

        if full_transcript_sequence is None:
            logger.warn(
                "Expected transcript %s (overlapping %s) to have sequence",
                transcript.name,
                variant)
            return None

        # get the interbase range of offsets which capture all reference
        # bases modified by the variant
        variant_start_offset, variant_end_offset = \
            interbase_range_affected_by_variant_on_transcript(
                variant=variant,
                transcript=transcript)

        reference_cdna_at_variant = full_transcript_sequence[
            variant_start_offset:variant_end_offset]

        if not variant_matches_reference_sequence(
                variant=variant,
                strand=transcript.strand,
                ref_seq_on_transcript=reference_cdna_at_variant):
            logger.warn(
                "Variant %s doesn't match reference sequence on transcript %s: "
                "may span splice junction",
                variant,
                transcript)
            return None

        if len(full_transcript_sequence) < 6:
            # need at least 6 nucleotides for a start and stop codon
            logger.warn(
                "Sequence of %s (overlapping %s) too short: %d",
                transcript,
                variant,
                len(full_transcript_sequence))
            return None

        logger.info(
            "Interbase offset range on %s for variant %s = %d:%d",
            transcript.name,
            variant,
            variant_start_offset,
            variant_end_offset)

        reference_cdna_before_variant = full_transcript_sequence[
            max(0, variant_start_offset - context_size):
            variant_start_offset]

        reference_cdna_after_variant = full_transcript_sequence[
            variant_end_offset:
            variant_end_offset + context_size]

        return ReferenceSequenceKey(
            strand=transcript.strand,
            sequence_before_variant_locus=reference_cdna_before_variant,
            sequence_at_variant_locus=reference_cdna_at_variant,
            sequence_after_variant_locus=reference_cdna_after_variant)

def wrap(lower, upper, x):
    #I have no idea how I came up with this algorithm; it should be simplified.
    #
    # Note that Python's % operator works on floats and arrays;
    # usually one can simply use that instead.  E.g. to wrap array or
    # scalar x into 0,2*pi, just use "x % (2*pi)".
    range_=upper-lower
    return lower + np.fmod(x-lower + 2*range_*(1-np.floor(x/(2*range_))), range_)

def _pixelsize(self, p):
        h = line(y, self._effective_thickness(p), 0.0)
        return h.sum()

def num_channels(self):
        if(self.inspect_value('index') is None):
            if(len(self.generators)>0):
                return self.generators[0].num_channels()
            return 0

        return self.get_current_generator().num_channels()

def _set_frequency_spacing(self, min_freq, max_freq):

        self.frequency_spacing = np.linspace(min_freq, max_freq, num=self._sheet_dimensions[0]+1, endpoint=True)

def get_postgres_encoding(python_encoding: str) -> str:

        Returns a |dict| providing the various energy values from the
        last SCF cycle performed in the output. Keys are those of
        :attr:`~opan.output.OrcaOutput.p_en`.
        Any energy value not relevant to the parsed
        output is assigned as |None|.

        Returns
        -------
        last_ens
            |dict| of |npfloat_|--
            Energies from the last SCF present in the output.


    host = host or os.environ['PGHOST']
    database = database or os.environ['PGDATABASE']
    user = user or os.environ['PGUSER']
    password = password or os.environ['PGPASSWORD']

    return psycopg2.connect(host=host,
                            database=database,
                            user=user,
                            password=password,
                            **kwargs)

def _setup():
    _SOCKET.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
    _SOCKET.bind(('', PORT))
    udp = threading.Thread(target=_listen, daemon=True)
    udp.start()

def discover(timeout=DISCOVERY_TIMEOUT):
    hosts = {}
    payload = MAGIC + DISCOVERY
    for _ in range(RETRIES):
        _SOCKET.sendto(bytearray(payload), ('255.255.255.255', PORT))
        start = time.time()
        while time.time() < start + timeout:
            for host, data in _BUFFER.copy().items():
                if not _is_discovery_response(data):
                    continue
                if host not in hosts:
                    _LOGGER.debug("Discovered device at %s", host)
                    entry = {}
                    entry['mac'] = data[7:13]
                    entry['imac'] = data[19:25]
                    entry['next'] = 0
                    entry['st'] = int(data[-1])
                    entry['time'] = _device_time(data[37:41])
                    entry['serverTime'] = int(time.time())
                    hosts[host] = entry
    return hosts

def _discover_mac(self):
        mac = None
        mac_reversed = None
        cmd = MAGIC + DISCOVERY
        resp = self._udp_transact(cmd, self._discovery_resp,
                                  broadcast=True,
                                  timeout=DISCOVERY_TIMEOUT)
        if resp:
            (mac, mac_reversed) = resp
        if mac is None:
            raise S20Exception("Couldn't discover {}".format(self.host))
        return (mac, mac_reversed)

def _subscribe(self):
        cmd = MAGIC + SUBSCRIBE + self._mac \
            + PADDING_1 + self._mac_reversed + PADDING_1
        status = self._udp_transact(cmd, self._subscribe_resp)
        if status is not None:
            self.last_subscribed = time.time()
            return status == ON
        else:
            raise S20Exception(
                "No status could be found for {}".format(self.host))

def _control(self, state):

        # Renew subscription if necessary
        if not self._subscription_is_recent():
            self._subscribe()

        cmd = MAGIC + CONTROL + self._mac + PADDING_1 + PADDING_2 + state
        _LOGGER.debug("Sending new state to %s: %s", self.host, ord(state))
        ack_state = self._udp_transact(cmd, self._control_resp, state)
        if ack_state is None:
            raise S20Exception(
                "Device didn't acknowledge control request: {}".format(
                    self.host))

def _discovery_resp(self, data):
        if _is_discovery_response(data):
            _LOGGER.debug("Discovered MAC of %s: %s", self.host,
                          binascii.hexlify(data[7:13]).decode())
            return (data[7:13], data[19:25])

def _subscribe_resp(self, data):
        if _is_subscribe_response(data):
            status = bytes([data[23]])
            _LOGGER.debug("Successfully subscribed to %s, state: %s",
                          self.host, ord(status))
            return status

def _control_resp(self, data, state):
        if _is_control_response(data):
            ack_state = bytes([data[22]])
            if state == ack_state:
                _LOGGER.debug("Received state ack from %s, state: %s",
                              self.host, ord(ack_state))
                return ack_state

def _udp_transact(self, payload, handler, *args,
                      broadcast=False, timeout=TIMEOUT):
        if self.host in _BUFFER:
            del _BUFFER[self.host]
        host = self.host
        if broadcast:
            host = '255.255.255.255'
        retval = None
        for _ in range(RETRIES):
            _SOCKET.sendto(bytearray(payload), (host, PORT))
            start = time.time()
            while time.time() < start + timeout:
                data = _BUFFER.get(self.host, None)
                if data:
                    retval = handler(data, *args)
                # Return as soon as a response is received
                if retval:
                    return retval

def load(source):
    parser = get_xml_parser()
    return etree.parse(source, parser=parser).getroot()

def translation_generator(
        variant_sequences,
        reference_contexts,
        min_transcript_prefix_length,
        max_transcript_mismatches,
        include_mismatches_after_variant,
        protein_sequence_length=None):
    for reference_context in reference_contexts:
        for variant_sequence in variant_sequences:
            translation = Translation.from_variant_sequence_and_reference_context(
                variant_sequence=variant_sequence,
                reference_context=reference_context,
                min_transcript_prefix_length=min_transcript_prefix_length,
                max_transcript_mismatches=max_transcript_mismatches,
                include_mismatches_after_variant=include_mismatches_after_variant,
                protein_sequence_length=protein_sequence_length)
            if translation is not None:
                yield translation

def translate_variant_reads(
        variant,
        variant_reads,
        protein_sequence_length,
        transcript_id_whitelist=None,
        min_alt_rna_reads=MIN_ALT_RNA_READS,
        min_variant_sequence_coverage=MIN_VARIANT_SEQUENCE_COVERAGE,
        min_transcript_prefix_length=MIN_TRANSCRIPT_PREFIX_LENGTH,
        max_transcript_mismatches=MAX_REFERENCE_TRANSCRIPT_MISMATCHES,
        include_mismatches_after_variant=INCLUDE_MISMATCHES_AFTER_VARIANT,
        variant_sequence_assembly=VARIANT_SEQUENCE_ASSEMBLY):
    if len(variant_reads) == 0:
        logger.info("No supporting reads for variant %s", variant)
        return []

    # Adding an extra codon to the desired RNA sequence length in case we
    # need to clip nucleotides at the start/end of the sequence
    cdna_sequence_length = (protein_sequence_length + 1) * 3

    variant_sequences = reads_to_variant_sequences(
        variant=variant,
        reads=variant_reads,
        preferred_sequence_length=cdna_sequence_length,
        min_alt_rna_reads=min_alt_rna_reads,
        min_variant_sequence_coverage=min_variant_sequence_coverage,
        variant_sequence_assembly=variant_sequence_assembly)

    if not variant_sequences:
        logger.info("No spanning cDNA sequences for variant %s", variant)
        return []

    # try translating the variant sequences from the same set of
    # ReferenceContext objects, which requires using the longest
    # context_size to be compatible with all of the sequences. Some
    # sequences maybe have fewer nucleotides than this before the variant
    # and will thus have to be trimmed.
    context_size = max(
        len(variant_sequence.prefix)
        for variant_sequence in variant_sequences)

    reference_contexts = reference_contexts_for_variant(
        variant,
        context_size=context_size,
        transcript_id_whitelist=transcript_id_whitelist)

    return list(translation_generator(
        variant_sequences=variant_sequences,
        reference_contexts=reference_contexts,
        min_transcript_prefix_length=min_transcript_prefix_length,
        max_transcript_mismatches=max_transcript_mismatches,
        include_mismatches_after_variant=include_mismatches_after_variant,
        protein_sequence_length=protein_sequence_length))

def as_translation_key(self):
        return TranslationKey(**{
            name: getattr(self, name)
            for name in TranslationKey._fields})

def from_variant_sequence_and_reference_context(
            cls,
            variant_sequence,
            reference_context,
            min_transcript_prefix_length,
            max_transcript_mismatches,
            include_mismatches_after_variant,
            protein_sequence_length=None):
        variant_sequence_in_reading_frame = match_variant_sequence_to_reference_context(
            variant_sequence,
            reference_context,
            min_transcript_prefix_length=min_transcript_prefix_length,
            max_transcript_mismatches=max_transcript_mismatches,
            include_mismatches_after_variant=include_mismatches_after_variant)

        if variant_sequence_in_reading_frame is None:
            logger.info("Unable to determine reading frame for %s", variant_sequence)
            return None

        cdna_sequence = variant_sequence_in_reading_frame.cdna_sequence
        cdna_codon_offset = variant_sequence_in_reading_frame.offset_to_first_complete_codon

        # get the offsets into the cDNA sequence which pick out the variant nucleotides
        cdna_variant_start_offset = variant_sequence_in_reading_frame.variant_cdna_interval_start
        cdna_variant_end_offset = variant_sequence_in_reading_frame.variant_cdna_interval_end

        # TODO: determine if the first codon is the start codon of a
        # transcript, for now any of the unusual start codons like CTG
        # will translate to leucine instead of methionine.
        variant_amino_acids, ends_with_stop_codon = translate_cdna(
            cdna_sequence[cdna_codon_offset:],
            first_codon_is_start=False,
            mitochondrial=reference_context.mitochondrial)

        variant_aa_interval_start, variant_aa_interval_end, frameshift = \
            find_mutant_amino_acid_interval(
                cdna_sequence=cdna_sequence,
                cdna_first_codon_offset=cdna_codon_offset,
                cdna_variant_start_offset=cdna_variant_start_offset,
                cdna_variant_end_offset=cdna_variant_end_offset,
                n_ref=len(reference_context.sequence_at_variant_locus),
                n_amino_acids=len(variant_amino_acids))

        if protein_sequence_length and len(variant_amino_acids) > protein_sequence_length:
            if protein_sequence_length <= variant_aa_interval_start:
                logger.warn(
                    ("Truncating amino acid sequence %s "
                     "to only %d elements loses all variant residues"),
                    variant_amino_acids,
                    protein_sequence_length)
                return None
            # if the protein is too long then shorten it, which implies
            # we're no longer stopping due to a stop codon and that the variant
            # amino acids might need a new stop index
            variant_amino_acids = variant_amino_acids[:protein_sequence_length]
            variant_aa_interval_end = min(variant_aa_interval_end, protein_sequence_length)
            ends_with_stop_codon = False

        return Translation(
            amino_acids=variant_amino_acids,
            frameshift=frameshift,
            ends_with_stop_codon=ends_with_stop_codon,
            variant_aa_interval_start=variant_aa_interval_start,
            variant_aa_interval_end=variant_aa_interval_end,
            untrimmed_variant_sequence=variant_sequence,
            reference_context=reference_context,
            variant_sequence_in_reading_frame=variant_sequence_in_reading_frame)

def postComponents(self, name, status, **kwargs):
        '''

        kwargs['name'] = name
        kwargs['status'] = status
        return self.__postRequest('/components', kwargs)

def postIncidents(self, name, message, status, visible, **kwargs):
        '''

        kwargs['name'] = name
        kwargs['message'] = message
        kwargs['status'] = status
        kwargs['visible'] = visible
        return self.__postRequest('/incidents', kwargs)

def postMetrics(self, name, suffix, description, default_value, **kwargs):
        '''

        kwargs['name'] = name
        kwargs['suffix'] = suffix
        kwargs['description'] = description

        kwargs['default_value'] = default_value
        return self.__postRequest('/metrics', kwargs)

def postMetricsPointsByID(self, id, value, **kwargs):
        '''

        kwargs['value'] = value
        return self.__postRequest('/metrics/%s/points' % id, kwargs)

def ctr_mass(geom, masses):

    # Imports
    import numpy as np
    from .base import safe_cast as scast

    # Shape check
    if len(geom.shape) != 1:
        raise ValueError("Geometry is not a vector")
    ## end if
    if len(masses.shape) != 1:
        raise ValueError("Masses cannot be parsed as a vector")
    ## end if
    if not geom.shape[0] % 3 == 0:
        raise ValueError("Geometry is not length-3N")
    ## end if
    if geom.shape[0] != 3*masses.shape[0] and geom.shape[0] != masses.shape[0]:
        raise ValueError("Inconsistent geometry and masses vector lengths")
    ## end if

    # If N masses are provided, expand to 3N; if 3N, retain.
    if geom.shape[0] == 3*masses.shape[0]:
        masses = masses.repeat(3)
    ## end if

    # Calculate the mass-weighted coordinates, reshape to group by coordinate
    #  column-wise, sum each column, then divide by the sum of masses, which
    #  must further be divided by three because there are three replicates
    #  (possibly perturbed) of the mass of each atom.
    ctr = np.multiply(geom, masses).reshape((geom.shape[0]//3, 3)) \
                                .sum(axis=0).squeeze() / (masses.sum() / 3)

    # Return the vector
    return ctr

def ctr_geom(geom, masses):

    # Imports
    import numpy as np

    # Calculate the shift vector. Possible bad shape of geom or masses is
    #  addressed internally by the ctr_mass call.
    shift = np.tile(ctr_mass(geom, masses), geom.shape[0] / 3)

    # Shift the geometry and return
    ctr_geom = geom - shift
    return ctr_geom

def inertia_tensor(geom, masses):

    # Imports
    import numpy as np

    # Center the geometry. Takes care of any improper shapes of geom or
    #  masses via the internal call to 'ctr_mass' within the call to 'ctr_geom'
    geom = ctr_geom(geom, masses)

    # Expand the masses if required. Shape should only ever be (N,) or (3N,),
    #  else would raise an exception within the above 'ctr_geom' call
    if geom.shape[0] == 3*masses.shape[0]:
        masses = masses.repeat(3)
    ## end if

    # Initialize the tensor matrix
    tensor = np.zeros((3,3))

    # Fill the matrix
    for i in range(3):
        for j in range(i,3):
            if i == j:
                # On-diagonal element; calculate indices to include
                ind = np.concatenate([np.array(list(map(lambda v: v % 3,
                                        range(i+1, i+3)))) + o for o in
                                        range(0,geom.shape[0],3)])

                # Calculate the tensor element
                tensor[i,i] = np.multiply(np.square(geom[ind]),
                                                        masses[ind]).sum()
            else:
                # Off-diagonal element; calculate the indices
                ind_i = np.array(range(i,geom.shape[0]+i,3))
                ind_j = np.array(range(j,geom.shape[0]+j,3))

                # Calculate the tensor element and its symmetric partner
                tensor[i,j] = np.multiply(
                        np.sqrt(np.multiply(masses[ind_i], masses[ind_j])) ,
                        np.multiply(geom[ind_i], geom[ind_j]) ).sum() * -1
                tensor[j,i] = tensor[i,j]
            ## end if
        ## next j
    ## next i

    # Return the tensor
    return tensor

def rot_consts(geom, masses, units=_EURC.INV_INERTIA, on_tol=_DEF.ORTHONORM_TOL):

    # Imports
    import numpy as np
    from ..const import EnumTopType as ETT, EnumUnitsRotConst as EURC, PRM, PHYS

    # Ensure units are valid
    if not units in EURC:
        raise ValueError("'{0}' is not a valid units value".format(units))
    ## end if

    # Retrieve the moments, axes and top type. Geom and masses are proofed
    #  internally in this call.
    mom, ax, top = principals(geom, masses, on_tol)

    # Check for special cases
    if top == ETT.ATOM:
        # All moments are zero; set to zero-moment threshold
        mom = np.repeat(PRM.ZERO_MOMENT_TOL, 3)
    elif top == ETT.LINEAR:
        # First moment is zero; set to zero-moment threshold
        mom[0] = PRM.ZERO_MOMENT_TOL
    ## end if

    # Calculate the values in the indicated units
    if units == EURC.INV_INERTIA:            # 1/(amu*B^2)
        rc = 1.0 / (2.0 * mom)
    elif units == EURC.ANGFREQ_ATOMIC:       # 1/Ta
        rc = PHYS.PLANCK_BAR / (2.0 * mom * PHYS.ME_PER_AMU)
    elif units == EURC.ANGFREQ_SECS:      # 1/s
        rc = PHYS.PLANCK_BAR / (2.0 * mom * PHYS.ME_PER_AMU) / PHYS.SEC_PER_TA
    elif units == EURC.CYCFREQ_ATOMIC:    # cyc/Ta
        rc = PHYS.PLANCK_BAR / (4.0 * np.pi * mom * PHYS.ME_PER_AMU)
    elif units == EURC.CYCFREQ_HZ:        # cyc/s
        rc = PHYS.PLANCK_BAR / (4.0 * np.pi * mom * PHYS.ME_PER_AMU) / \
                                                            PHYS.SEC_PER_TA
    elif units == EURC.CYCFREQ_MHZ:       # Mcyc/s
        rc = PHYS.PLANCK_BAR / (4.0 * np.pi * mom * PHYS.ME_PER_AMU) / \
                                                    PHYS.SEC_PER_TA / 1.0e6
    elif units == EURC.WAVENUM_ATOMIC:       # cyc/B
        rc = PHYS.PLANCK / (mom * PHYS.ME_PER_AMU) / \
            (8.0 * np.pi**2.0 * PHYS.LIGHT_SPEED)
    elif units == EURC.WAVENUM_CM:           # cyc/cm
        rc = PHYS.PLANCK / (mom * PHYS.ME_PER_AMU) / \
            (8.0 * np.pi**2.0 * PHYS.LIGHT_SPEED * PHYS.ANG_PER_BOHR) * 1.0e8
    else:               # pragma: no cover -- Valid units; not implemented
        raise NotImplementedError("Units conversion not yet implemented.")
    ## end if

    # Return the result
    return rc

def _fadn_orth(vec, geom):

    # Imports
    import numpy as np
    from scipy import linalg as spla
    from ..const import PRM
    from ..error import InertiaError
    from .vector import orthonorm_check as onchk

    # Geom and vec must both be the right shape
    if not (len(geom.shape) == 1 and geom.shape[0] % 3 == 0):
        raise ValueError("Geometry is not length 3N")
    ## end if
    if not vec.shape == (3,):
        raise ValueError("Reference vector is not length 3")
    ## end if

    # vec must not be the zero vector
    if spla.norm(vec) < PRM.ZERO_VEC_TOL:
        raise ValueError("Reference vector norm is too small")
    ## end if

    # Normalize the ref vec
    vec = vec / spla.norm(vec)

    # Iterate over reshaped geometry
    for disp in geom.reshape((geom.shape[0]//3, 3)):
        # See if the displacement is nonzero and not orthonormal. Trailing
        #  [0] index is to retrieve only the success/fail bool.
        if spla.norm(disp) >= PRM.ZERO_VEC_TOL and not onchk(
                np.column_stack((disp / spla.norm(disp),
                vec / spla.norm(vec))))[0]:
            # This is the displacement you are looking for
            out_vec = disp / spla.norm(disp)
            return out_vec
            ## end if
        ## end if
    ## next disp
    else:
        # Nothing fit the bill - must be atom, linear, or planar
        raise InertiaError(InertiaError.BAD_GEOM,
                    "No suitable atomic displacement found", "")

def _fadn_par(vec, geom):

    # Imports
    import numpy as np
    from scipy import linalg as spla
    from ..const import PRM
    from ..error import InertiaError
    from .vector import parallel_check as parchk

    # Geom and vec must both be the right shape
    if not (len(geom.shape) == 1 and geom.shape[0] % 3 == 0):
        raise ValueError("Geometry is not length 3N")
    ## end if
    if not vec.shape == (3,):
        raise ValueError("Reference vector is not length 3")
    ## end if

    # vec must not be the zero vector
    if spla.norm(vec) < PRM.ZERO_VEC_TOL:
        raise ValueError("Reference vector norm is too small")
    ## end if

     # Normalize the ref vec
    vec = vec / spla.norm(vec)

    # Iterate over reshaped geometry
    for disp in geom.reshape((geom.shape[0]//3, 3)):
        # See if the displacement is nonzero and nonparallel to the ref vec
        if spla.norm(disp) >= PRM.ZERO_VEC_TOL and \
                not parchk(disp.reshape(3), vec):
            # This is the displacement you are looking for
            out_vec = disp / spla.norm(disp)
            break
            ## end if
        ## end if
    ## next disp
    else:
        # Nothing fit the bill - must be a linear molecule?
        raise InertiaError(InertiaError.BAD_GEOM,
                    "Linear molecule, no non-parallel displacement", "")
    ## end for disp

    # Return the resulting vector
    return out_vec

def reference_contexts_for_variants(
        variants,
        context_size,
        transcript_id_whitelist=None):
    result = OrderedDict()
    for variant in variants:
        result[variant] = reference_contexts_for_variant(
            variant=variant,
            context_size=context_size,
            transcript_id_whitelist=transcript_id_whitelist)
    return result

def variants_to_reference_contexts_dataframe(
        variants,
        context_size,
        transcript_id_whitelist=None):

    df_builder = DataFrameBuilder(
        ReferenceContext,
        exclude=["variant"],
        converters=dict(transcripts=lambda ts: ";".join(t.name for t in ts)),
        extra_column_fns={
            "gene": lambda variant, _: ";".join(variant.gene_names),
        })
    for variant, reference_contexts in reference_contexts_for_variants(
            variants=variants,
            context_size=context_size,
            transcript_id_whitelist=transcript_id_whitelist).items():
        df_builder.add_many(variant, reference_contexts)
    return df_builder.to_dataframe()

def exponential(x, y, xscale, yscale):
    if xscale==0.0 or yscale==0.0:
        return x*0.0

    with float_error_ignore():
        x_w = np.divide(x,xscale)
        y_h = np.divide(y,yscale)
        return np.exp(-np.sqrt(x_w*x_w+y_h*y_h))

def line(y, thickness, gaussian_width):
    distance_from_line = abs(y)
    gaussian_y_coord = distance_from_line - thickness/2.0
    sigmasq = gaussian_width*gaussian_width

    if sigmasq==0.0:
        falloff = y*0.0
    else:
        with float_error_ignore():
            falloff = np.exp(np.divide(-gaussian_y_coord*gaussian_y_coord,2*sigmasq))

    return np.where(gaussian_y_coord<=0, 1.0, falloff)

def disk(x, y, height, gaussian_width):
    disk_radius = height/2.0

    distance_from_origin = np.sqrt(x**2+y**2)
    distance_outside_disk = distance_from_origin - disk_radius
    sigmasq = gaussian_width*gaussian_width

    if sigmasq==0.0:
        falloff = x*0.0
    else:
        with float_error_ignore():
            falloff = np.exp(np.divide(-distance_outside_disk*distance_outside_disk,
                                  2*sigmasq))

    return np.where(distance_outside_disk<=0,1.0,falloff)

def smooth_rectangle(x, y, rec_w, rec_h, gaussian_width_x, gaussian_width_y):

    gaussian_x_coord = abs(x)-rec_w/2.0
    gaussian_y_coord = abs(y)-rec_h/2.0

    box_x=np.less(gaussian_x_coord,0.0)
    box_y=np.less(gaussian_y_coord,0.0)
    sigmasq_x=gaussian_width_x*gaussian_width_x
    sigmasq_y=gaussian_width_y*gaussian_width_y

    with float_error_ignore():
        falloff_x=x*0.0 if sigmasq_x==0.0 else \
            np.exp(np.divide(-gaussian_x_coord*gaussian_x_coord,2*sigmasq_x))
        falloff_y=y*0.0 if sigmasq_y==0.0 else \
            np.exp(np.divide(-gaussian_y_coord*gaussian_y_coord,2*sigmasq_y))

    return np.minimum(np.maximum(box_x,falloff_x), np.maximum(box_y,falloff_y))

def pack_tups(*args):

    # Imports
    import numpy as np

    # Debug flag
    _DEBUG = False

    # Marker value for non-iterable items
    NOT_ITER = -1

    # Uninitialized test value
    UNINIT_VAL = -1

    # Print the input if in debug mode
    if _DEBUG: # pragma: no cover
        print("args = {0}".format(args))

    # Non-iterable subclass of str
    class StrNoIter(str):

    Ensures that `invar` properly casts to `totype`. Checks after
    casting that the result is actually of type `totype`. Any exceptions raised
    by the typecast itself are unhandled.

    Parameters
    ----------
    invar
        (arbitrary) -- Value to be typecast.

    totype
        |type| --  Type to which `invar` is to be cast.

    Returns
    -------
    outvar
        `type 'totype'` --  Typecast version of `invar`

    Raises
    ------
    ~exceptions.TypeError
        If result of typecast is not of type `totype`


    Assumes numeric input of a time interval in seconds.  Converts this
    interval to a string of the format "#h #m #s", indicating the number of
    hours, minutes, and seconds in the interval.  Intervals greater than 24h
    are unproblematic.

    Parameters
    ----------
    el_time
        |int| or |float| --
        Time interval in seconds to be converted to h/m/s format

    Returns
    -------
    stamp
        |str| -- String timestamp in #h #m #s format


    Cartesian coordinates are considered consistent with the input
    coords if each component matches to within `tol`.  If coords or
    atoms vectors are passed that are of mismatched lengths, a
    |False| value is returned.

    Both coords vectors must be three times the length of the atoms vectors
    or a :exc:`~exceptions.ValueError` is raised.

    Parameters
    ----------
    c1
        length-3N |npfloat_| --
        Vector of first set of stacked 'lab-frame' Cartesian coordinates

    a1
        length-N |str| or |int| --
        Vector of first set of atom symbols or atomic numbers

    c2
        length-3N |npfloat_| --
        Vector of second set of stacked 'lab-frame' Cartesian coordinates

    a2
        length-N |str| or |int| --
        Vector of second set of atom symbols or atomic numbers

    tol
        |float|, optional --
        Tolerance for acceptable deviation of each geometry coordinate
        from that in the reference instance to still be considered
        matching. Default value is specified by
        :attr:`opan.const.DEF.XYZ_COORD_MATCH_TOL`)

    Returns
    -------
    match
        |bool| --
        Whether input coords and atoms match (|True|) or
        not (|False|)

    fail_type
        :class:`~opan.const.EnumCheckGeomMismatch` or |None|
        -- Type of check failure

        If `match` == |True|:

            Returns as |None|

        If `match` == |False|:

            An :class:`~opan.const.EnumCheckGeomMismatch` value
            indicating the reason for the failed match:

                :attr:`~opan.const.EnumCheckGeomMismatch.DIMENSION`
                -- Mismatch in geometry size (number of atoms)

                :attr:`~opan.const.EnumCheckGeomMismatch.COORDS`
                -- Mismatch in one or more coordinates

                :attr:`~opan.const.EnumCheckGeomMismatch.ATOMS`
                -- Mismatch in one or more atoms

    fail_loc
        length-3N |bool| or length-N |bool| or |None| --
        Mismatched elements

        If `match` == |True|:

            Returns as |None|

        If `match` == |False|:

            For "array-level" problems such as a dimension mismatch, a
            |None| value is returned.

            For "element-level" problems, a vector is returned
            indicating positions of mismatch in either `coords` or `atoms`,
            depending on the value of `fail_type`.

                |True| elements indicate **MATCHING** values

                |False| elements mark **MISMATCHES**

    Raises
    ------
    ~exceptions.ValueError
        If a pair of coords & atoms array lengths is inconsistent:

        .. code-block:: python

            if len(c1) != 3 * len(a1) or len(c2) != 3 * len(a2):
                raise ValueError(...)


    For substitutions into template input files for external computational
    packages, no checks for valid syntax are performed.

    Each key in `subs` corresponds to a delimited
    substitution tag to be replaced in `template` by the entire text of the
    value of that key. For example, the dict ``{"ABC": "text"}`` would
    convert ``The <ABC> is working`` to  ``The text is working``, using the

    default delimiters of '<' and '>'. Substitutions are performed in
    iteration order from `subs`; recursive substitution
    as the tag parsing proceeds is thus
    feasible if an :class:`~collections.OrderedDict` is used and substitution
    key/value pairs are added in the proper order.

    Start and end delimiters for the tags are modified by `delims`. For
    example, to substitute a tag of the form **{\|TAG\|}**, the tuple
    ``("{|","|}")`` should be passed to `subs_delims`.  Any elements in
    `delims` past the second are ignored. No checking is
    performed for whether the delimiters are "sensible" or not.

    Parameters
    ----------
    template
        |str| --
        Template containing tags delimited by `subs_delims`,
        with tag names and substitution contents provided in `subs`

    subs
        |dict| of |str| --
        Each item's key and value are the tag name and corresponding content to
        be substituted into the provided template.

    delims
        iterable of |str| --
        Iterable containing the 'open' and 'close' strings used to mark tags
        in the template, which are drawn from elements zero and one,
        respectively. Any elements beyond these are ignored.

    Returns
    -------
    subst_text
        |str| --
        String generated from the parsed template, with all tag
        substitutions performed.


    Pass |None| to `varname` if `obj` itself is to be checked.
    Otherwise, `varname` is the string name of the attribute of `obj` to
    check.  In either case, `desc` is a string description of the
    object to be checked, for use in raising of exceptions.

    Raises the exception `exc` with typecode `tc` if the indicated
    object is determined not to be an |nparray|, with a NumPy float dtype.

    Intended primarily to serve as an early check for
    proper implementation of subclasses of
    :class:`~opan.grad.SuperOpanGrad` and
    :class:`~opan.hess.SuperOpanHess`. Early type-checking of key
    attributes will hopefully avoid confusing bugs downstream.

    Parameters
    ----------
    obj
        (arbitrary) --
        Object to be checked, or object with attribute to be checked.

    varname
        |str| or |None| --
        Name of the attribute of `obj` to be type-checked. |None|
        indicates to check `obj` itself.

    desc
        |str| --
        Description of the object being checked to be used in any
        raised exceptions.

    exc
        Subclass of :class:`~opan.error.OpanError` to be raised on
        a failed typecheck.

    tc
        Typecode of `exc` to be raised on a failed typecheck.

    errsrc
        |str| --
        String description of the source of the data leading to a
        failed typecheck.

        elem = next(self._iterable)
        for deque in self._deques:
            deque.append(elem)

def _advance_pattern_generators(self,p):

        valid_generators = []
        for g in p.generators:

            for trial in range(self.max_trials):
                # Generate a new position and add generator if it's ok

                if np.alltrue([self.__distance_valid(g,v,p) for v in valid_generators]):
                    valid_generators.append(g)
                    break

                g.force_new_dynamic_value('x')
                g.force_new_dynamic_value('y')

            else:
                self.warning("Unable to place pattern %s subject to given constraints" %
                             g.name)

        return valid_generators

def _advance_params(self):
        for p in ['x','y','direction']:
            self.force_new_dynamic_value(p)
        self.last_time = self.time_fn()

def register(self, settings_class=NoSwitcher, *simple_checks,
                 **conditions):
        if settings_class is NoSwitcher:

            def decorator(cls):
                self.register(cls, *simple_checks, **conditions)
                return cls
            return decorator

        available_checks = self.checks.keys()
        for condition in conditions.keys():
            if condition not in available_checks:
                raise InvalidCondition(
                    'There is no check for the condition "%s"' % condition)

        self._registry.append((settings_class, simple_checks, conditions))

def _peek_buffer(self, i=0):
        counter = itertools.count(0)

        def readline():
            try:
                return self._peek_buffer(next(counter))
            except StopIteration:
                return ''
        return readline

def _add_node(self, node, depth):

        Parameters
        ----------
        atom_syms
            Squeezes to array of N |str| --

            Element symbols for the XYZ. Must be valid elements as defined in
            the keys of :data:`const.atom_num <opan.const.atom_num>`.

        coords
            Squeezes to array of 3N |npfloat_| castables --
            Coordinates for the geometry.

        bohrs
            |bool|, optional --

            Units of coordinates (default |True|)

        Raises
        ------
        ~opan.XYZError
            (typecode :attr:`~opan.error.XYZError.OVERWRITE`)
            If :class:`ORCA_XYZ` object has already been initialized.

        ~exceptions.ValueError
            If atom_syms & coords dimensions are incompatible

        ~exceptions.ValueError
            If type of `atom_syms` and/or `coords` is invalid


        The indices of the geometries to be returned are indicated by an
        iterable of |int|\\ s passed as `g_nums`.

        As with :meth:`geom_single`, each geometry is returned as a
        length-3N |npfloat_| with each atom's x/y/z coordinates
        grouped together::

            [A1x, A1y, A1z, A2x, A2y, A2z, ...]

        In order to use NumPy `slicing or advanced indexing
        <http://docs.scipy.org/doc/numpy-1.10.0/reference/
        arrays.indexing.html>`__, :data:`geoms` must first be
        explicitly converted to |nparray|, e.g.::

            >>> x = opan.xyz.OpanXYZ(path='...')
            >>> np.array(x.geoms)[[2,6,9]]

        Parameters
        ----------
        g_nums
            length-R iterable of |int| --
            Indices of the desired geometries

        Yields
        ------
        geom
            length-3N |npfloat_| --
            Vectors of the atomic coordinates for each geometry
            indicated in `g_nums`

        Raises
        ------
        ~exceptions.IndexError
            If an item in `g_nums` is invalid (out of range)


        Parameters
        ----------
        g_num
            |int| -- Index of the desired geometry

        at_1
            |int| -- Index of the first atom

        at_2
            |int| -- Index of the second atom

        Returns
        -------
        dist
            |npfloat_| --
            Distance in Bohrs between `at_1` and `at_2` from
            geometry `g_num`

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided


        Distances are in Bohrs as with :meth:`dist_single`.

        See `above <toc-generators_>`_ for more information on
        calling options.

        Parameters
        ----------
        g_nums
            |int| or length-R iterable |int| or |None| --
            Index/indices of the desired geometry/geometries

        ats_1
            |int| or iterable |int| or |None| --
            Index/indices of the first atom(s)

        ats_2
            |int| or iterable |int| or |None| --
            Index/indices of the second atom(s)

        invalid_error
            |bool|, optional --

            If |False| (the default), |None| values are returned for
            results corresponding to invalid indices. If |True|,
            exceptions are raised per normal.

        Yields
        ------
        dist
            |npfloat_| --
            Interatomic distance in Bohrs between each atom pair of
            `ats_1` and `ats_2` from the corresponding geometries
            of `g_nums`.

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided.

        ~exceptions.ValueError
            If all iterable objects are not the same length.


        The indices `at_1` and `at_3` can be the same (yielding a
        trivial zero angle), but `at_2` must be different from
        both `at_1` and `at_3`.

        Parameters
        ----------
        g_num
            |int| --
            Index of the desired geometry

        at_1
            |int| --
            Index of the first atom

        at_2
            |int| --
            Index of the second atom

        at_3
            |int| --
            Index of the third atom

        Returns
        -------
        angle
            |npfloat_| --
            Spanning angle in degrees between `at_1`-`at_2`-`at_3`, from
            geometry `g_num`

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided

        ~exceptions.ValueError
            If `at_2` is equal to either `at_1` or `at_3`


        Angles are in degrees as with :meth:`angle_single`.

        See `above <toc-generators_>`_ for more information on
        calling options.

        Parameters
        ----------
        g_nums
            |int| or iterable |int| or |None| --
            Index of the desired geometry

        ats_1
            |int| or iterable |int| or |None| --
            Index of the first atom

        ats_2
            |int| or iterable |int| or |None| --
            Index of the second atom

        ats_3
            |int| or iterable |int| or |None| --
            Index of the third atom

        invalid_error
            |bool|, optional --

            If |False| (the default), |None| values are returned for
            results corresponding to invalid indices. If |True|,
            exceptions are raised per normal.

        Yields
        ------
        angle
            |npfloat_| --
            Spanning angles in degrees between corresponding |br|
            `ats_1`-`ats_2`-`ats_3`, from geometry/geometries `g_nums`

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided.

        ~exceptions.ValueError
            If all iterable objects are not the same length.

        ~exceptions.ValueError
            If any `ats_2` element is equal to either the corresponding `ats_1`
            or `ats_3` element.


        Angles are in degrees as with :meth:`dihed_single`.

        See `above <toc-generators_>`_ for more information on
        calling options.


        Parameters
        ----------
        g_nums
            |int| or iterable |int| or |None| --
            Indices of the desired geometry

        ats_1
            |int| or iterable |int| or |None| --
            Indices of the first atoms

        ats_2
            |int| or iterable |int| or |None| --
            Indices of the second atoms

        ats_3
            |int| or iterable |int| or |None| --
            Indices of the third atoms

        ats_4
            |int| or iterable |int| or |None| --
            Indices of the fourth atoms

        invalid_error
            |bool|, optional --

            If |False| (the default), |None| values are returned for
            results corresponding to invalid indices. If |True|,
            exceptions are raised per normal.

        Yields
        ------
        dihed
            |npfloat_| --
            Out-of-plane/dihedral angles in degrees for the indicated
            atom sets `ats_1`-`ats_2`-`ats_3`-`ats_4`, drawn from
            the respective `g_nums`.

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided.

        ~exceptions.ValueError
            If all iterable objects are not the same length.

        ~exceptions.ValueError
            If any corresponding `ats_#` indices are equal.

        ~opan.error.XYZError
            (typecode :data:`~opan.error.XYZError.DIHED`) If either
            of the atom trios (1-2-3 or
            2-3-4) is too close to linearity for any group of `ats_#`


        Returns the displacement vector pointing from `at_1`
        toward `at_2` from geometry `g_num`.
        If `at_1` == `at_2` a strict zero vector is returned.

        Displacement vector is returned in units of Bohrs.

        Parameters
        ----------
        g_num
            |int| -- Index of the desired geometry

        at_1
            |int| -- Index of the first atom

        at_2
            |int| -- Index of the second atom

        Returns
        -------
        displ
            length-3 |npfloat_| --
            Displacement vector from `at_1` to `at_2`

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided


        Displacements are in Bohrs as with :meth:`displ_single`.

        See `above <toc-generators_>`_ for more information on
        calling options.

        Parameters
        ----------
        g_nums
            |int| or length-R iterable |int| or |None| --
            Index/indices of the desired geometry/geometries

        ats_1
            |int| or length-R iterable |int| or |None| --
            Index/indices of the first atom(s)

        ats_2
            |int| or length-R iterable |int| or |None| --
            Index/indices of the second atom(s)

        invalid_error
            |bool|, optional --

            If |False| (the default), |None| values are returned for
            results corresponding to invalid indices. If |True|,
            exceptions are raised per normal.

        Yields
        ------
        displ
            |npfloat_| --
            Displacement vector in Bohrs between each atom pair of |br|
            `ats_1` :math:`\\rightarrow` `ats_2` from the corresponding
            geometries of `g_nums`.

        Raises
        ------
        ~exceptions.IndexError
            If an invalid (out-of-range) `g_num` or `at_#` is provided.

        ~exceptions.ValueError
            If all iterable objects are not the same length.


        Custom method, specifically tailored, taking in the arguments from
        an X_iter method and performing the replacement of |None| after
        error-checking the arguments for a max of one |None| value, and ensuring
        that if a |None| is present, no other non-|str| iterables are present.

        Parameters
        ----------
        args : 3-5 arguments of |int| or iterable |int|, or |None|
            First argument is always the indices for the geometries; all
            following are for the atoms in sequence as required for the
            particular :samp:`{x}_iter` method

        Returns
        -------
        arglist     : 3-5 arguments, matching input params
            Argument list, with |None| substituted if validly present

        Raises
        ------
        ~exceptions.ValueError  : If more than one |None| argument is present

        ~exceptions.ValueError  : If an arg is non-|str| iterable when one
        |None| is present

    The point is that we may be running on a remote host from the user's
    point of view, so they can't access `local_host` from a Web browser just
    by typing ``http://localhost:12345/``.
        Make sure columns are of the same length or else DataFrame construction
        will fail.

    if type(exc) in exceptions:
        typ = exceptions[type(exc)]
        return typ(exc, tb)
    else:
        try:
            typ = type(exc.__class__.__name__,
                       (RemoteException, type(exc)),
                       {'exception_type': type(exc)})
            exceptions[type(exc)] = typ
            return typ(exc, tb)
        except TypeError:
            return exc

def reads_overlapping_variants(variants, samfile, **kwargs):
    chromosome_names = set(samfile.references)
    for variant in variants:
        # I imagine the conversation went like this:
        # A: "Hey, I have an awesome idea"
        # B: "What's up?"
        # A: "Let's make two nearly identical reference genomes"
        # B: "But...that sounds like it might confuse people."
        # A: "Nah, it's cool, we'll give the chromosomes different prefixes!"
        # B: "OK, sounds like a good idea."
        if variant.contig in chromosome_names:
            chromosome = variant.contig
        elif "chr" + variant.contig in chromosome_names:
            chromosome = "chr" + variant.contig
        else:
            logger.warn(
                "Chromosome '%s' from variant %s not in alignment file %s",
                chromosome,
                variant,
                samfile.filename)
            yield variant, []
            continue
        allele_reads = reads_overlapping_variant(
            samfile=samfile,
            chromosome=chromosome,
            variant=variant,
            **kwargs)
        yield variant, allele_reads

def group_reads_by_allele(allele_reads):

    allele_to_reads_dict = defaultdict(list)
    for allele_read in allele_reads:
        allele_to_reads_dict[allele_read.allele].append(allele_read)
    return allele_to_reads_dict

def from_locus_read(cls, locus_read, n_ref):
        sequence = locus_read.sequence
        reference_positions = locus_read.reference_positions

        # positions of the nucleotides before and after the variant within
        # the read sequence
        read_pos_before = locus_read.base0_read_position_before_variant
        read_pos_after = locus_read.base0_read_position_after_variant

        # positions of the nucleotides before and after the variant on the
        # reference genome
        ref_pos_before = reference_positions[read_pos_before]

        if ref_pos_before is None:
            logger.warn(
                "Missing reference pos for nucleotide before variant on read: %s",
                locus_read)
            return None

        ref_pos_after = reference_positions[read_pos_after]

        if ref_pos_after is None:
            logger.warn(
                "Missing reference pos for nucleotide after variant on read: %s",
                locus_read)
            return None

        if n_ref == 0:
            if ref_pos_after - ref_pos_before != 1:
                # if the number of nucleotides skipped isn't the same
                # as the number of reference nucleotides in the variant then
                # don't use this read
                logger.debug(
                    "Positions before (%d) and after (%d) variant should be adjacent on read %s",
                    ref_pos_before,
                    ref_pos_after,
                    locus_read)
                return None

            # insertions require a sequence of non-aligned bases
            # followed by the subsequence reference position
            ref_positions_for_inserted = reference_positions[
                read_pos_before + 1:read_pos_after]
            if any(insert_pos is not None for insert_pos in ref_positions_for_inserted):
                # all these inserted nucleotides should *not* align to the
                # reference
                logger.debug(
                    "Skipping read, inserted nucleotides shouldn't map to reference")
                return None
        else:
            # substitutions and deletions
            if ref_pos_after - ref_pos_before != n_ref + 1:
                # if the number of nucleotides skipped isn't the same
                # as the number of reference nucleotides in the variant then
                # don't use this read
                logger.debug(
                    ("Positions before (%d) and after (%d) variant should be "
                     "adjacent on read %s"),
                    ref_pos_before,
                    ref_pos_after,
                    locus_read)
                return None

        nucleotides_at_variant_locus = sequence[read_pos_before + 1:read_pos_after]

        prefix = sequence[:read_pos_before + 1]
        suffix = sequence[read_pos_after:]

        prefix, suffix = convert_from_bytes_if_necessary(prefix, suffix)
        prefix, suffix = trim_N_nucleotides(prefix, suffix)

        return cls(
            prefix,
            nucleotides_at_variant_locus,
            suffix,
            name=locus_read.name)

def most_common_nucleotides(partitioned_read_sequences):
    counts, variant_column_indices = nucleotide_counts(
        partitioned_read_sequences)
    max_count_per_column = counts.max(axis=0)

    assert len(max_count_per_column) == counts.shape[1]
    max_nucleotide_index_per_column = np.argmax(counts, axis=0)
    assert len(max_nucleotide_index_per_column) == counts.shape[1]
    nucleotides = [
        index_to_dna_nucleotide[idx]
        for idx in max_nucleotide_index_per_column
    ]
    other_nucleotide_counts = counts.sum(axis=0) - max_count_per_column
    return "".join(nucleotides), max_count_per_column, other_nucleotide_counts

def point_displ(pt1, pt2):

    #Imports
    import numpy as np

    # Make iterable
    if not np.iterable(pt1):
        pt1 = np.float64(np.array([pt1]))
    else:
        pt1 = np.float64(np.array(pt1).squeeze())
    ## end if
    if not np.iterable(pt2):
        pt2 = np.float64(np.array([pt2]))
    else:
        pt2 = np.float64(np.array(pt2).squeeze())
    ## end if

    # Calculate the displacement vector and return
    displ = np.matrix(np.subtract(pt2, pt1)).reshape(3,1)
    return displ

def point_dist(pt1, pt2):

    # Imports
    from scipy import linalg as spla

    dist = spla.norm(point_displ(pt1, pt2))
    return dist

def point_rotate(pt, ax, theta):

    # Imports
    import numpy as np

    # Ensure pt is reducible to 3-D vector.
    pt = make_nd_vec(pt, nd=3, t=np.float64, norm=False)

    # Calculate the rotation
    rot_pt = np.dot(mtx_rot(ax, theta, reps=1), pt)

    # Should be ready to return
    return rot_pt

def point_reflect(pt, nv):

    # Imports
    import numpy as np
    from scipy import linalg as spla

    # Ensure pt is reducible to 3-D vector
    pt = make_nd_vec(pt, nd=3, t=np.float64, norm=False)

    # Transform the point and return
    refl_pt = np.dot(mtx_refl(nv, reps=1), pt)
    return refl_pt

def geom_reflect(g, nv):

    # Imports
    import numpy as np

    # Force g to n-vector
    g = make_nd_vec(g, nd=None, t=np.float64, norm=False)

    # Transform the geometry and return
    refl_g = np.dot(mtx_refl(nv, reps=(g.shape[0] // 3)), g) \
                .reshape((g.shape[0],1))
    return refl_g

def geom_rotate(g, ax, theta):

    # Imports
    import numpy as np

    # Force g to n-vector
    g = make_nd_vec(g, nd=None, t=np.float64, norm=False)

    # Perform rotation and return
    rot_g = np.dot(mtx_rot(ax, theta, reps=(g.shape[0] // 3)), g) \
                .reshape((g.shape[0],1))
    return rot_g

def symm_op(g, ax, theta, do_refl):

    # Imports
    import numpy as np

    # Depend on lower functions' geometry vector coercion. Just
    #  do the rotation and, if indicated, the reflection.
    gx = geom_rotate(g, ax, theta)
    if do_refl:
        gx = geom_reflect(gx, ax)
    ## end if

    # Should be good to go
    return gx

def geom_find_rotsymm(g, atwts, ax, improp, \
        nmax=_DEF.SYMM_MATCH_NMAX, \
        tol=_DEF.SYMM_MATCH_TOL):

    # Imports
    import numpy as np

    # Vectorize the geometry
    g = make_nd_vec(g, nd=None, t=np.float64, norm=False)

    # Ensure a 3-D axis vector
    ax = make_nd_vec(ax, nd=3, t=np.float64, norm=True)

    # Loop downward either until a good axis is found or nval < 1
    #  Should never traverse below n == 1 for regular rotation check;
    #  could for improper, though.
    nval = nmax + 1
    nfac = 1.0
    while nfac > tol and nval > 0:
        nval = nval - 1
        try:
            nfac = geom_symm_match(g, atwts, ax, \
                                    2*np.pi/nval, improp)

        except ZeroDivisionError as zde:
            # If it's because nval == zero, ignore. Else re-raise.
            if nval > 0:
                raise zde
            ## end if
        ## end try
    ## loop

    # Should be good to return
    return nval, nfac

def g_subset(g, atwts, atwt,
            digits=_DEF.SYMM_ATWT_ROUND_DIGITS):

    # Imports
    import numpy as np

    # Ensure g and atwts are n-D vectors
    g = make_nd_vec(g, nd=None, t=np.float64, norm=False)
    atwts = make_nd_vec(atwts, nd=None, t=np.float64, norm=False)

    # Ensure dims match (should already be checked at object creation...)
    if not (len(g) == 3*len(atwts)):
        raise ValueError("Dim mismatch [len(g) != 3*len(ats)].")
    ## end if

    # Pull into coordinate groups
    co = np.split(g, g.shape[0] // 3)

    # Filter by the indicated atomic weight
    cf = [c for (c,a) in zip(co, atwts) if \
                    np.round(a, digits) == np.round(atwt, digits)]

    # Expand back to single vector, if possible
    if not cf == []:
        g_sub = np.concatenate(cf, axis=0)
        g_sub = g_sub.reshape((g_sub.shape[0],1))
    else:
        g_sub = []
    ## end if

    # Return the subset
    return g_sub

def mtx_refl(nv, reps=1):

    # Imports
    import numpy as np
    from scipy import linalg as spla
    from ..const import PRM

    # Ensure |nv| is large enough for confident directionality
    if spla.norm(nv) < PRM.ZERO_VEC_TOL:
        raise ValueError("Norm of 'nv' is too small.")
    ## end if

    # Ensure nv is a normalized np.float64 3-vector
    nv = make_nd_vec(nv, nd=3, t=np.float64, norm=True)

    # Ensure reps is a positive scalar integer
    if not np.isscalar(reps):
        raise ValueError("'reps' must be scalar.")
    ## end if
    if not np.issubdtype(type(reps), int):
        raise ValueError("'reps' must be an integer.")
    ## end if
    if not reps > 0:
        raise ValueError("'reps' must be a positive integer.")
    ## end if

    # Initialize the single-point reflection transform matrix
    base_mtx = np.zeros(shape=(3,3), dtype=np.float64)

    # Construct the single-point transform matrix
    for i in range(3):
        for j in range(i,3):
            if i==j:
                base_mtx[i,j] = 1 - 2*nv[i]**2
            else:
                base_mtx[i,j] = base_mtx[j,i] = -2*nv[i]*nv[j]
            ## end if
        ## next j
    ## next i

    # Construct the block-diagonal replicated reflection matrix
    refl_mtx= spla.block_diag(*[base_mtx for i in range(reps)])

    # Return the result
    return refl_mtx

def mtx_rot(ax, theta, reps=1):

    # Imports
    import numpy as np
    from scipy import linalg as spla
    from ..const import PRM

    # Ensure |ax| is large enough for confident directionality
    if spla.norm(ax) < PRM.ZERO_VEC_TOL:
        raise ValueError("Norm of 'ax' is too small.")
    ## end if

    # Ensure ax is a normalized np.float64 3-vector
    ax = make_nd_vec(ax, nd=3, t=np.float64, norm=True)

    # Ensure reps is a positive scalar integer
    if not np.isscalar(reps):
        raise ValueError("'reps' must be scalar.")
    ## end if
    if not np.issubdtype(type(reps), int):
        raise ValueError("'reps' must be an integer.")
    ## end if
    if not reps > 0:
        raise ValueError("'reps' must be a positive integer.")
    ## end if

    # Ensure theta is scalar
    if not np.isscalar(theta):
        raise ValueError("'theta' must be scalar.")
    ## end if

    # Assemble the modified Levi-Civita matrix
    mod_lc = np.array([ [0, -ax[2], ax[1]],
                    [ax[2], 0, -ax[0]],
                    [-ax[1], ax[0], 0] ], dtype=np.float64)

    # Compute the outer product of the axis vector
    ax_oprod = np.dot(ax.reshape((3,1)), ax.reshape((1,3)))

    # Construct the base matrix
    #  Will need to refer to external math to explain this.
    base_mtx = np.add(
                        np.add( (1.0 - np.cos(theta)) * ax_oprod,
                                            np.cos(theta) * np.eye(3)
                              ),
                        np.sin(theta) * mod_lc
                     )

    # Construct the block-diagonal replicated reflection matrix
    rot_mtx= spla.block_diag(*[base_mtx for i in range(reps)])

    # Return the result
    return rot_mtx

def ff(items, targets):
    bins = [(target, []) for target in targets]
    skip = []

    for item in items:
        for target, content in bins:
            if item <= (target - sum(content)):
                content.append(item)
                break
        else:
            skip.append(item)
    return bins, skip

def ffd(items, targets, **kwargs):
    sizes = zip(items, weight(items, **kwargs))
    sizes = sorted(sizes, key=operator.itemgetter(1), reverse=True)
    items = map(operator.itemgetter(0), sizes)
    return ff(items, targets)

def mr(items, targets, **kwargs):
    bins = [(target, []) for target in targets]
    skip = []

    for item in items:
        capacities = [target - sum(content) for target, content in bins]
        weighted = weight(capacities, **kwargs)

        (target, content), capacity, _ = max(zip(bins, capacities, weighted),
                                             key=operator.itemgetter(2))
        if item <= capacity:
            content.append(item)
        else:
            skip.append(item)
    return bins, skip

def bf(items, targets, **kwargs):
    bins = [(target, []) for target in targets]
    skip = []

    for item in items:
        containers = []
        capacities = []
        for target, content in bins:
            capacity = target - sum(content)
            if item <= capacity:
                containers.append(content)
                capacities.append(capacity - item)

        if len(capacities):
            weighted = zip(containers, weight(capacities, **kwargs))
            content, _ = min(weighted, key=operator.itemgetter(1))
            content.append(item)
        else:
            skip.append(item)
    return bins, skip

def bfd(items, targets, **kwargs):
    sizes = zip(items, weight(items, **kwargs))
    sizes = sorted(sizes, key=operator.itemgetter(1), reverse=True)
    items = map(operator.itemgetter(0), sizes)
    return bf(items, targets, **kwargs)

def trim_sequences(variant_sequence, reference_context):
    cdna_prefix = variant_sequence.prefix
    cdna_alt = variant_sequence.alt
    cdna_suffix = variant_sequence.suffix

    # if the transcript is on the reverse strand then we have to
    # take the sequence PREFIX|VARIANT|SUFFIX
    # and take the complement of XIFFUS|TNAIRAV|XIFERP
    if reference_context.strand == "-":
        # notice that we are setting the *prefix* to be reverse complement
        # of the *suffix* and vice versa
        cdna_prefix, cdna_alt, cdna_suffix = (
            reverse_complement_dna(cdna_suffix),
            reverse_complement_dna(cdna_alt),
            reverse_complement_dna(cdna_prefix)
        )

    reference_sequence_before_variant = reference_context.sequence_before_variant_locus
    reference_sequence_after_variant = reference_context.sequence_after_variant_locus

    # trim the reference prefix and the RNA-derived prefix sequences to the same length
    if len(reference_sequence_before_variant) > len(cdna_prefix):
        n_trimmed_from_reference = len(reference_sequence_before_variant) - len(cdna_prefix)
        n_trimmed_from_variant = 0
    elif len(reference_sequence_before_variant) < len(cdna_prefix):
        n_trimmed_from_variant = len(cdna_prefix) - len(reference_sequence_before_variant)
        n_trimmed_from_reference = 0
    else:
        n_trimmed_from_variant = 0
        n_trimmed_from_reference = 0

    reference_sequence_before_variant = reference_sequence_before_variant[
        n_trimmed_from_reference:]
    cdna_prefix = cdna_prefix[n_trimmed_from_variant:]

    return (
        cdna_prefix,
        cdna_alt,
        cdna_suffix,
        reference_sequence_before_variant,
        reference_sequence_after_variant,
        n_trimmed_from_reference
    )

def count_mismatches_before_variant(reference_prefix, cdna_prefix):
    if len(reference_prefix) != len(cdna_prefix):
        raise ValueError(
            "Expected reference prefix '%s' to be same length as %s" % (
                reference_prefix, cdna_prefix))
    return sum(xi != yi for (xi, yi) in zip(reference_prefix, cdna_prefix))

def count_mismatches_after_variant(reference_suffix, cdna_suffix):

    len_diff = len(cdna_suffix) - len(reference_suffix)

    # if the reference is shorter than the read, the read runs into the intron - these count as
    # mismatches
    return sum(xi != yi for (xi, yi) in zip(reference_suffix, cdna_suffix)) + max(0, len_diff)

def compute_offset_to_first_complete_codon(
        offset_to_first_complete_reference_codon,
        n_trimmed_from_reference_sequence):
    if n_trimmed_from_reference_sequence <= offset_to_first_complete_reference_codon:
        return (
            offset_to_first_complete_reference_codon -
            n_trimmed_from_reference_sequence)
    else:
        n_nucleotides_trimmed_after_first_codon = (
            n_trimmed_from_reference_sequence -
            offset_to_first_complete_reference_codon)
        frame = n_nucleotides_trimmed_after_first_codon % 3
        return (3 - frame) % 3

def match_variant_sequence_to_reference_context(
        variant_sequence,
        reference_context,
        min_transcript_prefix_length,
        max_transcript_mismatches,
        include_mismatches_after_variant=False,
        max_trimming_attempts=2):
    variant_sequence_in_reading_frame = None

    # if we can't get the variant sequence to match this reference
    # context then keep trimming it by coverage until either
    for i in range(max_trimming_attempts + 1):
        # check the reverse-complemented prefix if the reference context is
        # on the negative strand since variant sequence is aligned to
        # genomic DNA (positive strand)
        variant_sequence_too_short = (
            (reference_context.strand == "+" and
                len(variant_sequence.prefix) < min_transcript_prefix_length) or
            (reference_context.strand == "-" and
                len(variant_sequence.suffix) < min_transcript_prefix_length)
        )
        if variant_sequence_too_short:
            logger.info(
                "Variant sequence %s shorter than min allowed %d (iter=%d)",
                variant_sequence,
                min_transcript_prefix_length,
                i + 1)
            return None

        variant_sequence_in_reading_frame = \
            VariantSequenceInReadingFrame.from_variant_sequence_and_reference_context(
                variant_sequence=variant_sequence,
                reference_context=reference_context)

        if variant_sequence_in_reading_frame is None:
            return None

        n_mismatch_before_variant = (
            variant_sequence_in_reading_frame.number_mismatches_before_variant)
        n_mismatch_after_variant = (
            variant_sequence_in_reading_frame.number_mismatches_after_variant)

        logger.info("Iter #%d/%d: %s" % (
            i + 1,
            max_trimming_attempts + 1,
            variant_sequence_in_reading_frame))

        total_mismatches = n_mismatch_before_variant
        if include_mismatches_after_variant:
            total_mismatches += n_mismatch_after_variant
        if total_mismatches <= max_transcript_mismatches:
            # if we got a variant sequence + reading frame with sufficiently
            # few mismatches then call it a day
            return variant_sequence_in_reading_frame

        logger.info(
            ("Too many mismatches (%d) between variant sequence %s and "
             "reference context %s (attempt=%d/%d)"),
            n_mismatch_before_variant,
            variant_sequence,
            reference_context,
            i + 1,
            max_trimming_attempts + 1)
        # if portions of the sequence are supported by only 1 read
        # then try trimming to 2 to see if the better supported
        # subsequence can be better matched against the reference
        current_min_coverage = variant_sequence.min_coverage()
        logger.info(
            "Trimming to subsequence covered by at least %d reads",
            current_min_coverage + 1)
        variant_sequence = variant_sequence.trim_by_coverage(
            current_min_coverage + 1)
    return None

def _check_codons(self):
        for stop_codon in self.stop_codons:
            if stop_codon in self.codon_table:
                if self.codon_table[stop_codon] != "*":
                    raise ValueError(
                        ("Codon '%s' not found in stop_codons, but codon table "
                         "indicates that it should be") % (stop_codon,))
            else:
                self.codon_table[stop_codon] = "*"

        for start_codon in self.start_codons:
            if start_codon not in self.codon_table:
                raise ValueError(
                    "Start codon '%s' missing from codon table" % (
                        start_codon,))

        for codon, amino_acid in self.codon_table.items():
            if amino_acid == "*" and codon not in self.stop_codons:
                raise ValueError(
                    "Non-stop codon '%s' can't translate to '*'" % (
                        codon,))

        if len(self.codon_table) != 64:
            raise ValueError(
                "Expected 64 codons but found %d in codon table" % (
                    len(self.codon_table,)))

def copy(
            self,
            name,
            start_codons=None,
            stop_codons=None,
            codon_table=None,
            codon_table_changes=None):
        new_start_codons = (
            self.start_codons.copy()
            if start_codons is None
            else start_codons)

        new_stop_codons = (
            self.stop_codons.copy()
            if stop_codons is None
            else stop_codons)

        new_codon_table = (
            self.codon_table.copy()
            if codon_table is None
            else codon_table)

        if codon_table_changes is not None:
            new_codon_table.update(codon_table_changes)

        return GeneticCode(
            name=name,
            start_codons=new_start_codons,
            stop_codons=new_stop_codons,
            codon_table=new_codon_table)

def start(self):
        self.running = True
        self.thread = threading.Thread(target=self._main_loop)
        self.thread.start()

def subscribe(self, field_names):
        available_controls = dict(self.raildriver.get_controller_list()).values()
        for field in field_names:
            if field not in available_controls:
                raise ValueError('Cannot subscribe to a missing controller {}'.format(field))
        self.subscribed_fields = field_names

def set_matrix_dimensions(self, bounds, xdensity, ydensity):
        self.bounds = bounds
        self.xdensity = xdensity
        self.ydensity = ydensity
        scs = SheetCoordinateSystem(bounds, xdensity, ydensity)
        for of in self.output_fns:
            if isinstance(of, TransferFn):
                of.initialize(SCS=scs, shape=scs.shape)

def state_push(self):
        "Save the state of the output functions, to be restored with state_pop."
        for of in self.output_fns:
            if hasattr(of,'state_push'):
                of.state_push()
        super(PatternGenerator, self).state_push()

def state_pop(self):
        "Restore the state of the output functions saved by state_push."
        for of in self.output_fns:
            if hasattr(of,'state_pop'):
                of.state_pop()
        super(PatternGenerator, self).state_pop()

def pil(self, **params_to_override):
        Push the state of all generators
        Pop the state of all generators
        generators = self._advance_pattern_generators(p)

        assert hasattr(p.operator,'reduce'),repr(p.operator)+" does not support 'reduce'."

        # CEBALERT: mask gets applied by all PGs including the Composite itself
        # (leads to redundant calculations in current lissom_oo_or usage, but
        # will lead to problems/limitations in the future).
        patterns = [pg(xdensity=p.xdensity,ydensity=p.ydensity,
                       bounds=p.bounds,mask=p.mask,
                       x=p.x+p.size*(pg.x*np.cos(p.orientation)- pg.y*np.sin(p.orientation)),
                       y=p.y+p.size*(pg.x*np.sin(p.orientation)+ pg.y*np.cos(p.orientation)),
                       orientation=pg.orientation+p.orientation,
                       size=pg.size*p.size)
                    for pg in generators]
        image_array = p.operator.reduce(patterns)
        return image_array

def compile_column(name: str, data_type: str, nullable: bool) -> str:

        if self.query:
            ddl_statement = self.compile_create_as()
        else:
            ddl_statement = self.compile_create()

        if no_data:
            ddl_statement += '\nWITH NO DATA'

        return ddl_statement, self.query_values

def predicted_effects_for_variant(
        variant,
        transcript_id_whitelist=None,
        only_coding_changes=True):

    effects = []
    for transcript in variant.transcripts:
        if only_coding_changes and not transcript.complete:
            logger.info(
                "Skipping transcript %s for variant %s because it's incomplete",
                transcript.name,
                variant)
            continue

        if transcript_id_whitelist and transcript.id not in transcript_id_whitelist:
            logger.info(
                "Skipping transcript %s for variant %s because it's not one of %d allowed",
                transcript.name,
                variant,
                len(transcript_id_whitelist))
            continue
        effects.append(variant.effect_on_transcript(transcript))

    effects = EffectCollection(effects)

    n_total_effects = len(effects)
    logger.info("Predicted total %d effects for variant %s" % (
        n_total_effects,
        variant))
    if not only_coding_changes:
        return effects
    else:
        nonsynonymous_coding_effects = effects.drop_silent_and_noncoding()
        logger.info(
            "Keeping %d/%d effects which affect protein coding sequence for %s: %s",
            len(nonsynonymous_coding_effects),
            n_total_effects,
            variant,
            nonsynonymous_coding_effects)

        usable_effects = [
            effect
            for effect in nonsynonymous_coding_effects
            if effect.mutant_protein_sequence is not None
        ]
        logger.info(
            "Keeping %d effects with predictable AA sequences for %s: %s",
            len(usable_effects),
            variant,
            usable_effects)
        return usable_effects

def reference_transcripts_for_variant(
        variant,
        transcript_id_whitelist=None,
        only_coding_changes=True):
    predicted_effects = predicted_effects_for_variant(
        variant=variant,
        transcript_id_whitelist=transcript_id_whitelist,
        only_coding_changes=only_coding_changes)
    return [effect.transcript for effect in predicted_effects]

def pileup_reads_at_position(samfile, chromosome, base0_position):

    # TODO: I want to pass truncate=True, stepper="all"
    # but for some reason I get this error:
    #      pileup() got an unexpected keyword argument 'truncate'
    # ...even though these options are listed in the docs for pysam 0.9.0
    #
    for column in samfile.pileup(
            chromosome,
            start=base0_position,
            end=base0_position + 1):

        if column.pos != base0_position:
            # if this column isn't centered on the base before the
            # variant then keep going
            continue

        return column.pileups

    # if we get to this point then we never saw a pileup at the
    # desired position
    return []

def locus_read_generator(
        samfile,
        chromosome,
        base1_position_before_variant,
        base1_position_after_variant,
        use_duplicate_reads=USE_DUPLICATE_READS,
        use_secondary_alignments=USE_SECONDARY_ALIGNMENTS,
        min_mapping_quality=MIN_READ_MAPPING_QUALITY):
    logger.debug(
        "Gathering reads at locus %s: %d-%d",
        chromosome,
        base1_position_before_variant,
        base1_position_after_variant)
    base0_position_before_variant = base1_position_before_variant - 1
    base0_position_after_variant = base1_position_after_variant - 1

    count = 0

    # We get a pileup at the base before the variant and then check to make sure
    # that reads also overlap the reference position after the variant.
    #
    # TODO: scan over a wider interval of pileups and collect reads that don't
    # overlap the bases before/after a variant due to splicing
    for pileup_element in pileup_reads_at_position(
            samfile=samfile,
            chromosome=chromosome,
            base0_position=base0_position_before_variant):
        read = LocusRead.from_pysam_pileup_element(
            pileup_element,
            base0_position_before_variant=base0_position_before_variant,
            base0_position_after_variant=base0_position_after_variant,
            use_secondary_alignments=use_secondary_alignments,
            use_duplicate_reads=use_duplicate_reads,
            min_mapping_quality=min_mapping_quality)

        if read is not None:
            count += 1
            yield read

    logger.info(
        "Found %d reads overlapping locus %s: %d-%d",
        count,
        chromosome,
        base1_position_before_variant,
        base1_position_after_variant)

def locus_reads_dataframe(*args, **kwargs):
    df_builder = DataFrameBuilder(
        LocusRead,
        variant_columns=False,
        converters={
            "reference_positions": list_to_string,
            "quality_scores": list_to_string,
        })
    for locus_read in locus_read_generator(*args, **kwargs):
        df_builder.add(variant=None, element=locus_read)
    return df_builder.to_dataframe()

def copy_from_csv_sql(qualified_name: str, delimiter=',', encoding='utf8',
                      null_str='', header=True, escape_str='\\', quote_char='"',
                      force_not_null=None, force_null=None):
    Sort protein sequences in decreasing order of priority
    Translates each coding variant in a collection to one or more
    Translation objects, which are then aggregated into equivalent
    ProteinSequence objects.

    Parameters
    ----------
    variant_and_overlapping_reads_generator : generator
        Yields sequence of varcode.Variant objects paired with sequences
        of AlleleRead objects that support that variant.

    transcript_id_whitelist : set, optional
        If given, expected to be a set of transcript IDs which we should use
        for determining the reading frame around a variant. If omitted, then
        try to use all overlapping reference transcripts.

    protein_sequence_length : int
        Try to translate protein sequences of this length, though sometimes
        we'll have to return something shorter (depending on the RNAseq data,
        and presence of stop codons).

    min_alt_rna_reads : int
        Drop variant sequences at loci with fewer than this number of reads
        supporting the alt allele.

    min_variant_sequence_coverage : int
        Trim variant sequences to positions supported by at least this number
        of RNA reads.

    min_transcript_prefix_length : int
        Minimum number of bases we need to try matching between the reference
        context and variant sequence.

    max_transcript_mismatches : int
        Don't try to determine the reading frame for a transcript if more
        than this number of bases differ.

    include_mismatches_after_variant : bool
        Include mismatches after the variant locus in the count compared
        against max_transcript_mismatches.

    max_protein_sequences_per_variant : int
        Number of protein sequences to return for each ProteinSequence

    variant_cdna_sequence_assembly : bool
        If True, then assemble variant cDNA sequences based on overlap of
        RNA reads. If False, then variant cDNA sequences must be fully spanned
        and contained within RNA reads.

    Yields pairs of a Variant and a list of ProteinSequence objects
        Create a ProteinSequence object from a TranslationKey, along with
        all the extra fields a ProteinSequence requires.

    name = delete_prefix + table.name
    primary_key = table.primary_key
    key_names = set(primary_key.column_names)
    columns = [column for column in table.columns if column.name in key_names]
    table = Table(name, columns, primary_key)

    return table

def trim_variant_fields(location, ref, alt):
    if len(alt) > 0 and ref.startswith(alt):
        # if alt is a prefix of the ref sequence then we actually have a
        # deletion like:
        #   g.10 GTT > GT
        # which can be trimmed to
        #   g.12 'T'>''
        ref = ref[len(alt):]
        location += len(alt)
        alt = ""
    if len(ref) > 0 and alt.startswith(ref):
        # if ref sequence is a prefix of the alt sequence then we actually have
        # an insertion like:
        #   g.10 GT>GTT
        # which can be trimmed to
        #   g.11 ''>'T'
        # Note that we are selecting the position *before* the insertion
        # (as an arbitrary convention)
        alt = alt[len(ref):]
        location += len(ref) - 1
        ref = ""
    return location, ref, alt

def base0_interval_for_variant(variant):
    base1_location, ref, alt = trim_variant(variant)
    return base0_interval_for_variant_fields(
        base1_location=base1_location,
        ref=ref,
        alt=alt)

def interbase_range_affected_by_variant_on_transcript(variant, transcript):
    if variant.is_insertion:
        if transcript.strand == "+":
            # base-1 position of an insertion is the genomic nucleotide
            # before any inserted mutant nucleotides, so the start offset
            # of the actual inserted nucleotides is one past that reference
            # position
            start_offset = transcript.spliced_offset(variant.start) + 1
        else:
            # on the negative strand the genomic base-1 position actually
            # refers to the transcript base *after* the insertion, so we can
            # use that as the interbase coordinate for where the insertion
            # occurs
            start_offset = transcript.spliced_offset(variant.start)
        # an insertion happens *between* two reference bases
        # so the start:end offsets coincide
        end_offset = start_offset
    else:

        # reference bases affected by substitution or deletion defined by
        # range starting at first affected base
        offsets = []
        assert len(variant.ref) > 0
        for dna_pos in range(variant.start, variant.start + len(variant.ref)):
            try:
                offsets.append(transcript.spliced_offset(dna_pos))
            except ValueError:
                logger.info(
                    "Couldn't find position %d from %s on exons of %s",
                    dna_pos,
                    variant,
                    transcript)
        if len(offsets) == 0:
            raise ValueError(
                "Couldn't find any exonic reference bases affected by %s on %s",
                variant,
                transcript)
        start_offset = min(offsets)
        end_offset = max(offsets) + 1
    return (start_offset, end_offset)

def insert(conn, qualified_name: str, column_names, records):

    Notes
    -----
    records should be Iterable collection of namedtuples or tuples.

    with conn:
        with conn.cursor() as cursor:
            for record in records:
                cursor.execute(upsert_statement, record)

def delete_joined_table_sql(qualified_name, removing_qualified_name, primary_key):

    condition_template = 't.{}=d.{}'
    where_clause = ' AND '.join(condition_template.format(pkey, pkey)
                                for pkey in primary_key)
    delete_statement = (
        'DELETE FROM {table} t'
        ' USING {delete_table} d'
        ' WHERE {where_clause}').format(table=qualified_name,
                                        delete_table=removing_qualified_name,
                                        where_clause=where_clause)
    return delete_statement

def copy_from_csv(conn, file, qualified_name: str, delimiter=',', encoding='utf8',
                  null_str='', header=True, escape_str='\\', quote_char='"',
                  force_not_null=None, force_null=None):

    copy_sql = copy_from_csv_sql(qualified_name, delimiter, encoding,
                                 null_str=null_str, header=header,
                                 escape_str=escape_str, quote_char=quote_char,
                                 force_not_null=force_not_null,
                                 force_null=force_null)

    with conn:
        with conn.cursor() as cursor:
            cursor.copy_expert(copy_sql, file)

def get_user_tables(conn):

    qualified_name = compile_qualified_name(table, schema=schema)

    for record in select_dict(conn, query, params=(qualified_name,)):
        yield record

def reflect_table(conn, table_name, schema='public'):

    conn = psycopg2.connect(database='postgres')
    db = Database(db_name)
    conn.autocommit = True

    with conn.cursor() as cursor:
        cursor.execute(db.drop_statement())
        cursor.execute(db.create_statement())
    conn.close()

def install_extensions(extensions, **connection_parameters):

    from postpy.connections import connect

    conn = connect(**connection_parameters)
    conn.autocommit = True

    for extension in extensions:
        install_extension(conn, extension)

def update(self, status):
        logging.info('Executor sends status update {} for task {}'.format(
                     status.state, status.task_id))
        return self.driver.sendStatusUpdate(encode(status))

def message(self, data):
        logging.info('Driver sends framework message {}'.format(data))
        return self.driver.sendFrameworkMessage(data)

def get_current_time(self):
        hms = [int(self.get_current_controller_value(i)) for i in range(406, 409)]
        return datetime.time(*hms)

def get_loco_name(self):
        ret_str = self.dll.GetLocoName().decode()
        if not ret_str:
            return
        return ret_str.split('.:.')

def set_controller_value(self, index_or_name, value):
        if not isinstance(index_or_name, int):
            index = self.get_controller_index(index_or_name)
        else:
            index = index_or_name
        self.dll.SetControllerValue(index, ctypes.c_float(value))

def stop(self, failover=False):
        logging.info('Stops Scheduler Driver')
        return self.driver.stop(failover)

def request(self, requests):
        logging.info('Request resources from Mesos')
        return self.driver.requestResources(map(encode, requests))

def launch(self, offer_id, tasks, filters=Filters()):
        logging.info('Launches tasks {}'.format(tasks))
        return self.driver.launchTasks(encode(offer_id),
                                       map(encode, tasks),
                                       encode(filters))

def kill(self, task_id):
        logging.info('Kills task {}'.format(task_id))
        return self.driver.killTask(encode(task_id))

def reconcile(self, statuses):
        logging.info('Reconciles task statuses {}'.format(statuses))
        return self.driver.reconcileTasks(map(encode, statuses))

def accept(self, offer_ids, operations, filters=Filters()):
        logging.info('Accepts offers {}'.format(offer_ids))
        return self.driver.acceptOffers(map(encode, offer_ids),
                                        map(encode, operations),
                                        encode(filters))

def acknowledge(self, status):
        logging.info('Acknowledges status update {}'.format(status))
        return self.driver.acknowledgeStatusUpdate(encode(status))

def message(self, executor_id, slave_id, message):
        logging.info('Sends message `{}` to executor `{}` on slave `{}`'.format(
                     message, executor_id, slave_id))
        return self.driver.sendFrameworkMessage(encode(executor_id),
                                                encode(slave_id),
                                                message)

def _connect_func(builder, obj, signal_name, handler_name,
                  connect_object, flags, cls):
    '''Registers the template for the widget and hooks init_template'''

    # This implementation won't work if there are nested templates, but
    # we can't do that anyways due to PyGObject limitations so it's ok

    if not hasattr(cls, 'set_template'):
        raise TypeError("Requires PyGObject 3.13.2 or greater")

    cls.set_template(template_bytes)

    bound_methods = set()
    bound_widgets = set()

    # Walk the class, find marked callbacks and child attributes
    for name in dir(cls):
        o = getattr(cls, name, None)

        if inspect.ismethod(o):
            if hasattr(o, '_gtk_callback'):
                bound_methods.add(name)
                # Don't need to call this, as connect_func always gets called
                #cls.bind_template_callback_full(name, o)
        elif isinstance(o, _Child):
            cls.bind_template_child_full(name, True, 0)
            bound_widgets.add(name)

    # Have to setup a special connect function to connect at template init
    # because the methods are not bound yet
    cls.set_connect_func(_connect_func, cls)

    cls.__gtemplate_methods__ = bound_methods
    cls.__gtemplate_widgets__ = bound_widgets

    base_init_template = cls.init_template
    cls.init_template = lambda s: _init_template(s, cls, base_init_template)

def _init_template(self, cls, base_init_template):
    '''Try to convert content of README.md into rst format using pypandoc,
    write it into README and return it.

    If pypandoc cannot be imported write content of README.md unchanged into
    README and return it.
        The utililty requires boto3 clients to CloudFormation.

        Args:
            None

        Returns:
            Good or Bad; True or False
        Determine the drift of the stack.

        Args:
            None

        Returns:
            Good or Bad; True or False
        Report the drift of the stack.

        Args:
            None

        Returns:
            Good or Bad; True or False

        Note: not yet implemented

    :param archive: CombineArchive instance
    :param location:
    :return:

    :param fileName: path of archive
    :return: None
	Like cmd.exe's mklink except it will infer directory status of the
	target.
	Determine if the given path is a reparse point.
	Return False if the file does not exist or the file attributes cannot
	be determined.
	Assuming path is a reparse point, determine if it's a symlink.
	For a given path, determine the ultimate location of that path.
	Useful for resolving symlink targets.
	This functions wraps the GetFinalPathNameByHandle from the Windows
	SDK.

	Note, this function fails if a handle cannot be obtained (such as
	for C:\Pagefile.sys on a stock windows system). Consider using
	trace_symlink_target instead.
	Wrapper around os.path.join that works with Windows drive letters.

	>>> join('d:\\foo', '\\bar')
	'd:\\bar'
	Find a path from start to target where target is relative to start.

	>>> tmp = str(getfixture('tmpdir_as_cwd'))

	>>> findpath('d:\\')
	'd:\\'

	>>> findpath('d:\\', tmp)
	'd:\\'

	>>> findpath('\\bar', 'd:\\')
	'd:\\bar'

	>>> findpath('\\bar', 'd:\\foo') # fails with '\\bar'
	'd:\\bar'

	>>> findpath('bar', 'd:\\foo')
	'd:\\foo\\bar'

	>>> findpath('\\baz', 'd:\\foo\\bar') # fails with '\\baz'
	'd:\\baz'

	>>> os.path.abspath(findpath('\\bar')).lower()
	'c:\\bar'

	>>> os.path.abspath(findpath('bar'))
	'...\\bar'

	>>> findpath('..', 'd:\\foo\\bar')
	'd:\\foo'

	The parent of the root directory is the root directory.
	>>> findpath('..', 'd:\\')
	'd:\\'
	Given a file that is known to be a symlink, trace it to its ultimate
	target.

	Raises TargetNotPresent when the target cannot be determined.
	Raises ValueError when the specified link is not a symlink.
	jaraco.windows provides the os.symlink and os.readlink functions.
	Monkey-patch the os module to include them if not present.

    Superposition of analytical solutions without a gridded domain


    Builds the diagonals for the coefficient array


    :param fileName: file to include in the archive
    :return: None
    Calculate the standard deviation of a list of values
    @param values: list(float)
    @param average:
    @return:
        Append a value to the stats list

        Parameters
        ----------
        value : float
            The value to add
    Chain results from a list of functions. Inverted reduce.

    :param (function) steps: List of function callbacks
    :param initial: Starting value for pipeline.
		Add a value to a delimited variable, but only when the value isn't
		already present.
		Takes a SYSTEMTIME object, such as retrieved from a TIME_ZONE_INFORMATION
		structure or call to GetTimeZoneInformation and interprets
		it based on the given
		year to identify the actual day.

		This method is necessary because the SYSTEMTIME structure
		refers to a day by its
		day of the week and week of the month (e.g. 4th saturday in March).

		>>> SATURDAY = 6
		>>> MARCH = 3
		>>> st = SYSTEMTIME(2000, MARCH, SATURDAY, 4, 0, 0, 0, 0)

		# according to my calendar, the 4th Saturday in March in 2009 was the 28th
		>>> expected_date = datetime.datetime(2009, 3, 28)
		>>> Info._locate_day(2009, st) == expected_date
		True
    Return a url matcher suited for urlpatterns.

    pattern: the regex against which to match the requested URL.
    to: either a url name that `reverse` will find, a url that will simply be returned,
        or a function that will be given the request and url captures, and return the
        destination.
    permanent: boolean whether to send a 301 or 302 response.
    locale_prefix: automatically prepend `pattern` with a regex for an optional locale
        in the url. This locale (or None) will show up in captured kwargs as 'locale'.
    anchor: if set it will be appended to the destination url after a '#'.
    name: if used in a `urls.py` the redirect URL will be available as the name
        for use in calls to `reverse()`. Does _NOT_ work if used in a `redirects.py` file.
    query: a dict of query params to add to the destination url.
    vary: if you used an HTTP header to decide where to send users you should include that
        header's name in the `vary` arg.
    cache_timeout: number of hours to cache this redirect. just sets the proper `cache-control`
        and `expires` headers.
    decorators: a callable (or list of callables) that will wrap the view used to redirect
        the user. equivalent to adding a decorator to any other view.
    re_flags: a string of any of the characters: "iLmsux". Will modify the `pattern` regex
        based on the documented meaning of the flags (see python re module docs).
    to_args: a tuple or list of args to pass to reverse if `to` is a url name.
    to_kwargs: a dict of keyword args to pass to reverse if `to` is a url name.
    prepend_locale: if true the redirect URL will be prepended with the locale from the
        requested URL.
    merge_query: merge the requested query params from the `query` arg with any query params
        from the request.

    Usage:
    urlpatterns = [
        redirect(r'projects/$', 'mozorg.product'),
        redirect(r'^projects/seamonkey$', 'mozorg.product', locale_prefix=False),
        redirect(r'apps/$', 'https://marketplace.firefox.com'),
        redirect(r'firefox/$', 'firefox.new', name='firefox'),
        redirect(r'the/dude$', 'abides', query={'aggression': 'not_stand'}),
    ]
		Retrieve the size of the buffer needed by calling the method
		with a null pointer and length of zero. This should trigger an
		insufficient buffer error and return the size needed for the
		buffer.
		Get the table
		Using the table structure, return the array of entries based
		on the table size.
    Add 'orphan' to metadata for partials

    :type app: sphinx.application.Sphinx
    :type doctree: docutils.nodes.document
    Skip un parseable functions.

    :type app: sphinx.application.Sphinx
    :param str what: the type of the object which the docstring belongs to
        (one of "module", "class", "exception", "function", "method", "attribute")
    :param str name: the fully qualified name of the object
    :param type obj: the object itself
    :param bool skip: a boolean indicating if autodoc will skip this member
    :param sphinx.ext.autodoc.Options options: the options given to the directive
    :rtype: bool
    Plot if you want to - for troubleshooting - 1 figure
    Plot multiple subplot figure for 2D array

    Outputs a grid of deflections if an output directory is defined in the 
    configuration file
    
    If the filename given in the configuration file ends in ".npy", then a binary 
    numpy grid will be exported.
    
    Otherwise, an ASCII grid will be exported.
    Checks that Te and q0 array sizes are compatible
    For finite difference solution.
    Set-up for the finite difference solution method
    Set-up for the rectangularly-gridded superposition of analytical solutions 
    method for solving flexure
    Set-up for the ungridded superposition of analytical solutions 
    method for solving flexure

    If no *abcs* are given, the algorithm works exactly like the built-in C3
    linearization used for method resolution.

    If given, *abcs* is a list of abstract base classes that should be inserted
    into the resulting MRO. Unrelated ABCs are ignored and don't end up in the
    result. The algorithm inserts ABCs where their functionality is introduced,
    i.e. issubclass(cls, abc) returns True for the class itself but returns
    False for all its direct base classes. Implicit ABCs for a given class
    (either registered or inferred from the presence of a special method like
    __len__) are inserted directly after the last ABC explicitly listed in the
    MRO of said class. If two implicit ABCs end up next to each other in the
    resulting MRO, their ordering depends on the order of types in *abcs*.


    Transforms a function into a generic function, which can have different
    behaviours depending upon the type of its first argument. The decorated

    function acts as the default implementation, and additional
    implementations can be registered using the register() attribute of the
    generic function.


        Runs the dispatch algorithm to return the best available implementation
        for the given *cls* registered on *generic_func*.


        Registers a new implementation for the given *cls* on a *generic_func*.


        parser = ArgumentParser(description="Runs pylint recursively on a directory")

        parser.add_argument(
            "-v",
            "--verbose",
            dest="verbose",
            action="store_true",

            default=False,
            help="Verbose mode (report which files were found for testing).",
        )

        parser.add_argument(
            "--rcfile",
            dest="rcfile",
            action="store",

            default=".pylintrc",
            help="A relative or absolute path to your pylint rcfile. Defaults to\
                            `.pylintrc` at the current working directory",
        )

        parser.add_argument(
            "-V",
            "--version",
            action="version",
            version="%(prog)s ({0}) for Python {1}".format(__version__, PYTHON_VERSION),
        )

        options, _ = parser.parse_known_args(args)

        self.verbose = options.verbose

        if options.rcfile:
            if not os.path.isfile(options.rcfile):
                options.rcfile = os.getcwd() + "/" + options.rcfile
            self.rcfile = options.rcfile

        return options

def _parse_ignores(self):

        pylint_output = output if output is not None else sys.stdout
        pylint_error = error if error is not None else sys.stderr
        savedout, savederr = sys.__stdout__, sys.__stderr__
        sys.stdout = pylint_output
        sys.stderr = pylint_error

        pylint_files = self.get_files_from_dir(os.curdir)
        self._print_line(
            "Using pylint "
            + colorama.Fore.RED
            + pylint.__version__
            + colorama.Fore.RESET
            + " for python "
            + colorama.Fore.RED
            + PYTHON_VERSION
            + colorama.Fore.RESET
        )

        self._print_line("pylint running on the following files:")
        for pylint_file in pylint_files:
            # we need to recast this as a string, else pylint enters an endless recursion
            split_file = str(pylint_file).split("/")
            split_file[-1] = colorama.Fore.CYAN + split_file[-1] + colorama.Fore.RESET
            pylint_file = "/".join(split_file)
            self._print_line("- " + pylint_file)
        self._print_line("----")


        if not self._is_using_default_rcfile():
            self.args += ["--rcfile={}".format(self.rcfile)]

        exit_kwarg = {"do_exit": False}

        run = pylint.lint.Run(self.args + pylint_files, **exit_kwarg)
        sys.stdout = savedout
        sys.stderr = savederr

        sys.exit(run.linter.msg_status)

def strict(*types):
    pos = t.lexer.lexpos
    data = t.lexer.lexdata
    last_cr = data.rfind('\n', 0, pos)
    if last_cr < 0:
        last_cr = -1
    column = pos - last_cr
    return column

def no_sleep():
	mode = power.ES.continuous | power.ES.system_required
	handle_nonzero_success(power.SetThreadExecutionState(mode))
	try:
		yield
	finally:
		handle_nonzero_success(power.SetThreadExecutionState(power.ES.continuous))

def selfoss(reset_password=False):
    '''
    hostname = re.sub(r'^[^@]+@', '', env.host)  # without username if any
    sitename = query_input(
                   question='\nEnter site-name of Your trac web service',

                   default=flo('selfoss.{hostname}'))
    username = env.user

    site_dir = flo('/home/{username}/sites/{sitename}')

    checkout_latest_release_of_selfoss()
    create_directory_structure(site_dir)

    restored = install_selfoss(sitename, site_dir, username)

    nginx_site_config(username, sitename, hostname)
    enable_php5_socket_file()

    if not restored or reset_password:
        setup_selfoss_user(username, sitename, site_dir)

    print_msg('\n## reload nginx and restart php\n')
    run('sudo service nginx reload')
    run('sudo service php5-fpm restart')

def get_cache_path(filename):
	Get the current process token
	Get the LUID for the SeCreateSymbolicLinkPrivilege
	Get all privileges associated with the current process.
	Try to assign the symlink privilege to the current process token.
	Return True if the assignment is successful.
	Grant the 'create symlink' privilege to who.

	Based on http://support.microsoft.com/kb/132958
	Find the DLL for a given library.

	Accepts a string or loaded module

	>>> print(find_lib('kernel32').lower())
	c:\windows\system32\kernel32.dll

        :param pid: The HydroShare ID of the resource
        :raises: HydroShareNotAuthorized if the user is not authorized to view the metadata.
        :raises: HydroShareNotFound if the resource was not found.
        :raises: HydroShareHTTPException to signal an HTTP error.
        :return: A string representing the XML+RDF serialization of science metadata.
        Example of data XML+RDF returned:

        <?xml version="1.0"?>
        <!DOCTYPE rdf:RDF PUBLIC "-//DUBLIN CORE//DCMES DTD 2002/07/31//EN"
        "http://dublincore.org/documents/2002/07/31/dcmes-xml/dcmes-xml-dtd.dtd">
        <rdf:RDF xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:dcterms="http://purl.org/dc/terms/" xmlns:hsterms="http://hydroshare.org/terms/" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:rdfs1="http://www.w3.org/2001/01/rdf-schema#">
          <rdf:Description rdf:about="http://www.hydroshare.org/resource/87ffb608900e407ab4b67d30c93b329e">
            <dc:title>Great Salt Lake Level and Volume</dc:title>
            <dc:type rdf:resource="http://www.hydroshare.org/terms/GenericResource"/>
            <dc:description>
              <rdf:Description>
                <dcterms:abstract>Time series of level, area and volume in the Great Salt Lake. Volume and area of the Great Salt Lake are derived from recorded levels</dcterms:abstract>
              </rdf:Description>
            </dc:description>
            <hsterms:awardInfo>
              <rdf:Description rdf:about="http://www.nsf.gov">
                <hsterms:fundingAgencyName>National Science Foundation</hsterms:fundingAgencyName>
                <hsterms:awardTitle>Model Execution Cyberinfrastructure </hsterms:awardTitle>
                <hsterms:awardNumber>NSF_9087658_2017</hsterms:awardNumber>
              </rdf:Description>
            </hsterms:awardInfo>
            <dc:creator>
              <rdf:Description>
                <hsterms:name>John Smith</hsterms:name>
                <hsterms:creatorOrder>1</hsterms:creatorOrder>
                <hsterms:organization>Utah State University</hsterms:organization>
                <hsterms:email>john.smith@gmail.com</hsterms:email>
                <hsterms:address>Engineering Building, USU, Logan, Utah</hsterms:address>
                <hsterms:phone rdf:resource="tel:435-797-8967"/>
              </rdf:Description>
            </dc:creator>
            <dc:creator>
              <rdf:Description>
                <hsterms:name>Lisa Miller</hsterms:name>
                <hsterms:creatorOrder>2</hsterms:creatorOrder>
              </rdf:Description>
            </dc:creator>
            <dc:contributor>
              <rdf:Description>
                <hsterms:name>Jenny Parker</hsterms:name>
                <hsterms:organization>Univesity of Utah</hsterms:organization>
                <hsterms:email>jenny_parker@hotmail.com</hsterms:email>
              </rdf:Description>
            </dc:contributor>
            <dc:coverage>
              <dcterms:period>
                <rdf:value>start=2000-01-01T00:00:00; end=2010-12-12T00:00:00; scheme=W3C-DTF</rdf:value>
              </dcterms:period>
            </dc:coverage>
            <dc:date>
              <dcterms:created>
                <rdf:value>2017-01-03T17:06:18.932217+00:00</rdf:value>
              </dcterms:created>
            </dc:date>
            <dc:date>
              <dcterms:modified>
                <rdf:value>2017-01-03T17:35:34.067279+00:00</rdf:value>
              </dcterms:modified>
            </dc:date>
            <dc:format>image/tiff</dc:format>
            <dc:identifier>
              <rdf:Description>
                <hsterms:hydroShareIdentifier>http://www.hydroshare.org/resource/87ffb608900e407ab4b67d30c93b329e</hsterms:hydroShareIdentifier>
              </rdf:Description>
            </dc:identifier>
            <dc:language>eng</dc:language>
            <dc:rights>
              <rdf:Description>
                <hsterms:rightsStatement>This resource is shared under the Creative Commons Attribution CC BY.</hsterms:rightsStatement>
                <hsterms:URL rdf:resource="http://creativecommons.org/licenses/by/4.0/"/>
              </rdf:Description>
            </dc:rights>
            <dc:subject>NSF</dc:subject>
            <dc:subject>Model</dc:subject>
            <dc:subject>Cyberinfrastructure</dc:subject>
            <hsterms:extendedMetadata>
              <rdf:Description>
                <hsterms:key>model</hsterms:key>
                <hsterms:value>ueb</hsterms:value>
              </rdf:Description>
            </hsterms:extendedMetadata>
            <hsterms:extendedMetadata>
              <rdf:Description>
                <hsterms:key>os</hsterms:key>
                <hsterms:value>windows</hsterms:value>
              </rdf:Description>
            </hsterms:extendedMetadata>
          </rdf:Description>
          <rdf:Description rdf:about="http://www.hydroshare.org/terms/GenericResource">
            <rdfs1:label>Generic</rdfs1:label>
            <rdfs1:isDefinedBy>http://www.hydroshare.org/terms</rdfs1:isDefinedBy>
          </rdf:Description>
        </rdf:RDF>

        :param pid: The HydroShare ID of the resource
        :param destination: String representing the directory to save bag to. Bag will be saved to file named
            $(PID).zip in destination; existing file of the same name will be overwritten. If None, a stream to the
            zipped bag will be returned instead.
        :param unzip: True if the bag should be unzipped when saved to destination. Bag contents to be saved to
            directory named $(PID) residing in destination. Only applies when destination is not None.

        :param wait_for_bag_creation: True if to wait to download the bag in case the bag is not ready
            (bag needs to be recreated before it can be downloaded).
        :raises: HydroShareArgumentException if any arguments are invalid.
        :raises: HydroShareNotAuthorized if the user is not authorized to access the
            resource.
        :raises: HydroShareNotFound if the resource was not found.
        :raises: HydroShareHTTPException to signal an HTTP error
        :raise: HydroShareBagNotReady if the bag is not ready to be downloaded and wait_for_bag_creation is False

        :return: None if the bag was saved directly to disk.  Or a generator representing a buffered stream of the
            bytes comprising the bag returned by the REST end point.

        :return: A set of strings representing the HydroShare resource types

        :raises: HydroShareHTTPException to signal an HTTP error

        :param resource_type: string representing the a HydroShare resource type recognized by this
            server.
        :param title: string representing the title of the new resource
        :param resource_file: a read-only binary file-like object (i.e. opened with the flag 'rb') or a string
            representing path to file to be uploaded as part of the new resource
        :param resource_filename: string representing the filename of the resource file.  Must be specified
            if resource_file is a file-like object.  If resource_file is a string representing a valid file path,
            and resource_filename is not specified, resource_filename will be equal to os.path.basename(resource_file).
            is a string
        :param abstract: string representing abstract of resource
        :param keywords: list of strings representing keywords to associate with the resource
        :param edit_users: list of HydroShare usernames who will be given edit permissions
        :param view_users: list of HydroShare usernames who will be given view permissions
        :param edit_groups: list of HydroShare group names that will be given edit permissions
        :param view_groups: list of HydroShare group names that will be given view permissions
        :param metadata: json string data for each of the metadata elements

        :param extra_metadata: json string data for key/value pair metadata elements defined by user

        :param progress_callback: user-defined function to provide feedback to the user about the progress
            of the upload of resource_file.  For more information, see:
            http://toolbelt.readthedocs.org/en/latest/uploading-data.html#monitoring-your-streaming-multipart-upload

        :return: string representing ID of newly created resource.

        :raises: HydroShareArgumentException if any parameters are invalid.
        :raises: HydroShareNotAuthorized if user is not authorized to perform action.
        :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.

        Set access rules for a resource.  Current only allows for setting the public or private setting.

        :param pid: The HydroShare ID of the resource
        :param public: True if the resource should be made public.

        :param pid: The HydroShare ID of the resource
        :param resource_file: a read-only binary file-like object (i.e. opened with the flag 'rb') or a string
            representing path to file to be uploaded as part of the new resource
        :param resource_filename: string representing the filename of the resource file.  Must be specified
            if resource_file is a file-like object.  If resource_file is a string representing a valid file path,
            and resource_filename is not specified, resource_filename will be equal to os.path.basename(resource_file).
            is a string

        :param progress_callback: user-defined function to provide feedback to the user about the progress
            of the upload of resource_file.  For more information, see:
            http://toolbelt.readthedocs.org/en/latest/uploading-data.html#monitoring-your-streaming-multipart-upload

        :return: Dictionary containing 'resource_id' the ID of the resource to which the file was added, and
                'file_name' the filename of the file added.

        :raises: HydroShareNotAuthorized if user is not authorized to perform action.
        :raises: HydroShareNotFound if the resource was not found.
        :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.

        :param pid: The HydroShare ID of the resource
        :param filename: String representing the name of the resource file to get.
        :param destination: String representing the directory to save the resource file to. If None, a stream
            to the resource file will be returned instead.
        :return: The path of the downloaded file (if destination was specified), or a stream to the resource
            file.

        :raises: HydroShareArgumentException if any parameters are invalid.
        :raises: HydroShareNotAuthorized if user is not authorized to perform action.
        :raises: HydroShareNotFound if the resource was not found.
        :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.
        Delete a resource file

        :param pid: The HydroShare ID of the resource
        :param filename: String representing the name of the resource file to delete

        :return: Dictionary containing 'resource_id' the ID of the resource from which the file was deleted, and
            'file_name' the filename of the file deleted.

        :raises: HydroShareNotAuthorized if user is not authorized to perform action.
        :raises: HydroShareNotFound if the resource or resource file was not found.
        :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.

        :param pid: The HydroShare ID of the resource whose resource files are to be listed.

        :raises: HydroShareArgumentException if any parameters are invalid.
        :raises: HydroShareNotAuthorized if user is not authorized to perform action.
        :raises: HydroShareNotFound if the resource was not found.
        :raises: HydroShareHTTPException if an unexpected HTTP response code is encountered.

        :return: A generator that can be used to fetch dict objects, each dict representing
            the JSON object representation of the resource returned by the REST end point.  For example:

        {
            "count": 95,
            "next": "https://www.hydroshare.org/hsapi/resource/32a08bc23a86e471282a832143491b49/file_list/?page=2",
            "previous": null,
            "results": [
                {
                    "url": "http://www.hydroshare.org/django_irods/download/32a08bc23a86e471282a832143491b49/data/contents/foo/bar.txt",
                    "size": 23550,
                    "content_type": "text/plain"
                },
                {
                    "url": "http://www.hydroshare.org/django_irods/download/32a08bc23a86e471282a832143491b49/data/contents/dem.tif",
                    "size": 107545,
                    "content_type": "image/tiff"
                },
                {
                    "url": "http://www.hydroshare.org/django_irods/download/32a08bc23a86e471282a832143491b49/data/contents/data.csv",
                    "size": 148,
                    "content_type": "text/csv"
                },
                {
                    "url": "http://www.hydroshare.org/django_irods/download/32a08bc23a86e471282a832143491b49/data/contents/data.sqlite",
                    "size": 267118,
                    "content_type": "application/x-sqlite3"
                },
                {
                    "url": "http://www.hydroshare.org/django_irods/download/32a08bc23a86e471282a832143491b49/data/contents/viz.png",
                    "size": 128,
                    "content_type": "image/png"
                }
            ]
        }
    Returns the contents of the tag if the provided path consitutes the
    base of the current pages path.

    There are two ways to provide arguments to this tag. Firstly one may
    provide a single argument that starts with a forward slash. e.g.

        {% ifancestor '/path/to/page' %}...{% endifancestor}
        {% ifancestor path_variable %}...{% endifancestor}

    In this case the provided path will be used directly.

    Alternatively any arguments accepted by the standard "url" tag may
    be provided. They will be passed to the url tag and the resultant
    path will be used. e.g.

        {% ifancestor 'core:model:detail' model.pk %}...{% endifancestor}

    Ultimately the provided path is matched against the path of the
    current page. If the provided path is found at the root of the current
    path it will be considered an anscestor, and the contents of this tag
    will be rendered.

    Print ping exit statistics
		static method that calls os.walk, but filters out
		anything that doesn't match the filter
    Set the appropriate Cache-Control and Expires headers for the given
    number of hours.
        The main event of the utility. Create or update a Cloud Formation
        stack. Injecting properties where needed

        Args:
            None

        Returns:
            True if the stack create/update is started successfully else
            False if the start goes off in the weeds.

        Exits:
            If the user asked for a dryrun exit(with a code 0) the thing here. There is no
            point continuing after that point.

        List the existing stacks in the indicated region

        Args:
            None

        Returns:
            True if True

        Todo:
            Figure out what could go wrong and take steps
            to hanlde problems.
        Smash the given stack

        Args:
            None

        Returns:
            True if True

        Todo:
            Figure out what could go wrong and take steps
            to hanlde problems.
        The utililty requires boto3 clients to Cloud Formation and S3. Here is
        where we make them.

        Args:
            None

        Returns:
            Good or Bad; True or False
        Get parameters from Simple Systems Manager

        Args:
            p - a parameter name

        Returns:
            a value, decrypted if needed, if successful or None if things go
            sideways.
        Fill in the _parameters dict from the properties file.

        Args:
            None

        Returns:
            True

        Todo:
            Figure out what could go wrong and at least acknowledge the the
            fact that Murphy was an optimist.
        Fill in the _tags dict from the tags file.

        Args:
            None

        Returns:
            True

        Todo:
            Figure what could go wrong and at least acknowledge the
            the fact that Murphy was an optimist.
        Determine if we are creating a new stack or updating and existing one.
        The update member is set as you would expect at the end of this query.

        Args:
            None

        Returns:
            True
        We are putting stuff into S3, were supplied the bucket. Here we
        craft the key of the elements we are putting up there in the
        internet clouds.

        Args:
            None

        Returns:
            a tuple of teplate file key and property file key
        Spin in a loop while the Cloud Formation process either fails or succeeds

        Args:
            None

        Returns:
            Good or bad; True or False
        '''
        A wrapper for run_trial that catches exceptions and returns them.
        It is meant for async simulations
        Return parsed data. Parse it if not already parsed.

        Returns:
            list: list of dictionaries (one for each parsed line).
        Find the files and parse them.

        Returns:
            list: list of dictionaries (one for each parsed line).
    if value < 0:
        value = random.random()
    for d in sorted(distribution, key=lambda x: x['threshold']):
        threshold = d['threshold']

        # Check if the definition matches by id (first) or by threshold
        if not ((agent_id is not None and threshold == STATIC_THRESHOLD and agent_id in d['ids']) or \
                (value >= threshold[0] and value < threshold[1])):
            continue
        state = {}
        if 'state' in d:
            state = deepcopy(d['state'])
        return d['agent_type'], state

    raise Exception('Distribution for value {} not found in: {}'.format(value, distribution))

def launch(self, port=None):
    Get stats for status codes by date.

    Returns:
        list: status codes + date grouped by type: 2xx, 3xx, 4xx, 5xx, attacks.
        Creates a type converter for an enumeration or text-to-value mapping.

        :param enum_mappings: Defines enumeration names and values.
        :return: Type converter function object for the enum/mapping.
        Creates a type converter for a number of type converter alternatives.
        The first matching type converter is used.

        REQUIRES: type_converter.pattern attribute

        :param converters: List of type converters as alternatives.

        :param re_opts:  Regular expression options zu use (=default_re_opts).
        :param compiled: Use compiled regexp matcher, if true (=False).
        :param strict:   Enable assertion checks.
        :return: Type converter function object.

        .. note::

            Works only with named fields in :class:`parse.Parser`.
            Parser needs group_index delta for unnamed/fixed fields.

            This is not supported for user-defined types.
            Otherwise, you need to use :class:`parse_type.parse.Parser`
            (patched version of the :mod:`parse` module).

        Args:
            w (str): A string to be tested against the set of valid
                quantities units.

        Returns:
            True if the string can be used as a unit in the quantities
            module.
        appending consecutive words from the string and cross-referncing
        them with a set of valid units.

        Args:
            inp (str): Some text which hopefully contains descriptions
                of different units.

        Returns:
            A list of strings, each entry in which is a valid quantities
            unit.
        quantities object.

        Args:
            inp (str): A textual representation of some quantity of units,
                e.g., "fifty kilograms".

        Returns:
            A quantities object representing the described quantity and its
            units.
    Supports fq_name and wilcards resolution.

    >>> expand_paths(['virtual-network',
                      'floating-ip/2a0a54b4-a420-485e-8372-42f70a627ec9'])
    [Collection('virtual-network'),
     Resource('floating-ip', uuid='2a0a54b4-a420-485e-8372-42f70a627ec9')]

    :param paths: list of paths relative to the current path
                  that may contain wildcards (*, ?) or fq_names
    :type paths: [str]
    :param predicate: function to filter found resources
    :type predicate: f(resource) -> bool
    :param filters: list of filters for Collections
    :type filters: [(name, value), ...]
    :rtype: [Resource or Collection]
    :raises BadPath: path cannot be resolved
        Renders a Jinja2 template to text.
        Utility method to spawn a VSGWriter for each element in a collection.

        :param list pylist:   A list of VSG objects (PrProjects, VSGSolutions, etc)
        :param bool parallel: Flag to enable asynchronous writing.
        Sets a new value for a given configuration parameter.

        If it already exists, an Exception is thrown.
        To overwrite an existing value, set overwrite to True.

        :param name: Unique name of the parameter
        :param value: Value of the configuration parameter
        :param overwrite: If true, an existing parameter of *name* gets overwritten without warning or exception.
        :type overwrite: boolean
        It is assumed that the primary type converter for cardinality=1
        is registered in the type dictionary.

        :param schema:  Parse schema (or format) for parser (as string).
        :param type_dict:  Type dictionary with type converters.
        :param type_builder: Type builder to use for missing types.
        :return: Type dictionary with missing types. Empty, if none.
        :raises: MissingTypeError,
                if a primary type converter with cardinality=1 is missing.
        Selects only the missing type names that are not in the type dictionary.

        :param schema:     Parse schema to use (as string).
        :param type_dict:  Type dictionary with type converters.
        :return: Generator with missing type names (as string).
        Matches both the subject and action, not necessarily the conditions.
    Check that a given HAL id is a valid one.

    :param hal_id: The HAL id to be checked.
    :returns: Boolean indicating whether the HAL id is valid or not.

    >>> is_valid("hal-01258754, version 1")
    True

    >>> is_valid("hal-01258754")
    True

    >>> is_valid("hal-01258754v2")
    True

    >>> is_valid("foobar")
    False
    Extract HAL ids from a text.

    :param text: The text to extract HAL ids from.
    :returns: A list of matching HAL ids.

    >>> sorted(extract_from_text("hal-01258754 hal-01258754v2 foobar"))
    ['hal-01258754', 'hal-01258754v2']
        Creates a VSG solution from a configparser instance.

        :param object config: The instance of the configparser class
        :param str section: The section name to read.
        :param kwargs:  List of additional keyworded arguments to be passed into the VSGSolution.
        :return: A valid VSGSolution instance if succesful; None otherwise.
        Creates a VSG project from a configparser instance.

        :param object config: The instance of the configparser class
        :param str section: The section name to read.
        :param kwargs:  List of additional keyworded arguments to be passed into the VSGProject.
        :return: A valid VSGProject instance if succesful; None otherwise.
        Generates one or more VSGSuite instances from command line arguments.


        :param kwargs:  List of additional keyworded arguments to be passed into the VSGSuite defined in the :meth:`~VSGSuite.make_parser` method.
        Writes the configuration to disk.
    Return a plaintext representation of a bibitem from the ``.bbl`` file.

    .. note::

        This plaintext representation can be super ugly, contain URLs and so \
        on.

    .. note::

        You need to have ``delatex`` installed system-wide, or to build it in \
                this repo, according to the ``README.md`` before using this \
                function.

    :param bibitem: The text content of the bibitem.
    :returns: A cleaned plaintext citation from the bibitem.

        :param type_name:  Type name (as string).
        :return: Tuple (type_basename, cardinality)

        :param basename:  Type basename of primary type (as string).
        :param cardinality: Cardinality of the new type (as Cardinality item).
        :return: Type name with CardinalityField suffix (if needed)
        Create missing type variants for types with a cardinality field.

        :param type_names: List of type names with cardinality field suffix.
        :param type_dict:  Type dictionary with named type converters.
        :return: Type dictionary with missing type converter variants.
    Convert a single BibTeX entry dict to a BibTeX string.

    :param data: A dict representing BibTeX entry, as the ones from \
            ``bibtexparser.BibDatabase.entries`` output.
    :return: A formatted BibTeX string.
    Create a new BibTeX file.

    :param filename: The name of the BibTeX file to write.
    :param data: A ``bibtexparser.BibDatabase`` object.
    Update an entry in a BibTeX file.

    :param filename: The name of the BibTeX file to edit.
    :param identifier: The id of the entry to update, in the BibTeX file.
    :param data: A dict associating fields and updated values. Fields present \
            in the BibTeX file but not in this dict will be kept as is.
    Delete an entry in a BibTeX file.

    :param filename: The name of the BibTeX file to edit.
    :param identifier: The id of the entry to delete, in the BibTeX file.
    Get all entries from a BibTeX file.

    :param filename: The name of the BibTeX file.
    :param ignore_fields: An optional list of fields to strip from the BibTeX \
            file.

    :returns: A ``bibtexparser.BibDatabase`` object representing the fetched \
            entries.
    Convert a bibtex entry to a formatted filename according to a given mask.

    .. note ::

        Available formatters out of the box are:
            - ``journal``
            - ``title``
            - ``year``
            - ``first`` for the first author
            - ``last`` for the last author
            - ``authors`` for the list of authors
            - ``arxiv_version`` (discarded if no arXiv version in the BibTeX)

        Filename is slugified after applying the masks.

    :param data: A ``bibtexparser.BibDatabase`` object representing a \
            BibTeX entry, as the one from ``bibtexparser`` output.
    :param mask: A Python format string.
    :param extra_formatters: A dict of format string (in the mask) and \
            associated lambdas to perform the formatting.

    :returns: A formatted filename.
        if self.name:
            raise errors.Error('Already bound "{0}" with name "{1}" could not '
                               'be rebound'.format(self, self.name))
        self.name = name
        self.storage_name = ''.join(('_', self.name))
        return self

def bind_model_cls(self, model_cls):

        :param DomainModel model:
        :param object value:

        :param DomainModel model:

        :param object default:
        :rtype object:

        :param DomainModel model:
        :param object value:

        :param class model_cls:
        :param object data:
        :rtype DomainModel:

        :param DomainModel model:
        :rtype list:
    Getter function to retrieve objects from a given object dictionary.

    Used mainly to provide get() inside patterns.

    :param object_dict: objects, which must have 'name' and 'plugin' as attribute
    :type object_dict: dictionary
    :param name: name of the object
    :type name: str
    :param plugin: plugin name, which registers the object
    :return: None, single object or dict of objects

        :rtype: str

        :param field_name: name of the field to filter on
        :type field_name: str
        :param field_value: value to filter on

        :rtype: Collection
        Fetch collection from API server

        :param recursive: level of recursion
        :type recursive: int
        :param fields: fetch only listed fields.
                       contrail 3.0 required
        :type fields: [str]
        :param detail: fetch all fields
        :type detail: bool
        :param filters: list of filters
        :type filters: [(name, value), ...]
        :param parent_uuid: filter by parent_uuid
        :type parent_uuid: v4UUID str or list of v4UUID str
        :param back_refs_uuid: filter by back_refs_uuid
        :type back_refs_uuid: v4UUID str or list of v4UUID str

        :rtype: Collection

        :raises ResourceNotFound: if the resource doesn't exists

        :rtype: FQName

        :rtype: Resource
        :raises ResourceNotFound: parent resource doesn't exists

        :raises ResourceMissing: parent resource is not defined

        :param resource: parent resource
        :type resource: Resource

        :raises ResourceNotFound: resource not found on the API

        :rtype: datetime
        :raises ResourceNotFound: resource not found on the API

        If the resource doesn't have a uuid the resource will be created.
        If uuid is present the resource is updated.

        :rtype: Resource

        :param recursive: level of recursion for fetching resources
        :type recursive: int
        :param exclude_children: don't get children references
        :type exclude_children: bool
        :param exclude_back_refs: don't get back_refs references
        :type exclude_back_refs: bool

        :rtype: Resource

        :param recursive: level of recursion for fetching resources
        :type recursive: int

        >>> iip = Resource('instance-ip',
                           uuid='30213cf9-4b03-4afc-b8f9-c9971a216978',
                           fetch=True)
        >>> for vmi in iip['virtual_machine_interface_refs']:
                iip.remove_ref(vmi)
        >>> iip['virtual_machine_interface_refs']
        KeyError: u'virtual_machine_interface_refs'

        :param ref: reference to remove
        :type ref: Resource

        :rtype: Resource

        Can be used to set references on a resource
        that is not already created.

        :param ref: reference to add
        :type ref: Resource

        :rtype: Resource

        :param ref: reference to add
        :type ref: Resource

        :rtype: Resource

        :param back_ref: back_ref to add
        :type back_ref: Resource

        :rtype: Resource

        :param strings: list of strings to get from the cache
        :type strings: str list
        :param limit: limit search results
        :type limit: int

        :rtype: [Resource | Collection]
        Registers a new signal.

        :param signal: Unique name of the signal
        :param plugin: Plugin, which registers the new signal
        :param description: Description of the reason or use case, why this signal is needed.
                            Used for documentation.
        Unregisters an existing signal

        :param signal: Name of the signal
        Disconnect a receiver from a signal.
        Signal and receiver must exist, otherwise an exception is thrown.

        :param receiver: Name of the receiver
        Get one or more signals.

        :param signal: Name of the signal
        :type signal: str
        :param plugin: Plugin object, under which the signals where registered
        :type plugin: GwBasePattern
        Get one or more receivers.

        :param receiver: Name of the signal
        :type receiver: str
        :param plugin: Plugin object, under which the signals where registered
        :type plugin: GwBasePattern

    conf.init(), db.init(conf.DbPath)

    Listener(inqueue, outqueue).run()

def main():


        vkey = self._keyname(event.GetKey())

        if event.Message in self.KEYS_UP + self.KEYS_DOWN:

            if vkey in self.MODIFIERNAMES:

                self._realmodifiers[vkey] = event.Message in self.KEYS_DOWN

                self._modifiers[self.MODIFIERNAMES[vkey]] = self._realmodifiers[vkey]

        if event.Message not in self.KEYS_DOWN:

            return True



        is_altgr = False

        if (vkey, event.IsExtended()) in self.NUMPAD_SPECIALS:

            key = vkey = "Numpad-" + vkey

        elif not event.Ascii or vkey.startswith("Numpad"):

            key = vkey

        else:

            is_altgr = event.Ascii in self.ALT_GRS

            key = self._keyname(unichr(event.Ascii))



        if DEBUG: print("Adding key %s (real %s)" % (key.encode("utf-8"), vkey.encode("utf-8")))

        self._output(type="keys", key=key, realkey=vkey)



        if vkey not in self.MODIFIERNAMES and not is_altgr:

            modifier = "-".join(k for k in ["Ctrl", "Alt", "Shift", "Win"]

                                if self._modifiers[k])

            if modifier and modifier != "Shift": # Shift-X is not a combo

                if self._modifiers["Ctrl"] and event.Ascii:

                    key = self._keyname(unichr(event.KeyID))

                realmodifier = "-".join(k for k, v in self._realmodifiers.items() if v)

                realkey = "%s-%s" % (realmodifier, key)

                key = "%s-%s" % (modifier, key)

                if DEBUG: print("Adding combo %s (real %s)" % (key.encode("utf-8"), realkey.encode("utf-8")))

                self._output(type="combos", key=key, realkey=realkey)



        if DEBUG:

            print("CHARACTER: %r" % key)

            print('GetKey: {0}'.format(event.GetKey()))  # Name of the virtual keycode, str

            print('IsAlt: {0}'.format(event.IsAlt()))  # Was the alt key depressed?, bool

            print('IsExtended: {0}'.format(event.IsExtended()))  # Is this an extended key?, bool

            print('IsInjected: {0}'.format(event.IsInjected()))  # Was this event generated programmatically?, bool

            print('IsTransition: {0}'.format(event.IsTransition()))  #Is this a transition from up to down or vice versa?, bool

            print('ASCII: {0}'.format(event.Ascii))  # ASCII value, if one exists, str

            print('KeyID: {0}'.format(event.KeyID))  # Virtual key code, int

            print('ScanCode: {0}'.format(event.ScanCode))  # Scan code, int

            print('Message: {0}'.format(event.Message))  # Name of the virtual keycode, str

            print()

        return True

def _handle_mac(self, keycode):


        if character is None: return

        key = self._keyname(character, keycode)

        if key in self.MODIFIERNAMES:

            self._modifiers[self.MODIFIERNAMES[key]] = press

            self._realmodifiers[key] = press

        if press:

            self._output(type="keys", key=key, realkey=key)

        if press and key not in self.MODIFIERNAMES:

            modifier = "-".join(k for k in ["Ctrl", "Alt", "Shift", "Win"]

                                if self._modifiers[k])

            if modifier and modifier != "Shift": # Shift-X is not a combo

                realmodifier = "-".join(k for k, v in self._realmodifiers.items() if v)

                realkey = "%s-%s" % (realmodifier, key)

                key = "%s-%s" % (modifier, key)

                if DEBUG: print("Adding combo %s (real %s)" % (key.encode("utf-8"), realkey.encode("utf-8")))

                self._output(type="combos", key=key, realkey=realkey)

def _store_documentation(self, path, html, overwrite, quiet):

        echo("Storing groundwork application documents\n")
        echo("Application: %s" % self.app.name)
        echo("Number of documents: %s\n" % len(self.app.documents.get()))

        if not os.path.isabs(path):
            path = os.path.abspath(path)

        if not os.path.isdir(path):
            echo("Path %s is not a directory!" % path)
            sys.exit(1)

        if not os.path.exists(path):
            echo("Path %s does not exist" % path)
            sys.exit(1)

        for dirpath, dirnames, files in os.walk(path):
            if files:
                echo("Path %s is not empty!\n" % path)
                if not overwrite:
                    sys.exit(1)

        documents = []
        for key, document in self.app.documents.get().items():
            file_extension = ".html" if html else ".rst"

            # lowers the name, removes all whitespaces and adds the file extension
            file_name_parts = key.lower().split()
            file_name = "".join(file_name_parts)
            file_name += file_extension
            documents.append((file_name, document))

        echo("Going to write to following files:")
        for document in documents:
            echo("  %s" % document[0])

        echo("\nTarget directory: %s" % path)
        answer = None
        while answer not in ["N", "Y"] and not quiet:
            answer = prompt("Shall we go on? [Y]es, [N]o: ").upper()

        if answer == "N":
            sys.exit(0)

        for document in documents:
            try:
                with open(os.path.join(path, document[0]), "w") as doc_file:
                    doc_rendered = Environment().from_string(document[1].content).render(app=self.app,
                                                                                         plugin=document[1].plugin)
                    if html:
                        output = publish_parts(doc_rendered, writer_name="html")['whole']
                    else:
                        output = doc_rendered

                    doc_file.write(output)
            except Exception as e:
                echo("%s error occurred: %s" % (document[0], e))
            else:
                echo("%s stored." % document[0])

def _show_documentation(self):
        documents = []
        for key, document in self.app.documents.get().items():
            if key != "main":
                documents.append((key, document))
        documents = sorted(documents, key=lambda x: x[0])
        main = self.app.documents.get("main")
        if main is not None:
            documents.insert(0, (main.name, main))

        user_answer = ""
        index = 0
        while user_answer != "X":
            if index < 0:
                index = 0
            if index > len(documents) - 1:
                index = len(documents) - 1
            document = documents[index][1]

            os.system('cls' if os.name == 'nt' else 'clear')
            echo(Environment().from_string(document.content).render(app=self.app, plugin=document.plugin))

            source = "This document is registered by '%s' under the name '%s'" % (document.plugin.name, document.name)
            echo("-" * len(source))
            echo(source)
            echo("-" * len(source))
            commands = "Actions: "
            if index < len(documents) - 1:
                commands += "[N]ext, "
            if index > 0:
                commands += "[P]revious, "
            commands += "E[x]it"
            echo(commands)

            if index < len(documents) - 1:

                default = "N"
            elif index > 0:

                default = "P"
            else:

                default = "X"

            user_answer = prompt("Select your action", default=default).upper()

            if user_answer == "N":
                index += 1
            elif user_answer == "P":
                index -= 1

def execute_cleanup_tasks(ctx, cleanup_tasks, dry_run=False):

    # pylint: disable=redefined-outer-name
    executor = Executor(cleanup_tasks, ctx.config)
    for cleanup_task in cleanup_tasks.tasks:
        print("CLEANUP TASK: %s" % cleanup_task)
        executor.execute((cleanup_task, dict(dry_run=dry_run)))

def entrypoints(section):
    return {ep.name: ep.load() for ep in pkg_resources.iter_entry_points(section)}

def entrypoint(section, option):
    try:
        return entrypoints(section)[option]
    except KeyError:
        raise KeyError('Cannot resolve type "{}" to a recognised vsgen "{}" type.'.format(option, section))

def infer_declared(ms, namespace=None):
    '''
    conditions = []
    for m in ms:
        for cav in m.caveats:
            if cav.location is None or cav.location == '':
                conditions.append(cav.caveat_id_bytes.decode('utf-8'))
    return infer_declared_from_conditions(conditions, namespace)

def infer_declared_from_conditions(conds, namespace=None):
    '''
    conflicts = []
    # If we can't resolve that standard namespace, then we'll look for
    # just bare "declared" caveats which will work OK for legacy
    # macaroons with no namespace.
    if namespace is None:
        namespace = Namespace()
    prefix = namespace.resolve(STD_NAMESPACE)
    if prefix is None:
        prefix = ''
    declared_cond = prefix + COND_DECLARED

    info = {}
    for cond in conds:
        try:
            name, rest = parse_caveat(cond)
        except ValueError:
            name, rest = '', ''
        if name != declared_cond:
            continue
        parts = rest.split(' ', 1)
        if len(parts) != 2:
            continue
        key, val = parts[0], parts[1]
        old_val = info.get(key)
        if old_val is not None and old_val != val:
            conflicts.append(key)
            continue
        info[key] = val
    for key in set(conflicts):
        del info[key]
    return info

def _pre_activate_injection(self):
        # Let's be sure that this plugins class is registered and available on application level under
        # application.plugins.classes. This allows to reuse this class for *new* plugins.
        if not self.app.plugins.classes.exist(self.__class__.__name__):
            self.app.plugins.classes.register([self.__class__])

        self._load_needed_plugins()

        self.app.signals.send("plugin_activate_pre", self)

def register(self, signal, description):
        return self.__app.signals.register(signal, self._plugin, description)

def get(self, signal=None):
        return self.__app.signals.get(signal, self._plugin)

def get_receiver(self, receiver=None):
        return self.__app.signals.get_receiver(receiver, self._plugin)

def validate(mcs, bases, attributes):

        :type attributes: dict
        :rtype: list

        :type attributes: dict
        Event Handler when a file is deleted
    Replace multiple strings in a text.


    .. note::

        Replacements are made successively, without any warranty on the order \
        in which they are made.

    :param text: Text to replace in.
    :param replace_dict: Dictionary mapping strings to replace with their \
            substitution.
    :returns: Text after replacements.

    >>> replace_all("foo bar foo thing", {"foo": "oof", "bar": "rab"})
    'oof rab oof thing'
    Map the function on ``param``, or apply it, depending whether ``param`` \
            is a list or an item.

    :param function: The function to apply.
    :param param: The parameter to feed the function with (list or item).
    :returns: The computed value or ``None``.
    Get items from a sequence a batch at a time.

    .. note:

        Adapted from
        https://code.activestate.com/recipes/303279-getting-items-in-batches/.


    .. note:

        All batches must be exhausted immediately.

    :params iterable: An iterable to get batches from.
    :params size: Size of the batches.
    :returns: A new batch of the given size at each time.

    >>> [list(i) for i in batch([1, 2, 3, 4, 5], 2)]
    [[1, 2], [3, 4], [5]]
    Normalizes string, converts to lowercase, removes non-alpha characters,
    and converts spaces to hyphens to have nice filenames.


    From Django's "django/template/defaultfilters.py".

    >>> slugify("El pingüino Wenceslao hizo kilómetros bajo exhaustiva lluvia y frío, añoraba a su querido cachorro. ortez ce vieux whisky au juge blond qui fume sur son île intérieure, à Γαζέες καὶ μυρτιὲς δὲν θὰ βρῶ πιὰ στὸ χρυσαφὶ ξέφωτο いろはにほへとちりぬるを Pchnąć w tę łódź jeża lub ośm skrzyń fig กว่าบรรดาฝูงสัตว์เดรัจฉาน")
    'El_pinguino_Wenceslao_hizo_kilometros_bajo_exhaustiva_lluvia_y_frio_anoraba_a_su_querido_cachorro_ortez_ce_vieux_whisky_au_juge_blond_qui_fume_sur_son_ile_interieure_a_Pchnac_w_te_odz_jeza_lub_osm_skrzyn_fig'
    Get the citations of a given preprint, in plain text.

    .. note::

        Bulk download of sources from arXiv is not permitted by their API. \
                You should have a look at http://arxiv.org/help/bulk_data_s3.

    :param arxiv_id: The arXiv id (e.g. ``1401.2910`` or ``1401.2910v1``) in \
            a canonical form.
    :returns:  A list of cleaned plaintext citations.
    Get the DOIs of the papers cited in a .bbl file.

    .. note::

        Bulk download of sources from arXiv is not permitted by their API. \
                You should have a look at http://arxiv.org/help/bulk_data_s3.

    :param arxiv_id: The arXiv id (e.g. ``1401.2910`` or ``1401.2910v1``) in \
            a canonical form.
    :returns: A dict of cleaned plaintext citations and their associated DOI.
    arguments.
    Parse a plaintext file to get a clean list of plaintext citations. The \
            file should have one citation per line.

    :param file: Either the path to the plaintext file or the content of a \
            plaintext file.
    :returns:  A list of cleaned plaintext citations.
    Get the DOIs of the papers cited in a plaintext file. The file should \
            have one citation per line.

    .. note::

        This function is also used as a backend tool by most of the others \
        citations processors, to factorize the code.

    :param file: Either the path to the plaintext file or the content of a \
            plaintext file. It can also be a parsed list of plaintext \
            citations and, in this case, no preprocessing is done.
    :returns: A dict of cleaned plaintext citations and their associated DOI.
    Check that a given string is a valid ISBN.

    :param isbn_id: the isbn to be checked.
    :returns: boolean indicating whether the isbn is valid or not.

    >>> is_valid("978-3-16-148410-0")
    True

    >>> is_valid("9783161484100")
    True

    >>> is_valid("9783161484100aa")
    False

    >>> is_valid("abcd")
    False

    >>> is_valid("0136091814")
    True

    >>> is_valid("0136091812")
    False

    >>> is_valid("9780136091817")
    False

    >>> is_valid("123456789X")
    True
    Extract ISBNs from a text.

    :param text: Some text.
    :returns: A list of canonical ISBNs found in the text.

    >>> extract_from_text("978-3-16-148410-0 9783161484100 9783161484100aa abcd 0136091814 0136091812 9780136091817 123456789X")
    ['9783161484100', '9783161484100', '9783161484100', '0136091814', '123456789X']
    Get a BibTeX string for the given ISBN.

    :param isbn_identifier: ISBN to fetch BibTeX entry for.
    :returns: A BibTeX string or ``None`` if could not fetch it.

    >>> get_bibtex('9783161484100')
    '@book{9783161484100,\\n     title = {Berkeley, Oakland: Albany, Emeryville, Alameda, Kensington},\\n    author = {Peekaboo Maps},\\n      isbn = {9783161484100},\\n      year = {2009},\\n publisher = {Peek A Boo Maps}\\n}'
        command line

        rtype: command.Option generator
        the current cmd line

        rtype: command.Option generator
        command line

        rtype: command.Arg generator
        the current cmd line

        rtype: command.Arg generator
    Returns true if the elem_ref is an element reference

    :param elem_ref:
    :return:
    Gets the element referenced by elem_ref or returns the elem_ref directly if its not a reference.

    :param elem_ref:

    :param default:
    :return:
    Sets element referenced by the elem_ref. Returns the elem.

    :param elem_ref:
    :param elem:
    :return:
        inp = re.sub(r'(\b)a(\b)', r'\g<1>one\g<2>', inp)
        inp = re.sub(r'to the (.*) power', r'to \g<1>', inp)
        inp = re.sub(r'to the (.*?)(\b)', r'to \g<1>\g<2>', inp)
        inp = re.sub(r'log of', r'log', inp)
        inp = re.sub(r'(square )?root( of)?', r'sqrt', inp)
        inp = re.sub(r'squared', r'to two', inp)
        inp = re.sub(r'cubed', r'to three', inp)
        inp = re.sub(r'divided?( by)?', r'divide', inp)
        inp = re.sub(r'(\b)over(\b)', r'\g<1>divide\g<2>', inp)
        inp = re.sub(r'(\b)EE(\b)', r'\g<1>e\g<2>', inp)
        inp = re.sub(r'(\b)E(\b)', r'\g<1>e\g<2>', inp)
        inp = re.sub(r'(\b)pie(\b)', r'\g<1>pi\g<2>', inp)
        inp = re.sub(r'(\b)PI(\b)', r'\g<1>pi\g<2>', inp)


        def findImplicitMultiplications(inp):
        if len(numbers) is 1:
            return numbers[0]

        precedence = [[pow], [mul, div], [add, sub]]

        # Find most important operation
        for op_group in precedence:
            for i, op in enumerate(symbols):
                if op in op_group:
                    # Apply operation
                    a = numbers[i]
                    b = numbers[i + 1]
                    result = MathService._applyBinary(a, b, op)
                    new_numbers = numbers[:i] + [result] + numbers[i + 2:]
                    new_symbols = symbols[:i] + symbols[i + 1:]

                    return MathService._calculate(new_numbers, new_symbols)

def parseEquation(self, inp):
        inp = MathService._preprocess(inp)
        split = inp.split(' ')

        # Recursive call on unary operators
        for i, w in enumerate(split):
            if w in self.__unaryOperators__:
                op = self.__unaryOperators__[w]

                # Split equation into halves
                eq1 = ' '.join(split[:i])
                eq2 = ' '.join(split[i + 1:])

                # Calculate second half
                result = MathService._applyUnary(self.parseEquation(eq2), op)

                return self.parseEquation(eq1 + " " + str(result))


        def extractNumbersAndSymbols(inp):
            numbers = []
            symbols = []

            # Divide into values (numbers), operators (symbols)
            next_number = ""
            for w in inp.split(' '):
                if w in self.__binaryOperators__:
                    symbols.append(self.__binaryOperators__[w])

                    if next_number:
                        numbers.append(next_number)
                        next_number = ""

                else:
                    if next_number:
                        next_number += " "
                    next_number += w

            if next_number:
                numbers.append(next_number)

            # Cast numbers from words to integers

            def convert(n):
                if n in self.__constants__:
                    return self.__constants__[n]

                converter = NumberService()
                return converter.parse(n)

            numbers = [convert(n) for n in numbers]

            return numbers, symbols

        numbers, symbols = extractNumbersAndSymbols(inp)

        return MathService._calculate(numbers, symbols)

def register(self, command, description, function, params=[]):
        return self.app.commands.register(command, description, function, params, self.plugin)

def get(self, name=None):
        return self.app.commands.get(name, self.plugin)

def get(self, name=None, plugin=None):
        if plugin is not None:
            if name is None:
                command_list = {}
                for key in self._commands.keys():
                    if self._commands[key].plugin == plugin:
                        command_list[key] = self._commands[key]
                return command_list
            else:
                if name in self._commands.keys():
                    if self._commands[name].plugin == plugin:
                        return self._commands[name]
                    else:
                        return None
                else:
                    return None
        else:
            if name is None:
                return self._commands
            else:
                if name in self._commands.keys():
                    return self._commands[name]
                else:
                    return None

def unregister(self, command):
        if command not in self._commands.keys():
            self.log.warning("Can not unregister command %s" % command)
        else:
            # Click does not have any kind of a function to unregister/remove/deactivate already added commands.
            # So we need to delete the related objects manually from the click internal commands dictionary for
            # our root command.
            del(self._click_root_command.commands[command])
            # Finally lets delete the command from our internal dictionary too.
            del(self._commands[command])
            self.log.debug("Command %s got unregistered" % command)

def declared_caveat(key, value):
    '''
    if key.find(' ') >= 0 or key == '':
        return error_caveat('invalid caveat \'declared\' key "{}"'.format(key))
    return _first_party(COND_DECLARED, key + ' ' + value)

def _operation_caveat(cond, ops):
    '''
    for op in ops:
        if op.find(' ') != -1:
            return error_caveat('invalid operation name "{}"'.format(op))
    return _first_party(cond, ' '.join(ops))

def to_bytes(s):
    '''
    if isinstance(s, six.binary_type):
        return s
    if isinstance(s, six.string_types):
        return s.encode('utf-8')
    raise TypeError('want string or bytes, got {}', type(s))

def b64decode(s):
    '''
    # add padding if necessary.
    s = to_bytes(s)
    if not s.endswith(b'='):
        s = s + b'=' * (-len(s) % 4)
    try:
        if '_' or '-' in s:
            return base64.urlsafe_b64decode(s)
        else:
            return base64.b64decode(s)
    except (TypeError, binascii.Error) as e:
        raise ValueError(str(e))

def raw_urlsafe_b64encode(b):
    '''
    b = to_bytes(b)
    b = base64.urlsafe_b64encode(b)
    b = b.rstrip(b'=')  # strip padding
    return b

def cookie(
        url,
        name,
        value,
        expires=None):
    '''
    u = urlparse(url)
    domain = u.hostname
    if '.' not in domain and not _is_ip_addr(domain):
        domain += ".local"
    port = str(u.port) if u.port is not None else None
    secure = u.scheme == 'https'
    if expires is not None:
        if expires.tzinfo is not None:
            raise ValueError('Cookie expiration must be a naive datetime')
        expires = (expires - datetime(1970, 1, 1)).total_seconds()
    return http_cookiejar.Cookie(
        version=0,
        name=name,
        value=value,
        port=port,
        port_specified=port is not None,
        domain=domain,
        domain_specified=True,
        domain_initial_dot=False,
        path=u.path,
        path_specified=True,
        secure=secure,
        expires=expires,
        discard=False,
        comment=None,
        comment_url=None,
        rest=None,
        rfc2109=False,
    )

def _login(self):
        self.logger.debug("Logging into " + "{}/{}".format(self._im_api_url, "j_spring_security_check"))
        self._im_session.headers.update({'Content-Type':'application/x-www-form-urlencoded', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36'})
        #self._im_session.mount('https://', TLS1Adapter())
        #self._im_verify_ssl = False
        self.j_username = self._username
        self.j_password = self._password
        requests.packages.urllib3.disable_warnings() # Disable unverified connection warning.
        payload = {'j_username': self.j_username, 'j_password': self.j_password, 'submit':'Login'}
        
        # login to ScaleIO IM
        r = self._im_session.post(
            "{}/{}".format(self._im_api_url,"j_spring_security_check"),
            verify=self._im_verify_ssl,
            #headers = {'Content-Type':'application/x-www-form-urlencoded', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.99 Safari/537.36'},
            data=payload)
        self.logger.debug("Login POST response: " + "{}".format(r.text))

        self._im_logged_in = True
        

def _do_get(self, uri, **kwargs):
        #TODO:
        # Add error handling. Check for HTTP status here would be much more conveinent than in each calling method
        scaleioapi_get_headers = {'Content-type':'application/json','Version':'1.0'}
        self.logger.debug("_do_get() " + "{}/{}".format(self._api_url,uri))
        
        if kwargs:
            for key, value in kwargs.iteritems():
                if key == 'headers':
                    scaleio_get_headersvalue = value

        try:
            #response = self._im_session.get("{}/{}".format(self._api_url, uri), headers = scaleioapi_get_headers, payload = scaleio_payload).json()
            response = self._im_session.get("{}/{}".format(self._api_url, uri), **kwargs).json()
            #response = self._session.get(url, headers=scaleioapi_post_headers, **kwargs)
            if response.status_code == requests.codes.ok:
                return response
            else:
                raise RuntimeError("_do_get() - HTTP response error" + response.status_code)
        except:
            raise RuntimeError("_do_get() - Communication error with ScaleIO gateway")
        return response

def uploadFileToIM (self, directory, filename, title):
        self.logger.debug("uploadFileToIM(" + "{},{},{})".format(directory, filename, title))
        parameters = {'data-filename-placement':'inside',
                      'title':str(filename),
                      'filename':str(filename),
                      'type':'file',
                      'name':'files',
                      'id':'fileToUpload',
                      'multiple':''
                      }
        file_dict = {'files':(str(filename), open(directory + filename, 'rb'), 'application/x-rpm')}
        m = MultipartEncoder(fields=file_dict)
        
        temp_username = self._username
        temp_password = self._password
        temp_im_api_url = self._im_api_url
        temp_im_session = requests.Session()
        temp_im_session.mount('https://', TLS1Adapter())
        temp_im_verify_ssl = self._im_verify_ssl

        resp = temp_im_session.post(
            "{}/{}".format(temp_im_api_url,"types/InstallationPackage/instances/uploadPackage"),
            auth=HTTPBasicAuth(temp_username, temp_password),
            #headers = m.content_type,
            files = file_dict,
            verify = False,
            data = parameters
            )
        self.logger.info("Uploaded: " + "{}".format(filename))
        self.logger.debug("HTTP Response: " + "{}".format(resp.status_code))

async def dump_varint_t(writer, type_or, pv):
    width = int_mark_to_size(type_or)
    n = (pv << 2) | type_or

    buffer = _UINT_BUFFER
    for _ in range(width):
        buffer[0] = n & 0xff
        await writer.awrite(buffer)
        n >>= 8

    return width

async def dump_varint(writer, val):
    if val <= 63:
        return await dump_varint_t(writer, PortableRawSizeMark.BYTE, val)
    elif val <= 16383:
        return await dump_varint_t(writer, PortableRawSizeMark.WORD, val)
    elif val <= 1073741823:
        return await dump_varint_t(writer, PortableRawSizeMark.DWORD, val)
    else:
        if val > 4611686018427387903:
            raise ValueError('Int too big')
        return await dump_varint_t(writer, PortableRawSizeMark.INT64, val)

async def load_varint(reader):
    buffer = _UINT_BUFFER

    await reader.areadinto(buffer)
    width = int_mark_to_size(buffer[0] & PortableRawSizeMark.MASK)
    result = buffer[0]

    shift = 8
    for _ in range(width-1):
        await reader.areadinto(buffer)
        result += buffer[0] << shift
        shift += 8
    return result >> 2

async def dump_string(writer, val):
    await dump_varint(writer, len(val))
    await writer.awrite(val)

async def load_string(reader):
    ivalue = await load_varint(reader)
    fvalue = bytearray(ivalue)
    await reader.areadinto(fvalue)
    return bytes(fvalue)

async def dump_blob(writer, elem, elem_type, params=None):
    elem_is_blob = isinstance(elem, x.BlobType)
    data = bytes(getattr(elem, x.BlobType.DATA_ATTR) if elem_is_blob else elem)
    await dump_varint(writer, len(elem))
    await writer.awrite(data)

async def container_load(self, container_type, params=None, container=None, obj=None):
        elem_type = x.container_elem_type(container_type, params)
        elem_size = await self.get_element_size(elem_type=elem_type, params=params)

        # If container is of fixed size we know the size to load from the input.
        # Otherwise we have to read to the end
        data_left = len(self.iobj.buffer)
        c_len = container_type.SIZE
        if not container_type.FIX_SIZE:
            if data_left == 0:
                return None
            if data_left % elem_size != 0:
                raise helpers.ArchiveException('Container size mod elem size not 0')
            c_len = data_left // elem_size

        res = container if container else []
        for i in range(c_len):
            try:
                self.tracker.push_index(i)
                fvalue = await self._load_field(elem_type,
                                                params[1:] if params else None,
                                                x.eref(res, i) if container else None)
                self.tracker.pop()
            except Exception as e:
                raise helpers.ArchiveException(e, tracker=self.tracker) from e

            if not container:
                res.append(fvalue)
        return res

def make_index_for(package, index_dir, verbose=True):
    item_template = '<li><a href="{1}">{0}</a></li>'
    index_filename = os.path.join(index_dir, "index.html")
    if not os.path.isdir(index_dir):
        os.makedirs(index_dir)

    parts = []
    for pkg_filename in package.files:
        pkg_name = os.path.basename(pkg_filename)
        if pkg_name == "index.html":
            # -- ROOT-INDEX:
            pkg_name = os.path.basename(os.path.dirname(pkg_filename))
        else:
            pkg_name = package.splitext(pkg_name)
        pkg_relpath_to = os.path.relpath(pkg_filename, index_dir)
        parts.append(item_template.format(pkg_name, pkg_relpath_to))

    if not parts:
        print("OOPS: Package %s has no files" % package.name)
        return

    if verbose:
        root_index = not Package.isa(package.files[0])
        if root_index:
            info = "with %d package(s)" % len(package.files)
        else:
            package_versions = sorted(set(package.versions))
            info = ", ".join(reversed(package_versions))
        message = "%-30s  %s" % (package.name, info)
        print(message)

    with open(index_filename, "w") as f:
        packages = "\n".join(parts)
        text = index_template.format(title=package.name, packages=packages)
        f.write(text.strip())
        f.close()

def make_package_index(download_dir):
    if not os.path.isdir(download_dir):
        raise ValueError("No such directory: %r" % download_dir)

    pkg_rootdir = os.path.join(download_dir, "simple")
    if os.path.isdir(pkg_rootdir):
        shutil.rmtree(pkg_rootdir, ignore_errors=True)
    os.mkdir(pkg_rootdir)

    # -- STEP: Collect all packages.
    package_map = {}
    packages = []
    for filename in sorted(os.listdir(download_dir)):
        if not Package.isa(filename):
            continue
        pkg_filepath = os.path.join(download_dir, filename)
        package_name = Package.get_pkgname(pkg_filepath)
        package = package_map.get(package_name, None)
        if not package:
            # -- NEW PACKAGE DETECTED: Store/register package.
            package = Package(pkg_filepath)
            package_map[package.name] = package
            packages.append(package)
        else:
            # -- SAME PACKAGE: Collect other variant/version.
            package.files.append(pkg_filepath)

    # -- STEP: Make local PYTHON PACKAGE INDEX.
    root_package = Package(None, "Python Package Index")
    root_package.files = [ os.path.join(pkg_rootdir, pkg.name, "index.html")
                           for pkg in packages ]
    make_index_for(root_package, pkg_rootdir)
    for package in packages:
        index_dir = os.path.join(pkg_rootdir, package.name)
        make_index_for(package, index_dir)

def _convert_to_list(self, value, delimiters):
        if not value:
            return []
        if delimiters:
            return [l.strip() for l in value.split(delimiters)]
        return [l.strip() for l in value.split()]

def getlist(self, section, option, raw=False, vars=None, fallback=[], delimiters=','):
        v = self.get(section, option, raw=raw, vars=vars, fallback=fallback)
        return self._convert_to_list(v, delimiters=delimiters)

def getfile(self, section, option, raw=False, vars=None, fallback="", validate=False):
        v = self.get(section, option, raw=raw, vars=vars, fallback=fallback)
        v = self._convert_to_path(v)
        return v if not validate or os.path.isfile(v) else fallback

def getdir(self, section, option, raw=False, vars=None, fallback="", validate=False):
        v = self.get(section, option, raw=raw, vars=vars, fallback=fallback)
        v = self._convert_to_path(v)
        return v if not validate or os.path.isdir(v) else fallback

def getdirs(self, section, option, raw=False, vars=None, fallback=[]):
        globs = self.getlist(section, option, fallback=[])
        return [f for g in globs for f in glob.glob(g) if os.path.isdir(f)]

def register(self, name, content, description=None):
        return self.__app.documents.register(name, content, self._plugin, description)

def unregister(self, document):
        if document not in self.documents.keys():
            self.log.warning("Can not unregister document %s" % document)
        else:
            del (self.documents[document])
            self.__log.debug("Document %s got unregistered" % document)

def get(self, document=None, plugin=None):
        if plugin is not None:
            if document is None:
                documents_list = {}
                for key in self.documents.keys():
                    if self.documents[key].plugin == plugin:
                        documents_list[key] = self.documents[key]
                return documents_list
            else:
                if document in self.documents.keys():
                    if self.documents[document].plugin == plugin:
                        return self.documents[document]
                    else:
                        return None
                else:
                    return None
        else:
            if document is None:
                return self.documents
            else:
                if document in self.documents.keys():
                    return self.documents[document]
                else:
                    return None

def initialise_by_names(self, plugins=None):

        if plugins is None:
            plugins = []

        self._log.debug("Plugins Initialisation started")
        if not isinstance(plugins, list):
            raise AttributeError("plugins must be a list, not %s" % type(plugins))

        self._log.debug("Plugins to initialise: %s" % ", ".join(plugins))
        plugin_initialised = []
        for plugin_name in plugins:
            if not isinstance(plugin_name, str):
                raise AttributeError("plugin name must be a str, not %s" % type(plugin_name))

            plugin_class = self.classes.get(plugin_name)
            self.initialise(plugin_class.clazz, plugin_name)
            plugin_initialised.append(plugin_name)

        self._log.info("Plugins initialised: %s" % ", ".join(plugin_initialised))

def activate(self, plugins=[]):
        self._log.debug("Plugins Activation started")

        if not isinstance(plugins, list):
            raise AttributeError("plugins must be a list, not %s" % type(plugins))

        self._log.debug("Plugins to activate: %s" % ", ".join(plugins))

        plugins_activated = []
        for plugin_name in plugins:
            if not isinstance(plugin_name, str):
                raise AttributeError("plugin name must be a str, not %s" % type(plugin_name))

            if plugin_name not in self._plugins.keys() and plugin_name in self.classes._classes.keys():
                self._log.debug("Initialisation needed before activation.")
                try:
                    self.initialise_by_names([plugin_name])
                except Exception as e:
                    self._log.error("Couldn't initialise plugin %s. Reason %s" % (plugin_name, e))
                    if self._app.strict:
                        error = "Couldn't initialise plugin %s" % plugin_name
                        if sys.version_info[0] < 3:
                            error += "Reason: %s" % e
                        raise_from(Exception(error), e)
                    else:
                        continue
            if plugin_name in self._plugins.keys():
                self._log.debug("Activating plugin %s" % plugin_name)
                if not self._plugins[plugin_name].active:
                    try:
                        self._plugins[plugin_name].activate()
                    except Exception as e:
                        raise_from(
                            PluginNotActivatableException("Plugin %s could not be activated: %s" % (plugin_name,
                                                                                                    e)), e)
                    else:
                        self._log.debug("Plugin %s activated" % plugin_name)
                        plugins_activated.append(plugin_name)
                else:
                    self._log.warning("Plugin %s got already activated." % plugin_name)
                    if self._app.strict:
                        raise PluginNotInitialisableException()

        self._log.info("Plugins activated: %s" % ", ".join(plugins_activated))

def deactivate(self, plugins=[]):
        self._log.debug("Plugins Deactivation started")

        if not isinstance(plugins, list):
            raise AttributeError("plugins must be a list, not %s" % type(plugins))

        self._log.debug("Plugins to deactivate: %s" % ", ".join(plugins))

        plugins_deactivated = []
        for plugin_name in plugins:
            if not isinstance(plugin_name, str):
                raise AttributeError("plugin name must be a str, not %s" % type(plugin_name))

            if plugin_name not in self._plugins.keys():
                self._log.info("Unknown activated plugin %s" % plugin_name)
                continue
            else:
                self._log.debug("Deactivating plugin %s" % plugin_name)
                if not self._plugins[plugin_name].active:
                    self._log.warning("Plugin %s seems to be already deactivated" % plugin_name)
                else:
                    try:
                        self._plugins[plugin_name].deactivate()
                    except Exception as e:
                        raise_from(
                            PluginNotDeactivatableException("Plugin %s could not be deactivated" % plugin_name), e)
                    else:
                        self._log.debug("Plugin %s deactivated" % plugin_name)
                        plugins_deactivated.append(plugin_name)

        self._log.info("Plugins deactivated: %s" % ", ".join(plugins_deactivated))

def get(self, name=None):
        if name is None:
            return self._plugins
        else:
            if name not in self._plugins.keys():
                return None
            else:
                return self._plugins[name]

def is_active(self, name):
        if name in self._plugins.keys():
            return self._plugins["name"].active
        return None

def register(self, classes=[]):
        if not isinstance(classes, list):
            raise AttributeError("plugins must be a list, not %s." % type(classes))

        plugin_registered = []

        for plugin_class in classes:
            plugin_name = plugin_class.__name__
            self.register_class(plugin_class, plugin_name)
            self._log.debug("Plugin %s registered" % plugin_name)
            plugin_registered.append(plugin_name)

        self._log.info("Plugins registered: %s" % ", ".join(plugin_registered))

def get(self, name=None):
        if name is None:
            return self._classes
        else:
            if name not in self._classes.keys():
                return None
            else:
                return self._classes[name]

def write(self):
        filters = {
            'MSGUID': lambda x: ('{%s}' % x).upper(),
            'relslnfile': lambda x: os.path.relpath(x, os.path.dirname(self.FileName))
        }
        context = {
            'sln': self
        }
        return self.render(self.__jinja_template__, self.FileName, context, filters)

def load_annotations(self, aname, sep=','):
        ann = pd.read_csv(aname)

        cell_names = np.array(list(self.adata.obs_names))
        all_cell_names = np.array(list(self.adata_raw.obs_names))

        if(ann.shape[1] > 1):
            ann = pd.read_csv(aname, index_col=0, sep=sep)
            if(ann.shape[0] != all_cell_names.size):
                ann = pd.read_csv(aname, index_col=0, header=None, sep=sep)
        else:
            if(ann.shape[0] != all_cell_names.size):
                ann = pd.read_csv(aname, header=None, sep=sep)
        ann.index = np.array(list(ann.index.astype('<U100')))
        ann1 = np.array(list(ann.T[cell_names].T.values.flatten()))
        ann2 = np.array(list(ann.values.flatten()))

        self.adata_raw.obs['annotations'] = pd.Categorical(ann2)
        self.adata.obs['annotations'] = pd.Categorical(ann1)

def dispersion_ranking_NN(self, nnm, num_norm_avg=50):

        self.knn_avg(nnm)
        D_avg = self.adata.layers['X_knn_avg']

        mu, var = sf.mean_variance_axis(D_avg, axis=0)

        dispersions = np.zeros(var.size)
        dispersions[mu > 0] = var[mu > 0] / mu[mu > 0]

        self.adata.var['spatial_dispersions'] = dispersions.copy()

        ma = np.sort(dispersions)[-num_norm_avg:].mean()
        dispersions[dispersions >= ma] = ma

        weights = ((dispersions / dispersions.max())**0.5).flatten()

        self.adata.var['weights'] = weights

        return weights

def plot_correlated_groups(self, group=None, n_genes=5, **kwargs):
        geneID_groups = self.adata.uns['gene_groups']
        if(group is None):
            for i in range(len(geneID_groups)):
                self.show_gene_expression(geneID_groups[i][0], **kwargs)
        else:
            for i in range(n_genes):
                self.show_gene_expression(geneID_groups[group][i], **kwargs)

def plot_correlated_genes(
            self,
            name,
            n_genes=5,
            number_of_features=1000,
            **kwargs):
        all_gene_names = np.array(list(self.adata.var_names))
        if((all_gene_names == name).sum() == 0):
            print(
                "Gene not found in the filtered dataset. Note that genes "
                "are case sensitive.")
            return
        sds = self.corr_bin_genes(
            input_gene=name,
            number_of_features=number_of_features)
        if (n_genes + 1 > sds.size):
            x = sds.size
        else:
            x = n_genes + 1

        for i in range(1, x):
            self.show_gene_expression(sds[i], **kwargs)
        return sds[1:]

def run_tsne(self, X=None, metric='correlation', **kwargs):
        if(X is not None):
            dt = man.TSNE(metric=metric, **kwargs).fit_transform(X)
            return dt

        else:
            dt = man.TSNE(metric=self.distance,
                          **kwargs).fit_transform(self.adata.obsm['X_pca'])
            tsne2d = dt
            self.adata.obsm['X_tsne'] = tsne2d

def run_umap(self, X=None, metric=None, **kwargs):

        import umap as umap

        if metric is None:
            metric = self.distance

        if(X is not None):
            umap_obj = umap.UMAP(metric=metric, **kwargs)
            dt = umap_obj.fit_transform(X)
            return dt

        else:
            umap_obj = umap.UMAP(metric=metric, **kwargs)
            umap2d = umap_obj.fit_transform(self.adata.obsm['X_pca'])
            self.adata.obsm['X_umap'] = umap2d

def scatter(self, projection=None, c=None, cmap='rainbow', linewidth=0.0,
                edgecolor='k', axes=None, colorbar=True, s=10, **kwargs):

        if (not PLOTTING):
            print("matplotlib not installed!")
        else:
            if(isinstance(projection, str)):
                try:
                    dt = self.adata.obsm[projection]
                except KeyError:
                    print('Please create a projection first using run_umap or'
                          'run_tsne')

            elif(projection is None):
                try:
                    dt = self.adata.obsm['X_umap']
                except KeyError:
                    try:
                        dt = self.adata.obsm['X_tsne']
                    except KeyError:
                        print("Please create either a t-SNE or UMAP projection"
                              "first.")
                        return
            else:
                dt = projection

            if(axes is None):
                plt.figure()
                axes = plt.gca()

            if(c is None):
                plt.scatter(dt[:, 0], dt[:, 1], s=s,
                            linewidth=linewidth, edgecolor=edgecolor, **kwargs)
            else:

                if isinstance(c, str):
                    try:
                        c = self.adata.obs[c].get_values()
                    except KeyError:
                        0  # do nothing

                if((isinstance(c[0], str) or isinstance(c[0], np.str_)) and
                   (isinstance(c, np.ndarray) or isinstance(c, list))):
                    i = ut.convert_annotations(c)
                    ui, ai = np.unique(i, return_index=True)
                    cax = axes.scatter(dt[:,0], dt[:,1], c=i, cmap=cmap, s=s,
                                       linewidth=linewidth,
                                       edgecolor=edgecolor,
                                       **kwargs)

                    if(colorbar):
                        cbar = plt.colorbar(cax, ax=axes, ticks=ui)
                        cbar.ax.set_yticklabels(c[ai])
                else:
                    if not (isinstance(c, np.ndarray) or isinstance(c, list)):
                        colorbar = False
                    i = c

                    cax = axes.scatter(dt[:,0], dt[:,1], c=i, cmap=cmap, s=s,
                                       linewidth=linewidth,
                                       edgecolor=edgecolor,
                                       **kwargs)

                    if(colorbar):
                        plt.colorbar(cax, ax=axes)

def show_gene_expression(self, gene, avg=True, axes=None, **kwargs):

        all_gene_names = np.array(list(self.adata.var_names))
        cell_names = np.array(list(self.adata.obs_names))
        all_cell_names = np.array(list(self.adata_raw.obs_names))

        idx = np.where(all_gene_names == gene)[0]
        name = gene
        if(idx.size == 0):
            print(
                "Gene note found in the filtered dataset. Note that genes "
                "are case sensitive.")
            return

        if(avg):
            a = self.adata.layers['X_knn_avg'][:, idx].toarray().flatten()

            if a.sum() == 0:
                a = np.log2(self.adata_raw.X[np.in1d(
                    all_cell_names, cell_names), :][:,
                                                idx].toarray().flatten() + 1)
        else:
            a = np.log2(self.adata_raw.X[np.in1d(
                all_cell_names, cell_names), :][:,
                                                idx].toarray().flatten() + 1)

        if axes is None:
            plt.figure()
            axes = plt.gca()

        self.scatter(c=a, axes=axes, **kwargs)
        axes.set_title(name)

def louvain_clustering(self, X=None, res=1, method='modularity'):

        if X is None:
            X = self.adata.uns['neighbors']['connectivities']
            save = True
        else:
            if not sp.isspmatrix_csr(X):
                X = sp.csr_matrix(X)
            save = False

        import igraph as ig
        import louvain

        adjacency = sparse_knn(X.dot(X.T) / self.k, self.k).tocsr()
        sources, targets = adjacency.nonzero()
        weights = adjacency[sources, targets]
        if isinstance(weights, np.matrix):
            weights = weights.A1
        g = ig.Graph(directed=True)
        g.add_vertices(adjacency.shape[0])
        g.add_edges(list(zip(sources, targets)))
        try:
            g.es['weight'] = weights
        except BaseException:
            pass

        if method == 'significance':
            cl = louvain.find_partition(g, louvain.SignificanceVertexPartition)
        else:
            cl = louvain.find_partition(
                g,
                louvain.RBConfigurationVertexPartition,
                resolution_parameter=res)

        if save:
            self.adata.obs['louvain_clusters'] = pd.Categorical(np.array(cl.membership))
        else:
            return np.array(cl.membership)

def kmeans_clustering(self, numc, X=None, npcs=15):

        from sklearn.cluster import KMeans
        if X is None:
            D_sub = self.adata.uns['X_processed']
            X = (
                D_sub -
                D_sub.mean(0)).dot(
                self.adata.uns['pca_obj'].components_[
                    :npcs,
                    :].T)
            save = True
        else:
            save = False

        cl = KMeans(n_clusters=numc).fit_predict(Normalizer().fit_transform(X))

        if save:
            self.adata.obs['kmeans_clusters'] = pd.Categorical(cl)
        else:
            return cl

def identify_marker_genes_rf(self, labels=None, clusters=None,
                                 n_genes=4000):
        if(labels is None):
            try:
                keys = np.array(list(self.adata.obs_keys()))
                lbls = self.adata.obs[ut.search_string(
                    keys, '_clusters')[0][0]].get_values()
            except KeyError:
                print("Please generate cluster labels first or set the "
                      "'labels' keyword argument.")
                return
        elif isinstance(labels, str):
            lbls = self.adata.obs[labels].get_values().flatten()
        else:
            lbls = labels

        from sklearn.ensemble import RandomForestClassifier

        markers = {}
        if clusters == None:
            lblsu = np.unique(lbls)
        else:
            lblsu = np.unique(clusters)

        indices = np.argsort(-self.adata.var['weights'].values)
        X = self.adata.layers['X_disp'][:, indices[:n_genes]].toarray()
        for K in range(lblsu.size):
            print(K)
            y = np.zeros(lbls.size)

            y[lbls == lblsu[K]] = 1

            clf = RandomForestClassifier(n_estimators=100, max_depth=None,
                                         random_state=0)

            clf.fit(X, y)

            idx = np.argsort(-clf.feature_importances_)

            markers[lblsu[K]] = self.adata.uns['ranked_genes'][idx]
        
        if clusters is None:
            self.adata.uns['marker_genes_rf'] = markers

        return markers

def identify_marker_genes_corr(self, labels=None, n_genes=4000):
        if(labels is None):
            try:
                keys = np.array(list(self.adata.obs_keys()))
                lbls = self.adata.obs[ut.search_string(
                    keys, '_clusters')[0][0]].get_values()
            except KeyError:
                print("Please generate cluster labels first or set the "
                      "'labels' keyword argument.")
                return
        elif isinstance(labels, str):
            lbls = self.adata.obs[labels].get_values().flatten()
        else:
            lbls = labels


        w=self.adata.var['weights'].values
        s = StandardScaler()
        idxg = np.argsort(-w)[:n_genes]
        y1=s.fit_transform(self.adata.layers['X_disp'][:,idxg].A)*w[idxg]
        
        all_gene_names = np.array(list(self.adata.var_names))[idxg]

        markers = {}
        lblsu=np.unique(lbls)
        for i in lblsu:
            Gcells = np.array(list(self.adata.obs_names[lbls==i]))
            z1 = y1[np.in1d(self.adata.obs_names,Gcells),:]
            m1 = (z1 - z1.mean(1)[:,None])/z1.std(1)[:,None]            
            ref = z1.mean(0)
            ref = (ref-ref.mean())/ref.std()
            g2 = (m1*ref).mean(0)    
            markers[i] = all_gene_names[np.argsort(-g2)]
            

        self.adata.uns['marker_genes_corr'] = markers
        return markers

def add(self, action=None, subject=None, **conditions):
        self.add_rule(Rule(True, action, subject, **conditions))

def addnot(self, action=None, subject=None, **conditions):
        self.add_rule(Rule(False, action, subject, **conditions))

def can(self, action, subject, **conditions):
        for rule in self.relevant_rules_for_match(action, subject):
            if rule.matches_conditions(action, subject, **conditions):
                return rule.base_behavior
        return False

def relevant_rules_for_match(self, action, subject):
        Accepts an array of actions and returns an array of actions which match
        Alias one or more actions into another one.

        self.alias_action('create', 'read', 'update', 'delete', to='crud')


    return select(table, cols, where, group, order, limit, **kwargs).fetchall()

def fetchone(table, cols="*", where=(), group="", order=(), limit=(), **kwargs):


    values = dict(values, **kwargs).items()

    sql, args = makeSQL("INSERT", table, values=values)

    return execute(sql, args).lastrowid

def select(table, cols="*", where=(), group="", order=(), limit=(), **kwargs):


    where = dict(where, **kwargs).items()

    sql, args = makeSQL("UPDATE", table, values=values, where=where)

    return execute(sql, args).rowcount

def delete(table, where=(), **kwargs):


    connection = _connectioncache.get(path)

    if not connection:

        is_new = not os.path.exists(path) or not os.path.getsize(path)

        try: is_new and os.makedirs(os.path.dirname(path))

        except OSError: pass

        connection = sqlite3.connect(path, isolation_level=None,

            check_same_thread=False, detect_types=sqlite3.PARSE_DECLTYPES)

        for x in init_statements or (): connection.execute(x)

        try: is_new and ":memory:" not in path.lower() and os.chmod(path, 0707)

        except OSError: pass

        connection.row_factory = lambda cur, row: dict(sqlite3.Row(cur, row))

        _connectioncache[path] = connection

    return connection.cursor()

def continue_prompt(message=""):
    answer = False
    message = message + "\n'Yes' or 'No' to continue: "
    while answer not in ('Yes', 'No'):
        answer = prompt(message, eventloop=eventloop())
        if answer == "Yes":
            answer = True
            break
        if answer == "No":
            answer = False
            break
    return answer

def printo(msg, encoding=None, errors='replace', std_type='stdout'):
    std = getattr(sys, std_type, sys.stdout)
    if encoding is None:
        try:
            encoding = std.encoding
        except AttributeError:
            encoding = None
    # Fallback to ascii if no encoding is found
    if encoding is None:
        encoding = 'ascii'
    # https://docs.python.org/3/library/sys.html#sys.stdout
    # write in the binary buffer directly in python3
    if hasattr(std, 'buffer'):
        std = std.buffer
    std.write(msg.encode(encoding, errors=errors))
    std.write(b'\n')
    std.flush()

def format_tree(tree):


    def _traverse_tree(tree, parents=None):
        tree['parents'] = parents
        childs = tree.get('childs', [])
        nb_childs = len(childs)
        for index, child in enumerate(childs):
            child_parents = list(parents) + [index == nb_childs - 1]
            tree['childs'][index] = _traverse_tree(
                tree['childs'][index],
                parents=child_parents)
        return tree

    tree = _traverse_tree(tree, parents=[])


    def _get_rows_data(tree, rows):
        prefix = ''
        for p in tree['parents'][:-1]:
            if p is False:
                prefix += '│   '
            else:
                prefix += '    '
        if not tree['parents']:
            pass
        elif tree['parents'][-1] is True:
            prefix += '└── '
        else:
            prefix += '├── '
        if isinstance(tree['node'], string_types):
            tree['node'] = [tree['node']]
        rows.append([prefix + tree['node'][0]] + tree['node'][1:])
        for child in tree.get('childs', []):
            rows = _get_rows_data(child, rows)
        return rows

    rows = _get_rows_data(tree, [])
    return format_table(rows)

def parallel_map(func, iterable, args=None, kwargs=None, workers=None):
    if args is None:
        args = ()
    if kwargs is None:
        kwargs = {}
    if workers is not None:
        pool = Pool(workers)
    else:
        pool = Group()
    iterable = [pool.spawn(func, i, *args, **kwargs) for i in iterable]
    pool.join(raise_error=True)
    for idx, i in enumerate(iterable):
        i_type = type(i.get())
        i_value = i.get()
        if issubclass(i_type, BaseException):
            raise i_value
        iterable[idx] = i_value
    return iterable

def parse(self, words):

        def exact(words):

        Supports two kinds of descriptions: those with a 'point' (e.g.,
        "one point two five") and those with a fraction (e.g., "one and
        a quarter").

        Args:
            words (str): Description of the floating-point number.

        Returns:
            A double representation of the words.

        Args:
            words (str): Description of the integer.

        Returns:
            An integer representation of the words.
            Converts raw number string to an integer.
            Based on text2num.py by Greg Hewill.
        For example, crops off floats if they're too accurate.

        Arguments:
            m (float): Floating-point number to be cleaned.

        Returns:
            Human-ready string description of the number.
        Convenient method for GET requests
        Returns http request status value from a POST request
        Convenient method for POST requests
        Returns http request status value from a POST request
    Dumps blob message.
    Supports both blob and raw value.

    :param writer:
    :param elem:
    :param elem_type:
    :param params:
    :return:
    Serializes container as popo

    :param obj:
    :param container:
    :param container_type:
    :param params:
    :param field_archiver:
    :return:
    Loads container of elements from the object representation. Supports the container ref.
    Returns loaded container.

    :param reader:
    :param container_type:
    :param params:
    :param container:
    :param field_archiver:
    :return:

    Dumps a message field to the object. Field is defined by the message field specification.

    :param obj:
    :param msg:
    :param field:
    :param field_archiver:
    :return:

    Loads message field from the object. Field is defined by the message field specification.
    Returns loaded value, supports field reference.

    :param reader:
    :param msg:
    :param field:
    :param field_archiver:
    :return:
    Dumps message to the object.
    Returns message popo representation.

    :param obj:
    :param msg:
    :param field_archiver:
    :return:
    Loads message if the given type from the object.
    Supports reading directly to existing message.

    :param obj:
    :param msg_type:
    :param msg:
    :param field_archiver:
    :return:
    Transform variant to the popo object representation.

    :param obj:
    :param elem:
    :param elem_type:
    :param params:
    :param field_archiver:
    :return:
    Dumps generic field to the popo object representation, according to the element specification.
    General multiplexer.

    :param obj:
    :param elem:
    :param elem_type:
    :param params:
    :return:
    Loads a field from the reader, based on the field type specification. Demultiplexer.

    :param obj:
    :param elem_type:
    :param params:
    :param elem:
    :return:
    Instantiate the given data using the blueprinter.

    Arguments
    ---------

        blueprint (collections.Mapping):

            a blueprint (JSON Schema with Seep properties)
    The entry point of the script.
        return tuple(field.bind_name(name)
                     for name, field in six.iteritems(attributes)
                     if isinstance(field, fields.Field))

def prepare_fields_attribute(attribute_name, attributes, class_name):
        return dict(
            (field.name, field.bind_model_cls(cls)) for field in model_fields)

def bind_collection_to_model_cls(cls):
        cls.Collection = type('{0}.Collection'.format(cls.__name__),
                              (cls.Collection,),
                              {'value_type': cls})
        cls.Collection.__module__ = cls.__module__

def checklist(ctx):
[ ]  Everything is checked in
[ ]  All tests pass w/ tox

RELEASE CHECKLIST:
[{x1}]  Bump version to new-version and tag repository (via bump_version)
[{x2}]  Build packages (sdist, bdist_wheel via prepare)
[{x3}]  Register and upload packages to testpypi repository (first)
[{x4}]    Verify release is OK and packages from testpypi are usable
[{x5}]  Register and upload packages to pypi repository
[{x6}]  Push last changes to Github repository

POST-RELEASE CHECKLIST:
[ ]  Bump version to new-develop-version (via bump_version)
[ ]  Adapt CHANGES (if necessary)
[ ]  Commit latest changes to Github repository
    print("build_packages:")
    ctx.run("python setup.py sdist bdist_wheel", echo=True, hide=hide)

def register(self, name, function, description=None):
        return self.__app.threads.register(name, function, self._plugin, description)

def unregister(self, thread):
        if thread not in self.threads.keys():
            self.log.warning("Can not unregister thread %s" % thread)
        else:
            del (self.threads[thread])
            self.__log.debug("Thread %s got unregistered" % thread)

def get(self, thread=None, plugin=None):
        if plugin is not None:
            if thread is None:
                threads_list = {}
                for key in self.threads.keys():
                    if self.threads[key].plugin == plugin:
                        threads_list[key] = self.threads[key]
                return threads_list
            else:
                if thread in self.threads.keys():
                    if self.threads[thread].plugin == plugin:
                        return self.threads[thread]
                    else:
                        return None
                else:
                    return None
        else:
            if thread is None:
                return self.threads
            else:
                if thread in self.threads.keys():
                    return self.threads[thread]
                else:
                    return None

def create_schema_from_xsd_directory(directory, version):
    schema = Schema(version)
    for f in _get_xsd_from_directory(directory):
        logger.info("Loading schema %s" % f)
        fill_schema_from_xsd_file(f, schema)
    return schema

def fill_schema_from_xsd_file(filename, schema):
    ifmap_statements = _parse_xsd_file(filename)
    properties_all = []

    for v in ifmap_statements.values():
        if (isinstance(v[0], IDLParser.Link)):
            src_name = v[1]
            target_name = v[2]
            src = schema._get_or_add_resource(src_name)
            target = schema._get_or_add_resource(target_name)
            if "has" in v[3]:
                src.children.append(target_name)
                target.parent = src_name
            if "ref" in v[3]:
                src.refs.append(target_name)
                target.back_refs.append(src_name)
        elif isinstance(v[0], IDLParser.Property):
            target_name = v[1][0]
            prop = ResourceProperty(v[0].name, is_list=v[0].is_list, is_map=v[0].is_map)
            if target_name != 'all':
                target = schema._get_or_add_resource(target_name)
                target.properties.append(prop)
            else:
                properties_all.append(prop)

    for r in schema.all_resources():
        schema.resource(r).properties += properties_all

def split_ls(func):
    @wraps(func)

    def wrapper(self, files, silent=True, exclude_deleted=False):
        if not isinstance(files, (tuple, list)):
            files = [files]

        counter = 0
        index = 0
        results = []

        while files:
            if index >= len(files):
                results += func(self, files, silent, exclude_deleted)
                break

            length = len(str(files[index]))
            if length + counter > CHAR_LIMIT:
                # -- at our limit
                runfiles = files[:index]
                files = files[index:]
                counter = 0
                index = 0
                results += func(self, runfiles, silent, exclude_deleted)
                runfiles = None
                del runfiles
            else:
                index += 1
                counter += length

        return results

    return wrapper

def __getVariables(self):
        if isinstance(self._client, six.string_types):
            self._client = Client(self._client, self)

        return self._client

def status(self):

        :param cmd: Command to run
        :type cmd: list
        :param stdin: Standard Input to send to the process
        :type stdin: str
        :param marshal_output: Whether or not to marshal the output from the command
        :type marshal_output: bool
        :param kwargs: Passes any other keyword arguments to subprocess
        :raises: :class:`.error.CommandError`
        :returns: list, records of results

        :param description: The description to set or lookup
        :type description: str
        :returns: :class:`.Changelist`

        :param filename: File path to add
        :type filename: str
        :param change: Changelist to add the file to
        :type change: int
        :returns: :class:`.Revision`

        :param filename: File path to add
        :type filename: str
        if self._change:
            cl = str(self._change)
            self._p4dict = {camel_case(k): v for k, v in six.iteritems(self._connection.run(['change', '-o', cl])[0])}

        if files:
            self._files = []
            if self._p4dict.get('status') == 'pending' or self._change == 0:

                change = self._change or 'default'
                data = self._connection.run(['opened', '-c', str(change)])
                self._files = [Revision(r, self._connection) for r in data]
            else:
                data = self._connection.run(['describe', str(self._change)])[0]
                depotfiles = []
                for k, v in six.iteritems(data):
                    if k.startswith('depotFile'):
                        depotfiles.append(v)
                self._files = self._connection.ls(depotfiles)

def remove(self, rev, permanent=False):
        if not isinstance(rev, Revision):
            raise TypeError('argument needs to be an instance of Revision')

        if rev not in self:
            raise ValueError('{} not in changelist'.format(rev))

        self._files.remove(rev)
        if not permanent:

            rev.changelist = self._connection.default

def revert(self, unchanged_only=False):
        if self._reverted:
            raise errors.ChangelistError('This changelist has been reverted')

        change = self._change
        if self._change == 0:

            change = 'default'

        cmd = ['revert', '-c', str(change)]

        if unchanged_only:
            cmd.append('-a')

        files = [f.depotFile for f in self._files]
        if files:
            cmd += files
            self._connection.run(cmd)

        self._files = []
        self._reverted = True

def submit(self):
        try:
            self.revert()
        except errors.ChangelistError:
            pass

        self._connection.run(['change', '-d', str(self._change)])

def create(description='<Created by Python>', connection=None):
        connection = connection or Connection()
        description = description.replace('\n', '\n\t')
        form = NEW_FORMAT.format(client=str(connection.client), description=description)
        result = connection.run(['change', '-i'], stdin=form, marshal_output=False)

        return Changelist(int(result.split()[1]), connection)

def query(self):

        :param changelist: Optional changelist to checkout the file into
        :type changelist: :class:`.Changelist`

        :param lock: Lock or unlock the file
        :type lock: bool
        :param changelist: Optional changelist to checkout the file into
        :type changelist: :class:`.Changelist`

        :param force: Force the file to sync
        :type force: bool
        :param safe: Don't sync files that were changed outside perforce
        :type safe: bool
        :param revision: Sync to a specific revision
        :type revision: int
        :param changelist: Changelist to sync to
        :type changelist: int

        :param unchanged: Only revert if the file is unchanged
        :type unchanged: bool

        :param changelist: Changelist to add the move to
        :type changelist: :class:`.Changelist`

        :param changelist: Changelist to add the move to
        :type changelist: :class:`.Changelist`
        if 'digest' not in self._p4dict:
            self._p4dict = self._connection.run(['fstat', '-m', '1', '-Ol', self.depotFile])[0]

        return self._p4dict['digest']

def view(self):
        stream = self._p4dict.get('stream')
        if stream:
            return Stream(stream, self._connection)

async def set_version(self, tp, params, version=None, elem=None):
        self.registry.set_tr(None)
        tw = TypeWrapper(tp, params)
        if not tw.is_versioned():
            return TypeWrapper.ELEMENTARY_RES

        # If not in the DB, store to the archive at the current position
        if not self.version_db.is_versioned(tw):
            if version is None:
                version = self._cur_version(tw, elem)

            await dump_uvarint(self.iobj, 0)
            await dump_uvarint(self.iobj, version)
            self.version_db.set_version(tw, 0, version)

        return self.version_db.get_version(tw)[1]

async def version(self, tp, params, version=None, elem=None):
        if self.writing:
            return await self.set_version(tp, params, version, elem)
        else:
            return await self.get_version(tp, params)

async def root_message(self, msg, msg_type=None):
        await self.root()
        await self.message(msg, msg_type)

async def dump_message(self, msg, msg_type=None):
        mtype = msg.__class__ if msg_type is None else msg_type
        fields = mtype.f_specs()
        for field in fields:
            await self.message_field(msg=msg, field=field)

async def load_message(self, msg_type, msg=None):
        msg = msg_type() if msg is None else msg
        fields = msg_type.f_specs() if msg_type else msg.__class__.f_specs()
        for field in fields:
            await self.message_field(msg, field)

        return msg

def contrail_error_handler(f):
    @wraps(f)

    def wrapper(*args, **kwargs):
        try:
            return f(*args, **kwargs)
        except HttpError as e:
            # Replace message by details to provide a
            # meaningful message
            if e.details:
                e.message, e.details = e.details, e.message
                e.args = ("%s (HTTP %s)" % (e.message, e.http_status),)
            raise
    return wrapper

def make(self, host="localhost", port=8082, protocol="http", base_uri="", os_auth_type="http", **kwargs):
        loader = loading.base.get_plugin_loader(os_auth_type)
        plugin_options = {opt.dest: kwargs.pop("os_%s" % opt.dest)
                          for opt in loader.get_options()
                          if 'os_%s' % opt.dest in kwargs}
        plugin = loader.load_from_options(**plugin_options)
        return self.load_from_argparse_arguments(Namespace(**kwargs),
                                                 host=host,
                                                 port=port,
                                                 protocol=protocol,
                                                 base_uri=base_uri,
                                                 auth=plugin)

def post_json(self, url, data, cls=None, **kwargs):
        kwargs['data'] = to_json(data, cls=cls)

        kwargs['headers'] = self.default_headers
        return self.post(url, **kwargs).json()

def put_json(self, url, data, cls=None, **kwargs):
        kwargs['data'] = to_json(data, cls=cls)

        kwargs['headers'] = self.default_headers
        return self.put(url, **kwargs).json()

def fqname_to_id(self, fq_name, type):
        data = {
            "type": type,
            "fq_name": list(fq_name)
        }
        return self.post_json(self.make_url("/fqname-to-id"), data)["uuid"]

def id_to_fqname(self, uuid, type=None):
        data = {
            "uuid": uuid
        }
        result = self.post_json(self.make_url("/id-to-fqname"), data)
        result['fq_name'] = FQName(result['fq_name'])
        if type is not None and not result['type'].replace('_', '-') == type:
            raise HttpError('uuid %s not found for type %s' % (uuid, type), http_status=404)
        return result

def add_kv_store(self, key, value):
        data = {
            'operation': 'STORE',
            'key': key,
            'value': value
        }
        return self.post(self.make_url("/useragent-kv"), data=to_json(data),

                         headers=self.default_headers).text

def remove_kv_store(self, key):
        data = {
            'operation': 'DELETE',
            'key': key
        }
        return self.post(self.make_url("/useragent-kv"), data=to_json(data),

                         headers=self.default_headers).text

def canonical_ops(ops):
    '''
    new_ops = sorted(set(ops), key=lambda x: (x.entity, x.action))
    return new_ops

def _macaroon_id_ops(ops):
    '''
    id_ops = []
    for entity, entity_ops in itertools.groupby(ops, lambda x: x.entity):
        actions = map(lambda x: x.action, entity_ops)
        id_ops.append(id_pb2.Op(entity=entity, actions=actions))
    return id_ops

def macaroon(self, version, expiry, caveats, ops):
        '''
        if len(ops) == 0:
            raise ValueError('cannot mint a macaroon associated '
                             'with no operations')

        ops = canonical_ops(ops)
        root_key, storage_id = self.root_keystore_for_ops(ops).root_key()

        id = self._new_macaroon_id(storage_id, expiry, ops)

        id_bytes = six.int2byte(LATEST_VERSION) + \
            id.SerializeToString()

        if macaroon_version(version) < MACAROON_V2:
            # The old macaroon format required valid text for the macaroon id,
            # so base64-encode it.
            id_bytes = raw_urlsafe_b64encode(id_bytes)

        m = Macaroon(
            root_key,
            id_bytes,
            self.location,
            version,
            self.namespace,
        )
        m.add_caveat(checkers.time_before_caveat(expiry), self.key,
                     self.locator)
        m.add_caveats(caveats, self.key, self.locator)
        return m

def ops_entity(self, ops):
        '''
        # Hash the operations, removing duplicates as we go.
        hash_entity = hashlib.sha256()
        for op in ops:
            hash_entity.update('{}\n{}\n'.format(
                op.action, op.entity).encode())
        hash_encoded = base64.urlsafe_b64encode(hash_entity.digest())
        return 'multi-' + hash_encoded.decode('utf-8').rstrip('=')

def macaroon_ops(self, macaroons):
        '''
        if len(macaroons) == 0:
            raise ValueError('no macaroons provided')

        storage_id, ops = _decode_macaroon_id(macaroons[0].identifier_bytes)
        root_key = self.root_keystore_for_ops(ops).get(storage_id)
        if root_key is None:
            raise VerificationError(
                'macaroon key not found in storage')
        v = Verifier()
        conditions = []


        def validator(condition):
            # Verify the macaroon's signature only. Don't check any of the
            # caveats yet but save them so that we can return them.
            conditions.append(condition)
            return True
        v.satisfy_general(validator)
        try:
            v.verify(macaroons[0], root_key, macaroons[1:])
        except Exception as exc:
            # Unfortunately pymacaroons doesn't control
            # the set of exceptions that can be raised here.
            # Possible candidates are:
            # pymacaroons.exceptions.MacaroonUnmetCaveatException
            # pymacaroons.exceptions.MacaroonInvalidSignatureException
            # ValueError
            # nacl.exceptions.CryptoError
            #
            # There may be others too, so just catch everything.
            raise six.raise_from(
                VerificationError('verification failed: {}'.format(str(exc))),
                exc,
            )

        if (self.ops_store is not None
            and len(ops) == 1
                and ops[0].entity.startswith('multi-')):
            # It's a multi-op entity, so retrieve the actual operations
            # it's associated with.
            ops = self.ops_store.get_ops(ops[0].entity)

        return ops, conditions

def extend(self, iterable):
        return super(Collection, self).insert(
            index, self._ensure_value_is_valid(value))

def _ensure_value_is_valid(self, value):
    Returns container element type

    :param container_type:
    :param params:
    :return:
    Check that a given DOI is a valid canonical DOI.

    :param doi: The DOI to be checked.
    :returns: Boolean indicating whether the DOI is valid or not.

    >>> is_valid('10.1209/0295-5075/111/40005')
    True

    >>> is_valid('10.1016.12.31/nature.S0735-1097(98)2000/12/31/34:7-7')
    True

    >>> is_valid('10.1002/(SICI)1522-2594(199911)42:5<952::AID-MRM16>3.0.CO;2-S')
    True

    >>> is_valid('10.1007/978-3-642-28108-2_19')
    True

    >>> is_valid('10.1007.10/978-3-642-28108-2_19')
    True

    >>> is_valid('10.1016/S0735-1097(98)00347-7')
    True

    >>> is_valid('10.1579/0044-7447(2006)35\[89:RDUICP\]2.0.CO;2')
    True

    >>> is_valid('<geo coords="10.4515260,51.1656910"></geo>')
    False
    Get an OA version for a given DOI.

    .. note::

        Uses beta.dissem.in API.

    :param doi: A canonical DOI.
    :returns: The URL of the OA version of the given DOI, or ``None``.

    >>> get_oa_version('10.1209/0295-5075/111/40005')
    'http://arxiv.org/abs/1506.06690'
    Get OA policy for a given DOI.

    .. note::

        Uses beta.dissem.in API.

    :param doi: A canonical DOI.
    :returns: The OpenAccess policy for the associated publications, or \
            ``None`` if unknown.

    >>> tmp = get_oa_policy('10.1209/0295-5075/111/40005'); (tmp["published"], tmp["preprint"], tmp["postprint"], tmp["romeo_id"])
    ('can', 'can', 'can', '1896')

    >>> get_oa_policy('10.1215/9780822387268') is None
    True
    Get the original link behind the DOI.

    :param doi: A canonical DOI.
    :returns: The canonical URL behind the DOI, or ``None``.

    >>> get_linked_version('10.1209/0295-5075/111/40005')
    'http://stacks.iop.org/0295-5075/111/i=4/a=40005?key=crossref.9ad851948a976ecdf216d4929b0b6f01'
    Get a BibTeX entry for a given DOI.

    .. note::

        Adapted from https://gist.github.com/jrsmith3/5513926.

    :param doi: The canonical DOI to get BibTeX from.
    :returns: A BibTeX string or ``None``.

    >>> get_bibtex('10.1209/0295-5075/111/40005')
    '@article{Verney_2015,\\n\\tdoi = {10.1209/0295-5075/111/40005},\\n\\turl = {http://dx.doi.org/10.1209/0295-5075/111/40005},\\n\\tyear = 2015,\\n\\tmonth = {aug},\\n\\tpublisher = {{IOP} Publishing},\\n\\tvolume = {111},\\n\\tnumber = {4},\\n\\tpages = {40005},\\n\\tauthor = {Lucas Verney and Lev Pitaevskii and Sandro Stringari},\\n\\ttitle = {Hybridization of first and second sound in a weakly interacting Bose gas},\\n\\tjournal = {{EPL}}\\n}'
        Configures the logging module with a given dictionary, which in most cases was loaded from a configuration
        file.


        If no dictionary is provided, it falls back to a default configuration.

        See `Python docs
        <https://docs.python.org/3.5/library/logging.config.html#logging.config.dictConfig>`_ for more information.

        :param logger_dict: dictionary for logger.

        Dumps a message field to the writer. Field is defined by the message field specification.

        :param writer:
        :param msg:
        :param field:
        :param fvalue:
        :return:

        Loads message field from the reader. Field is defined by the message field specification.
        Returns loaded value, supports field reference.

        :param reader:
        :param msg:
        :param field:
        :return:
        Manually starts timer with the message.

        :param message:  The display message.
        Manually stops timer with the message.

        :param message:  The display message.
    Try to fetch BibTeX from a found identifier.

    .. note::

        Calls the functions in the respective identifiers module.

    :param identifier: a tuple (type, identifier) with a valid type.
    :returns: A BibTeX string or ``None`` if an error occurred.
    # TODO: Should return a BiBTeX object?
        Only try to parse as JSON if the JSON content type
        header is set.
    Parse a BibTeX file to get a clean list of plaintext citations.

    :param bibtex: Either the path to the BibTeX file or the content of a \
            BibTeX file.
    :returns:  A list of cleaned plaintext citations.

    section, parts = "DEFAULT", filename.rsplit(":", 1)

    if len(parts) > 1 and os.path.isfile(parts[0]): filename, section = parts

    if not os.path.isfile(filename): return



    vardict, parser = globals(), configparser.RawConfigParser()

    parser.optionxform = str # Force case-sensitivity on names

    try:


        def parse_value(raw):

            try: return json.loads(raw) # Try to interpret as JSON

            except ValueError: return raw # JSON failed, fall back to raw

        txt = open(filename).read() # Add DEFAULT section if none present

        if not re.search("\\[\\w+\\]", txt): txt = "[DEFAULT]\n" + txt

        parser.readfp(StringIO.StringIO(txt), filename)

        for k, v in parser.items(section): vardict[k] = parse_value(v)

    except Exception:

        logging.warn("Error reading config from %s.", filename, exc_info=True)

def save(filename=ConfigPath):


    if values: return values

    save_types = basestring, int, float, tuple, list, dict, type(None)

    for k, v in globals().items():

        if isinstance(v, save_types) and not k.startswith("_"): values[k] = v

    return values

def fix_pdf(pdf_file, destination):
    tmp = tempfile.NamedTemporaryFile()
    with open(tmp.name, 'wb') as output:
        with open(pdf_file, "rb") as fh:
            for line in fh:
                output.write(line)
                if b'%%EOF' in line:
                    break
    shutil.copy(tmp.name, destination)

def tearpage_backend(filename, teared_pages=None):

    # Handle default argument
    if teared_pages is None:
        teared_pages = [0]

    # Copy the pdf to a tmp file
    with tempfile.NamedTemporaryFile() as tmp:
        # Copy the input file to tmp
        shutil.copy(filename, tmp.name)

        # Read the copied pdf
        # TODO: Use with syntax
        try:
            input_file = PdfFileReader(open(tmp.name, 'rb'))
        except PdfReadError:
            fix_pdf(filename, tmp.name)
            input_file = PdfFileReader(open(tmp.name, 'rb'))
        # Seek for the number of pages
        num_pages = input_file.getNumPages()

        # Write pages excepted the first one
        output_file = PdfFileWriter()
        for i in range(num_pages):
            if i in teared_pages:
                continue
            output_file.addPage(input_file.getPage(i))

        tmp.close()
        outputStream = open(filename, "wb")
        output_file.write(outputStream)

def tearpage_needed(bibtex):
    for publisher in BAD_JOURNALS:
        if publisher in bibtex.get("journal", "").lower():
            # Bad journal is found, add pages to tear
            return BAD_JOURNALS[publisher]

    # If no bad journals are found, return an empty list
    return []

def tearpage(filename, bibtex=None, force=None):
    # Fetch pages to tear
    pages_to_tear = []
    if force is not None:
        pages_to_tear = force
    elif bibtex is not None:
        pages_to_tear = tearpage_needed(bibtex)

    if len(pages_to_tear) > 0:
        # If tearing is needed, do it and return True
        tearpage_backend(filename, teared_pages=pages_to_tear)
        return True

    # Else, simply return False
    return False

def edit(filename, connection=None):
    c = connection or connect()
    rev = c.ls(filename)
    if rev:
        rev[0].edit()

def sync(filename, connection=None):
    c = connection or connect()
    rev = c.ls(filename)
    if rev:
        rev[0].sync()

def open(filename, connection=None):
    c = connection or connect()
    res = c.ls(filename)
    if res and res[0].revision:
        res[0].edit()
    else:
        c.add(filename)

def is_valid(arxiv_id):
    match = REGEX.match(arxiv_id)
    return  (match is not None) and (match.group(0) == arxiv_id)

def get_bibtex(arxiv_id):
    # Fetch bibtex using arxiv2bib module
    try:
        bibtex = arxiv2bib.arxiv2bib([arxiv_id])
    except HTTPError:
        bibtex = []

    for bib in bibtex:
        if isinstance(bib, arxiv2bib.ReferenceErrorInfo):
            continue
        else:
            # Return fetched bibtex
            return bib.bibtex()
    # An error occurred, return None
    return None

def extract_from_text(text):
    # Remove the leading "arxiv:".
    return tools.remove_duplicates([re.sub("arxiv:", "", i[0],
                                           flags=re.IGNORECASE)
                                    for i in REGEX.findall(text) if i[0] != ''])

def from_doi(doi):
    try:
        request = requests.get("http://export.arxiv.org/api/query",
                               params={
                                   "search_query": "doi:%s" % (doi,),
                                   "max_results": 1
                               })
        request.raise_for_status()
    except RequestException:
        return None
    root = xml.etree.ElementTree.fromstring(request.content)
    for entry in root.iter("{http://www.w3.org/2005/Atom}entry"):
        arxiv_id = entry.find("{http://www.w3.org/2005/Atom}id").text
        # arxiv_id is an arXiv full URL. We only want the id which is the last
        # URL component.
        return arxiv_id.split("/")[-1]
    return None

def get_sources(arxiv_id):
    try:
        request = requests.get(ARXIV_EPRINT_URL.format(arxiv_id=arxiv_id))
        request.raise_for_status()
        file_object = io.BytesIO(request.content)
        return tarfile.open(fileobj=file_object)
    except (RequestException, AssertionError, tarfile.TarError):
        return None

def extractDates(inp, tz=None, now=None):
    service = DateService(tz=tz, now=now)
    return service.extractDates(inp)

def extractTimes(self, inp):

        def handleMatch(time):
            relative = False

            if not time:
                return None

            # Default times: 8am, 12pm, 7pm
            elif time.group(1) == 'morning':
                h = 8
                m = 0
            elif time.group(1) == 'afternoon':
                h = 12
                m = 0
            elif time.group(1) == 'evening':
                h = 19
                m = 0
            elif time.group(4) and time.group(5):
                h, m = 0, 0

                # Extract hours difference
                converter = NumberService()
                try:
                    diff = converter.parse(time.group(4))
                except:
                    return None

                if time.group(5) == 'hours':
                    h += diff
                else:
                    m += diff

                # Extract minutes difference
                if time.group(6):
                    converter = NumberService()
                    try:
                        diff = converter.parse(time.group(7))
                    except:
                        return None

                    if time.group(8) == 'hours':
                        h += diff
                    else:
                        m += diff

                relative = True
            else:
                # Convert from "HH:MM pm" format
                t = time.group(2)
                h, m = int(t.split(':')[0]) % 12, int(t.split(':')[1])

                try:
                    if time.group(3) == 'pm':
                        h += 12
                except IndexError:
                    pass

            if relative:
                return self.now + datetime.timedelta(hours=h, minutes=m)
            else:
                return datetime.datetime(
                    self.now.year, self.now.month, self.now.day, h, m
                )

        inp = self._preprocess(inp)
        return [handleMatch(time) for time in self._timeRegex.finditer(inp)]

def extractDates(self, inp):

        def merge(param):
            day, time = param
            if not (day or time):
                return None

            if not day:
                return time
            if not time:
                return day

            return datetime.datetime(
                day.year, day.month, day.day, time.hour, time.minute
            )

        days = self.extractDays(inp)
        times = self.extractTimes(inp)
        return map(merge, zip_longest(days, times, fillvalue=None))

def extractDate(self, inp):
        dates = self.extractDates(inp)
        for date in dates:
            return date
        return None

def convertDay(self, day, prefix="", weekday=False):

        def sameDay(d1, d2):
            d = d1.day == d2.day
            m = d1.month == d2.month
            y = d1.year == d2.year
            return d and m and y

        tom = self.now + datetime.timedelta(days=1)

        if sameDay(day, self.now):
            return "today"
        elif sameDay(day, tom):
            return "tomorrow"

        if weekday:
            dayString = day.strftime("%A, %B %d")
        else:
            dayString = day.strftime("%B %d")

        # Ex) Remove '0' from 'August 03'
        if not int(dayString[-2]):
            dayString = dayString[:-2] + dayString[-1]

        return prefix + " " + dayString

def convertTime(self, time):
        # if ':00', ignore reporting minutes
        m_format = ""
        if time.minute:
            m_format = ":%M"

        timeString = time.strftime("%I" + m_format + " %p")

        # if '07:30', cast to '7:30'
        if not int(timeString[0]):
            timeString = timeString[1:]

        return timeString

def convertDate(self, date, prefix="", weekday=False):
        dayString = self.convertDay(
            date, prefix=prefix, weekday=weekday)
        timeString = self.convertTime(date)
        return dayString + " at " + timeString

def _move(self):
        newpath = self.action['newpath']
        try:
            self.fs.move(self.fp,newpath)
        except OSError:
            raise tornado.web.HTTPError(400)
        return newpath

def _copy(self):
        copypath = self.action['copypath']
        try:
            self.fs.copy(self.fp,copypath)
        except OSError:
            raise tornado.web.HTTPError(400)
        return copypath

def _rename(self):
        newname = self.action['newname']
        try:
            newpath = self.fs.rename(self.fp,newname)
        except OSError:
            raise tornado.web.HTTPError(400)
        return newpath

def get(self):
        res = self.fs.get_filesystem_details()
        res = res.to_dict()
        self.write(res)

def put(self):
        self.fp = self.get_body_argument('filepath')
        self.action = self.get_body_argument('action')

        try:
            ptype = self.fs.get_type_from_path(self.fp)
        except OSError:
            raise tornado.web.HTTPError(404)
        if ptype == 'directory':
            self.handler_name = 'filesystem:directories-details'
        else:
            self.handler_name = 'filesystem:files-details'

        if self.action['action'] == 'move':
            newpath = self._move()
            self.write({'filepath':newpath})
        elif self.action['action'] == 'copy':
            newpath = self._copy()
            self.write({'filepath':newpath})
        elif self.action['action'] == 'rename':
            newpath = self._rename()
            self.write({'filepath':newpath})
        else:
            raise tornado.web.HTTPError(400)

def post(self, *args):
        filepath = self.get_body_argument('filepath')
        if not self.fs.exists(filepath):
            raise tornado.web.HTTPError(404)

        Filewatcher.add_directory_to_watch(filepath)
        self.write({'msg':'Watcher added for {}'.format(filepath)})

def delete(self, filepath):
        Filewatcher.remove_directory_to_watch(filepath)
        self.write({'msg':'Watcher deleted for {}'.format(filepath)})

def get(self, filepath):
        try:
            res = self.fs.get_file_details(filepath)
            res = res.to_dict()
            self.write(res)
        except OSError:
            raise tornado.web.HTTPError(404)

def put(self, filepath):
        action = self.get_body_argument('action')

        if action['action'] == 'update_group':
            newgrp = action['group']
            try:
                self.fs.update_group(filepath,newgrp)
                self.write({'msg':'Updated group for {}'.format(filepath)})
            except OSError:
                raise tornado.web.HTTPError(404)
        elif action['action'] == 'update_permissions':
            newperms = action['permissions']
            try:
                self.fs.update_permissions(filepath,newperms)
                self.write({'msg':'Updated permissions for {}'.format(filepath)})
            except OSError:
                raise tornado.web.HTTPError(404)
        else:
            raise tornado.web.HTTPError(400)

def delete(self, filepath):
        try:
            self.fs.delete(filepath)
            self.write({'msg':'File deleted at {}'.format(filepath)})
        except OSError:
            raise tornado.web.HTTPError(404)

def post(self):
        filepath = self.get_body_argument('filepath')

        try:
            self.fs.create_directory(filepath)
            encoded_filepath = tornado.escape.url_escape(filepath,plus=True)
            resource_uri = self.reverse_url('filesystem:directories-details', encoded_filepath)
            self.write({'uri':resource_uri})
        except OSError:
            raise tornado.web.HTTPError(404)

def get(self, filepath):
        exists = self.fs.exists(filepath)
        if exists:
            mime = magic.Magic(mime=True)
            mime_type = mime.from_file(filepath)
            if mime_type in self.unsupported_types:
                self.set_status(204)
                return
            else:
                contents = self.fs.read_file(filepath)
            self.write({'filepath':filepath,'contents': contents})
        else:
            raise tornado.web.HTTPError(404)

def post(self, filepath):
        try:
            content = self.get_body_argument('content')
            self.fs.write_file(filepath, content)
            self.write({'msg': 'Updated file at {}'.format(filepath)})
        except OSError:
            raise tornado.web.HTTPError(404)

def get_content(self, start=None, end=None):
        with open(self.filepath, "rb") as file:
            if start is not None:
                file.seek(start)
            if end is not None:
                remaining = end - (start or 0)
            else:
                remaining = None
            while True:
                chunk_size = 64 * 1024
                if remaining is not None and remaining < chunk_size:
                    chunk_size = remaining
                chunk = file.read(chunk_size)
                if chunk:
                    if remaining is not None:
                        remaining -= len(chunk)
                    yield chunk
                else:
                    if remaining is not None:
                        assert remaining == 0
                    return

def set_headers(self):
        self.set_header("Accept-Ranges", "bytes")

        content_type = self.get_content_type()
        if content_type:
            self.set_header("Content-Type", content_type)

def __deactivate_shared_objects(self, plugin, *args, **kwargs):
        shared_objects = self.get()
        for shared_object in shared_objects.keys():
            self.unregister(shared_object)

def get(self, name=None):
        return self.app.shared_objects.get(name, self.plugin)

def get(self, name=None, plugin=None):
        if plugin is not None:
            if name is None:
                shared_objects_list = {}
                for key in self._shared_objects.keys():
                    if self._shared_objects[key].plugin == plugin:
                        shared_objects_list[key] = self._shared_objects[key]
                return shared_objects_list
            else:
                if name in self._shared_objects.keys():
                    if self._shared_objects[name].plugin == plugin:
                        return self._shared_objects[name]
                    else:
                        return None
                else:
                    return None
        else:
            if name is None:
                return self._shared_objects
            else:
                if name in self._shared_objects.keys():
                    return self._shared_objects[name]
                else:
                    return None

def unregister(self, shared_object):
        if shared_object not in self._shared_objects.keys():
            self.log.warning("Can not unregister shared object %s" % shared_object)
        else:
            del (self._shared_objects[shared_object])
            self.log.debug("Shared object %s got unregistered" % shared_object)

def list_signals(self):
        print("Signal list")
        print("***********\n")
        for key, signal in self.app.signals.signals.items():
            print("%s (%s)\n  %s\n" % (signal.name, signal.plugin.name, signal.description))

def list_receivers(self):
        print("Receiver list")
        print("*************\n")
        for key, receiver in self.app.signals.receivers.items():
            print("%s <-- %s (%s):\n  %s\n" % (receiver.name,
                                               receiver.signal,
                                               receiver.plugin.name,
                                               receiver.description))

def toxcmd_main(args=None):

        .. code-block:: python

            yes_no_pattern = r"yes|no"
            many_yes_no = Cardinality.one_or_more.make_pattern(yes_no_pattern)

        :param pattern:  Regular expression for type (as string).
        :param listsep:  List separator for multiple items (as string, optional)
        :return: Regular expression pattern for type with cardinality.
        by using the type converter for T.

        :param cardinality: Cardinality to use (0..1, 0..*, 1..*).
        :param converter: Type converter (function) for data type T.
        :param pattern:  Regexp pattern for an item (=converter.pattern).
        :return: type-converter for optional<T> (T or None).
        by using the type converter for one item of T.

        :param converter: Type converter (function) for data type T.
        :param pattern:  Regexp pattern for an item (=converter.pattern).
        :return: type-converter for optional<T> (T or None).

    mimetype = "image/svg+xml" if filepath.endswith(".svg") else "auto"

    return bottle.static_file(filepath, root=conf.StaticPath, mimetype=mimetype)

def mouse(table, day=None):


    cols, group = "realkey AS key, COUNT(*) AS count", "realkey"

    where = (("day", day),) if day else ()

    counts_display = counts = db.fetch(table, cols, where, group, "count DESC")

    if "combos" == table:

        counts_display = db.fetch(table, "key, COUNT(*) AS count", where,

                                  "key", "count DESC")

    events = db.fetch(table, where=where, order="stamp")

    for e in events: e["dt"] = datetime.datetime.fromtimestamp(e["stamp"])

    stats, collatedevents = stats_keyboard(events, table)

    days, input = db.fetch("counts", order="day", type=table), "keyboard"

    return bottle.template("heatmap.tpl", locals(), conf=conf)

def inputindex(input):


    stats = dict((k, {"count": 0}) for k, tt in conf.InputTables)

    countminmax = "SUM(count) AS count, MIN(day) AS first, MAX(day) AS last"

    for input, table in [(x, t) for x, tt in conf.InputTables for t in tt]:

        row = db.fetchone("counts", countminmax, type=table)

        if not row["count"]: continue # for input, table

        stats[input]["count"] += row["count"]

        for func, key in [(min, "first"), (max, "last")]:

            stats[input][key] = (row[key] if key not in stats[input]

                                 else func(stats[input][key], row[key]))

    return bottle.template("index.tpl", locals(), conf=conf)

def stats_keyboard(events, table):


    return (timedelta.total_seconds() if hasattr(timedelta, "total_seconds")

            else timedelta.days * 24 * 3600 + timedelta.seconds +

                 timedelta.microseconds / 1000000.)

def init():


    global app

    bottle.run(app, host=conf.WebHost, port=conf.WebPort,

               debug=conf.WebAutoReload, reloader=conf.WebAutoReload,

               quiet=conf.WebQuiet)

def download(url, proxies=None):

    # Handle default argument
    if proxies is None:
        proxies = [""]

    # Loop over all available connections
    for proxy in proxies:
        # Handle no proxy case
        if proxy == "":
            socket.socket = DEFAULT_SOCKET
        # Handle SOCKS proxy
        elif proxy.startswith('socks'):
            if proxy[5] == '4':
                proxy_type = socks.SOCKS4
            else:
                proxy_type = socks.SOCKS5
            proxy = proxy[proxy.find('://') + 3:]
            try:
                proxy, port = proxy.split(':')
            except ValueError:
                port = None

            socks.set_default_proxy(proxy_type, proxy, port)
            socket.socket = socks.socksocket
        # Handle generic HTTP proxy
        else:
            try:
                proxy, port = proxy.split(':')
            except ValueError:
                port = None

            socks.set_default_proxy(socks.HTTP, proxy, port)
            socket.socket = socks.socksocket

        downloaded = _download_helper(url)
        if downloaded is not None:
            return downloaded

    # In case of running out of proxies, return (None, None)
    return (None, None)

def make_format(format_spec):
        fill = ''
        align = ''
        zero = ''
        width = format_spec.width
        if format_spec.align:
            align = format_spec.align[0]
            if format_spec.fill:
                fill = format_spec.fill[0]
        if format_spec.zero:
            zero = '0'

        precision_part = ""
        if format_spec.precision:
            precision_part = ".%s" % format_spec.precision

        # -- FORMAT-SPEC: [[fill]align][0][width][.precision][type]
        return "%s%s%s%s%s%s" % (fill, align, zero, width,
                                 precision_part, format_spec.type)

def extract_fields(cls, schema):
        # -- BASED-ON: parse.Parser._generate_expression()
        for part in parse.PARSE_RE.split(schema):
            if not part or part == '{{' or part == '}}':
                continue
            elif part[0] == '{':
                # this will be a braces-delimited field to handle
                yield cls.parse(part)

def _registerHandler(self, handler):
        self._logger.addHandler(handler)
        self._handlers.append(handler)

def _unregisterHandler(self, handler, shutdown=True):
        if handler in self._handlers:
            self._handlers.remove(handler)
            self._logger.removeHandler(handler)
            if shutdown:
                try:
                    handler.close()
                except KeyError:
                    # Depending on the Python version, it's possible for this call
                    # to fail most likely because some logging module objects get
                    # garbage collected before the VSGLogger object is.
                    pass

def getLogger(cls, name=None):
        return logging.getLogger("{0}.{1}".format(cls.BASENAME, name) if name else cls.BASENAME)

def debug(cls, name, message, *args):
        cls.getLogger(name).debug(message, *args)

def info(cls, name, message, *args):
        cls.getLogger(name).info(message, *args)

def warning(cls, name, message, *args):
        cls.getLogger(name).warning(message, *args)

def error(cls, name, message, *args):
        cls.getLogger(name).error(message, *args)

def critical(cls, name, message, *args):
        cls.getLogger(name).critical(message, *args)

def exception(cls, name, message, *args):
        cls.getLogger(name).exception(message, *args)

def allow(self, ctx, ops):
        '''
        auth_info, _ = self.allow_any(ctx, ops)
        return auth_info

def allow_any(self, ctx, ops):
        '''
        authed, used = self._allow_any(ctx, ops)
        return self._new_auth_info(used), authed

def allow_capability(self, ctx, ops):
        '''
        nops = 0
        for op in ops:
            if op != LOGIN_OP:
                nops += 1
        if nops == 0:
            raise ValueError('no non-login operations required in capability')

        _, used = self._allow_any(ctx, ops)
        squasher = _CaveatSquasher()
        for i, is_used in enumerate(used):
            if not is_used:
                continue
            for cond in self._conditions[i]:
                squasher.add(cond)
        return squasher.final()

def register(self, name, path, description, final_words=None):
        return self.__app.recipes.register(name, path, self._plugin, description, final_words)

def get(self, name=None):
        return self.__app.recipes.get(name, self._plugin)

def build(self, recipe):
        return self.__app.recipes.build(recipe, self._plugin)

def register(self, name, path, plugin, description=None, final_words=None):
        if name in self.recipes.keys():
            raise RecipeExistsException("Recipe %s was already registered by %s" %
                                        (name, self.recipes["name"].plugin.name))

        self.recipes[name] = Recipe(name, path, plugin, description, final_words)
        self.__log.debug("Recipe %s registered by %s" % (name, plugin.name))
        return self.recipes[name]

def unregister(self, recipe):
        if recipe not in self.recipes.keys():
            self.__log.warning("Can not unregister recipe %s" % recipe)
        else:
            del (self.recipes[recipe])
            self.__log.debug("Recipe %s got unregistered" % recipe)

def get(self, recipe=None, plugin=None):
        if plugin is not None:
            if recipe is None:
                recipes_list = {}
                for key in self.recipes.keys():
                    if self.recipes[key].plugin == plugin:
                        recipes_list[key] = self.recipes[key]
                return recipes_list
            else:
                if recipe in self.recipes.keys():
                    if self.recipes[recipe].plugin == plugin:
                        return self.recipes[recipe]
                    else:
                        return None
                else:
                    return None
        else:
            if recipe is None:
                return self.recipes
            else:
                if recipe in self.recipes.keys():
                    return self.recipes[recipe]
                else:
                    return None

def build(self, recipe, plugin=None):
        if recipe not in self.recipes.keys():
            raise RecipeMissingException("Recipe %s unknown." % recipe)

        recipe_obj = self.recipes[recipe]

        if plugin is not None:
            if recipe_obj.plugin != plugin:
                raise RecipeWrongPluginException("The requested recipe does not belong to the given plugin. Use"
                                                 "the app object, to retrieve the requested recipe: "
                                                 "my_app.recipes.get(%s)" % recipe)

        recipe_obj.build()

def build(self, output_dir=None, **kwargs):
        if output_dir is None:
            output_dir = os.getcwd()

        target = cookiecutter(self.path, output_dir=output_dir, **kwargs)

        if self.final_words is not None and len(self.final_words) > 0:
            print("")
            print(self.final_words)
        return target

def where_am_i():
    locations = {'Work':0, 'Home':0}
    for ssid in scan_for_ssids():
        #print('checking scanned_ssid ', ssid)
        for l in logged_ssids:
            #print('checking logged_ssid ', l)
            if l['name'] == ssid:
                locations[l['location']] += 1
                #print('MATCH')
    print('Where Am I: SSIDS Matching Home = ', locations['Home'], ' SSIDs matching Work = ', locations['Work'])
    
    return max(locations.keys(), key=lambda k: locations[k])

def summarise(self):
        res = ''
        if self.user == 'Developer': 
            if self.host == 'Home PC':
                res += 'At Home'
            else:
                res += 'Away from PC'
        elif self.user == 'User' and self.host == 'Home PC':
            res += 'Remote desktop into home PC'
        res += '\n'
        res += self.transport
        return res

def get_host(self):
        import socket
        host_name = socket.gethostname()
        for h in hosts:
            if h['name'] == host_name:
                return h['type'], h['name']
        return dict(type='Unknown', name=host_name)

def get_user(self):
        for name in ('LOGNAME', 'USER', 'LNAME', 'USERNAME'):
            user = os.environ.get(name)
            if user:
                break     
        for u in users:
            if u['name'] == user:
                return u['type'], u['name']

def get_host_usage(self):
        import psutil
        process_names = [proc.name for proc in psutil.process_iter()]
        cpu_pct = psutil.cpu_percent(interval=1)
        mem = psutil.virtual_memory()
        return str(cpu_pct), str(len(process_names)), str(mem.available), str(mem.total)

def schema():
        for p in self.project_list:
            if p.nme == name:
                return p
        return None

def execute_tasks(self):
        for t in self.tasks:
            print('RUNNING ' + str(t.task_id) + ' = ' + t.name)
            t.execute()
            if t.success != '__IGNORE__RESULT__':
                print(t)
                print('TASK RESULT :', t.result, ' but success = ' , t.success )
                if t.result != t.success:
                    #raise Exception('Project execution failed at task ' + str(t.task_id) + ' = ' + t.name)
                    print('ABORTING TASK EXECUTION SEQUENCE'  + str(t.task_id) + ' = ' + t.name)
                    break

def build_report(self, op_file, tpe='md'):
        if tpe == 'md':
            res = self.get_report_md()
        elif tpe == 'rst':
            res = self.get_report_rst()
        elif tpe == 'html':
            res = self.get_report_html()
        else:
            res = 'Unknown report type passed to project.build_report'
        
        with open(op_file, 'w') as f:
            f.write(res)

def get_report_rst(self):
        res = ''
        res += '-----------------------------------\n' 
        res += self.nme  + '\n'
        res += '-----------------------------------\n\n'
        res += self.desc + '\n'
        res += self.fldr + '\n\n'
        res += '.. contents:: \n\n\n'

        res += 'Overview\n' + '===========================================\n\n'
        res += 'This document contains details on the project ' + self.nme + '\n\n'
        
        for d in self.details:
            res += ' - ' + d[0] + ' = ' + d[1] + '\n\n'
            
        
        res += '\nTABLES\n' + '===========================================\n\n'
        
        for t in self.datatables:
            res +=  t.name + '\n'
            res += '-------------------------\n\n'
            res += t.format_rst() + '\n\n'
        
        
        
        return res

def get_report_html(self):
        res = '<h2>Project:' + self.nme  + '</h2>'
        res += '<p>' + self.desc + '</p>'
        res += '<p>' + self.fldr + '</p>'
        
        res += '<BR><h3>TABLES</h3>'
        
        for t in self.datatables:
            res += '<b>' + t.name + '<b><BR>'
            res += '<p>' + str(t) + '</p>'
        return res

def add_param(self, param_key, param_val):
        self.params.append([param_key, param_val])
        if param_key == '__success_test':
            self.success = param_val

def execute(self):
        func_params = []
        exec_str = self.func.__name__ + '(' 
        for p in self.params:
            if p[0][0:2] != '__':   # ignore custom param names
                exec_str += p[0] + '="' + self._force_str(p[1]) + '", '
                func_params.append(p[1])
        exec_str = exec_str[:-2]
        exec_str += ')  # task' + str(self.task_id) + ': ' + self.name
        
        
        self.result = self.func(*func_params)
        print(exec_str + ' loaded ', self.result)

def create_column_index(annotations):
    _column_index = OrderedDict({'Column Name' : annotations['Column Name']})
    categorical_rows = annotation_rows('C:', annotations)
    _column_index.update(categorical_rows)
    numerical_rows = {name: [float(x) if x != '' else float('NaN') for x in values]
            for name, values in annotation_rows('N:', annotations).items()} # to floats
    _column_index.update(numerical_rows)
    column_index = pd.MultiIndex.from_tuples(list(zip(*_column_index.values())), names=list(_column_index.keys()))
    if len(column_index.names) == 1:
        # flatten single-level index
        name = column_index.names[0]
        column_index = column_index.get_level_values(name)
    return column_index

def read_perseus(path_or_file, **kwargs):
    annotations = read_annotations(path_or_file, separator)
    column_index = create_column_index(annotations)
    if 'usecols' in kwargs:
	    usecols = kwargs['usecols']
	    if type(usecols[0]) is str:
		    usecols = sorted([list(column_index).index(x) for x in usecols])
	    column_index = column_index[usecols]
    kwargs['dtype'] = dict(kwargs.get('dtype', {}), **annotations.get('dtype', {}))
    kwargs['converters'] = dict(kwargs.get('converters', {}), **annotations.get('converters', {}))
    df = pd.read_csv(path_or_file, sep=separator, comment='#', **kwargs)
    df.columns = column_index
    return df

def to_perseus(df, path_or_file, main_columns=None,
        separator=separator,
        convert_bool_to_category=True,
        numerical_annotation_rows = set([])):
    _df = df.copy()
    if not _df.columns.name:
        _df.columns.name = 'Column Name'
    column_names = _df.columns.get_level_values('Column Name')
    annotations = {}
    main_columns = _infer_main_columns(_df) if main_columns is None else main_columns
    annotations['Type'] = ['E' if column_names[i] in main_columns else dtype_to_perseus(dtype)
            for i, dtype in enumerate(_df.dtypes)]
    # detect multi-numeric columns
    for i, column in enumerate(_df.columns):
        valid_values = [value for value in _df[column] if value is not None]
        if len(valid_values) > 0 and all(type(value) is list for value in valid_values):
            annotations['Type'][i] = 'M'
            _df[column] = _df[column].apply(lambda xs: ';'.join(str(x) for x in xs))
    if convert_bool_to_category:
        for i, column in enumerate(_df.columns):
            if _df.dtypes[i] is np.dtype('bool'):
                values = _df[column].values
                _df[column][values] = '+'
                _df[column][~values] = ''
    annotation_row_names = set(_df.columns.names) - {'Column Name'}
    for name in annotation_row_names:
        annotation_type = 'N' if name in numerical_annotation_rows else 'C'
        annotations['{}:{}'.format(annotation_type, name)] = _df.columns.get_level_values(name)
    with PathOrFile(path_or_file, 'w') as f:
        f.write(separator.join(column_names) + '\n')
        for name, values in annotations.items():
            f.write('#!{{{name}}}{values}\n'.format(name=name, values=separator.join([str(x) for x in values])))
        _df.to_csv(f, header=None, index=False, sep=separator)

def get_page(search_text):
    lst = search_aikif(search_text)
    txt = '<table class="as-table as-table-zebra as-table-horizontal">'
    for result in lst:
        txt += '<TR><TD>' + result + '</TD></TR>'
    txt += '</TABLE>\n\n'
    return txt

def search_aikif(txt, formatHTML=True):
    results = []
    num_found = 0
    import aikif.lib.cls_filelist as mod_fl
    my_files = mod_fl.FileList([aikif_folder ], ['*.*'], ['*.pyc'])
    files = my_files.get_list()
    for f in files:
        try:
            num_found = 0
            with open(f, 'r') as cur:
                line_num = 0
                for line in cur:
                    line_num += 1
                    if txt in line:
                        num_found += 1
                        if formatHTML is True:
                            results.append(format_result(line, line_num, txt))
                        else:
                            results.append([f, line, line_num, txt])
            if num_found > 0:
                if formatHTML is True:
                    results.append('<h3>' + f + ' = ' + str(num_found) + ' results</h3>')
                else:    
                    print(f + ' = ' + str(num_found) + '')
        except Exception:
            results.append('problem with file ' + f)
    if len(results) == 0:
        results.append("No results")
    return results

def format_result(line, line_num, txt):
    Modules for testing happiness of 'persons' in 'worlds'
    based on simplistic preferences. Just a toy - dont take seriously

        ----- WORLD SUMMARY for : Mars -----
        population = 0
        tax_rate   = 0.0
        tradition  = 0.9
        equity     = 0.0
        Preferences for Rover
        tax_min  = 0.0
        equity  = 0.0
        tax_max  = 0.9
        tradition  = 0.9

        Rover is Indifferent in Mars (0)
        DETAILS
                    tax: Economic = 0.1 -> 0.3
              tradition: Personal = 0.3 -> 0.9
                 equity: Personal = 0.1 -> 0.9
                 growth: Economic = 0.01 -> 0.09


        find the best world to make people happy
        extended print details of happiness parameters
        this is going to be the tricky bit - probably not possible
        to get the 'exact' rating for a value. Will need to do sentiment
        analysis of the text to see how it matches the rating. Even that
        sounds like it wont work - maybe a ML algorithm would do it, but
        that requires a large body of text already matched to values - and

        values aren't even defined as far as I have found.

        UPDATE - this could work if we assume values can be single words,
        eg tax=0.3, freedom=0.7, healthcare=0.3, welfare=0.3 etc
    convert a list to html using table formatting 
    returns the html for a simple edit form 
    returns the html to display a listbox
    returns the html with supplied list as a HTML listbox 
    formats a standard filelist to htmk using table formats 
    creates a html link for a file using folder fldr 
    converts a dictionary to a HTML table row
    reads a CSV file and converts it to HTML
    reads a CSV file and converts it to a HTML List
        the goal of the explore agent is to move to the 
        target while avoiding blockages on the grid.
        This function is messy and needs to be looked at.
        It currently has a bug in that the backtrack oscillates
        so need a new method of doing this - probably checking if
        previously backtracked in that direction for those coords, ie
        keep track of cells visited and number of times visited?
        wrapper for debugging print and log methods
        returns a Y,X value showing which direction the
        agent should move in order to get to the target
        dumps the status of the agent
    audio_dict = {}
    print("IDv2 tag info for %s:" % fname)
    try:
        audio = mutagenx.id3.ID3(fname, translate=False)
    except StandardError as err:
        print("ERROR = " + str(err))
    #else:
        #print(audio.pprint().encode("utf-8", "replace"))
        #for frame in audio.values():
        #    print(repr(frame))
    
    try:
        audio_dict["title"] = audio["title"]
    except KeyError:
        print("No title")
        
    try:
        audio_dict["artist"] = audio["artist"] # tags['TPE1'] 
    except KeyError:
        print("No artist")
        
    try:
        audio_dict["album"] = audio["album"]
    except KeyError:
        print("No album")
        
    try:
        audio_dict["length"] = audio["length"]
    except KeyError:
        print("No length")
        
    #pprint.pprint(audio.tags)
        
    return audio_dict

def calculate_columns(sequence):
    columns = {}

    for row in sequence:
        for key in row.keys():
            if key not in columns:
                columns[key] = len(key)

            value_length = len(str(row[key]))
            if value_length > columns[key]:
                columns[key] = value_length

    return columns

def calculate_row_format(columns, keys=None):
    row_format = ''
    if keys is None:
        keys = columns.keys()
    else:
        keys = [key for key in keys if key in columns]

    for key in keys:
        if len(row_format) > 0:
            row_format += "|"
        row_format += "%%(%s)-%ds" % (key, columns[key])

    return '|' + row_format + '|'

def pprint(sequence, keys=None):
    if len(sequence) > 0:
        columns = calculate_columns(sequence)
        row_format = calculate_row_format(columns, keys)
        header = row_format % dict([(key, key.title()) for key in columns])
        separator = row_format % dict([(key, '-' * columns[key]) for key in columns])

        print(separator)
        print(header)
        print(separator)

        for row in sequence:
            print(row_format % row)

        print(separator)

def matrix_worker(data):
    matrix = data['matrix']
    Logger.get_logger(__name__ + '.worker').info(
        "Processing pipeline for matrix entry '%s'", matrix['name'])

    env = matrix['env'].copy()
    env.update({'PIPELINE_MATRIX': matrix['name']})

    pipeline = Pipeline(model=data['model'], env=env, options=data['options'])
    pipeline.hooks = data['hooks']
    return pipeline.process(data['pipeline'])

def can_process_matrix(entry, matrix_tags):
        if len(matrix_tags) == 0:
            return True

        count = 0
        if 'tags' in entry:
            for tag in matrix_tags:
                if tag in entry['tags']:
                    count += 1

        return count > 0

def run_matrix_ordered(self, process_data):
        output = []
        for entry in self.matrix:
            env = entry['env'].copy()
            env.update({'PIPELINE_MATRIX': entry['name']})

            if Matrix.can_process_matrix(entry, process_data.options.matrix_tags):
                self.logger.info("Processing pipeline for matrix entry '%s'", entry['name'])
                pipeline = Pipeline(model=process_data.model, env=env,
                                    options=process_data.options)
                pipeline.hooks = process_data.hooks
                result = pipeline.process(process_data.pipeline)
                output += result['output']
                if not result['success']:
                    return {'success': False, 'output': output}
        return {'success': True, 'output': output}

def run_matrix_in_parallel(self, process_data):
        if self.parallel and not process_data.options.dry_run:
            return self.run_matrix_in_parallel(process_data)
        return self.run_matrix_ordered(process_data)

def _sqlfile_to_statements(sql):
    statements = (sqlparse.format(stmt, strip_comments=True).strip() for stmt in sqlparse.split(sql))
    return [stmt for stmt in statements if stmt]

def generate_migration_name(self, name, suffix):
        return os.path.join(self.dir,
                            'm{datestr}_{name}.{suffix}'.format(
                                datestr=datetime.datetime.utcnow().strftime('%Y%m%d%H%M%S'),
                                name=name.replace(' ', '_'),
                                suffix=suffix))

def _call_migrate(self, module, connection_param):
        args = [connection_param]
        spec = inspect.getargspec(module.migrate)
        if len(spec.args) == 2:
            args.append(self.db_config)
        return module.migrate(*args)

def _identify_datatype(self, input_data):
        if isinstance(input_data, (int, float)) :
            self.data_type = 'number'
        elif isinstance(input_data, (list)): #, set
            self.data_type = 'list'
        elif isinstance(input_data, dict):
            self.data_type = 'dict'
        elif type(input_data) is str:
            if self.input_data[0:4] == 'http':
                self.data_type = 'url'
            elif os.path.exists(input_data):
                self.data_type = 'file'
            else:
                self.data_type = 'str'
                
        lg.record_result('_identify_datatype', self.name + ' is ' + self.data_type)

def _calc_size_stats(self):
        self.total_records = 0
        self.total_length = 0
        self.total_nodes = 0
        if type(self.content['data']) is dict:
            self.total_length += len(str(self.content['data']))
            self.total_records += 1
            self.total_nodes = sum(len(x) for x in self.content['data'].values())
                               
        elif hasattr(self.content['data'], '__iter__') and type(self.content['data']) is not str:
            self._get_size_recursive(self.content['data'])
        else:
            self.total_records += 1
            self.total_length += len(str(self.content['data']))
            
        return str(self.total_records) + ' records [or ' +  str(self.total_nodes) + ' nodes], taking ' + str(self.total_length) + ' bytes'

def _get_size_recursive(self, dat):
        self.total_records += 1
        #self.total_nodes += 1
        for rec in dat:
            if hasattr(rec, '__iter__') and type(rec) is not str:
                self._get_size_recursive(rec)
            else:
                self.total_nodes += 1
                self.total_length += len(str(rec))

def _make_version(major, minor, micro, releaselevel, serial):
    url = "https://django-pagination-bootstrap.readthedocs.io"
    if releaselevel != 'final':
        # For pre-releases, use a version-specific URL.
        url += "/en/" + _make_version(major, minor, micro, releaselevel, serial)
    return url

def get_list_of_paths(self):


        all_paths = []

        for p in self.fl_metadata:

            try:

                all_paths.append(p['path'])

            except:

                try:

                    print('cls_filelist - no key path, ignoring folder ' + str(p))

                except:

                    print('cls_filelist - no key path, ignoring odd character folder')



        return list(set(all_paths))

def add_file_metadata(self, fname):




        file_dict = {}

        file_dict["fullfilename"] = fname

        try:

            file_dict["name"] = os.path.basename(fname)

            file_dict["date"] = self.GetDateAsString(fname)

            file_dict["size"] = os.path.getsize(fname)

            file_dict["path"] = os.path.dirname(fname)

        except IOError:

            print('Error getting metadata for file')



        self.fl_metadata.append(file_dict)

def print_file_details_as_csv(self, fname, col_headers):


        uses a List of files and collects meta data on them and saves

        to an text file as a list or with metadata depending on opFormat.

        connect here - use the other classes cls_oracle, cls_mysql, etc
        otherwise this has the credentials used to access a share folder
        collects a filelist of all .py programs
        Save the list of items to AIKIF core and optionally to local file fname
        gets details on the program, size, date, list of functions
        and produces a Markdown file for documentation
    Given a DAVIDenrich output it converts ensembl gene ids to genes names and adds this column to the output

    :param df: a dataframe output from DAVIDenrich
    :param GTF: a GTF dataframe from readGTF()
    :param name_id: instead of a gtf dataframe a dataframe with the columns 'gene_name' and 'gene_id' can be given as input

    :returns: a pandas dataframe with a gene name column added to it.
    Returns a list of gene names for given gene ids.

    :param x: a string with the list of IDs separated by ', '
    :param df: a dataframe with the reference column and a the column to retrieve
    :param refCol: the header of the column containing the identifiers
    :param fieldTOretrieve: the field to retrieve from parsedGTF eg. 'gene_name'

    :returns: list of fieldTOretrieve separeted by ', ' in the same order as the given in x
    application = Application(**options)

    # fails application when your defined threshold is higher than your ratio of com/loc.
    if not application.run():
        sys.exit(1)
    return application

def load_configuration(self):
        Verify whether to ignore a path.

        Args:
            path (str): path to check.

        Returns:
            bool: True when to ignore given path.
        Iterating files for given extensions.

        Args:
            supported_extensions (list): supported file extentsion for which to check loc and com.

        Returns:
            str: yield each full path and filename found.
        Find out lines of code and lines of comments.

        Args:
            path_and_filename (str): path and filename to parse  for loc and com.
            pattern (str): regex to search for line commens and block comments

        Returns:
            int, int: loc and com for given file.
    Lists BioMart datasets.


    :param host: address of the host server, default='http://www.ensembl.org/biomart'

    :returns: nothing

    Lists BioMart filters for a specific dataset.

    :param dataset: dataset to list filters of.

    :param host: address of the host server, default='http://www.ensembl.org/biomart'

    :returns: nothing

        Prepares the data in CSV format
        return a trace of parents and children of the obect
        return the list of children of a node
        return the list of links of a node
        find the child object by name and return the object
        returns the filename
        save table to folder in appropriate files
        NOTE - ONLY APPEND AT THIS STAGE - THEN USE DATABASE
        Prepares the header in CSV format
        extracts event information from core tables into diary files
            Takes a list of VariantProbeCoverages and returns a Call for the Variant.
            Note, in the simplest case the list will be of length one. However, we may be typing the
            Variant on multiple backgrouds leading to multiple VariantProbes for a single Variant.

        ansible_playbook = "ansible.playbook.dry.run.see.comment"
        ansible_inventory = "ansible.inventory.dry.run.see.comment"

        ansible_playbook_content = render(config.script, model=config.model, env=config.env,
                                          variables=config.variables, item=config.item)
        ansible_inventory_content = render(entry['inventory'], model=config.model, env=config.env,
                                           variables=config.variables, item=config.item)

        if not config.dry_run:
            ansible_playbook = write_temporary_file(ansible_playbook_content, 'ansible-play-', '.yaml')
            ansible_playbook_content = ''
            ansible_inventory = write_temporary_file(ansible_inventory_content, prefix='ansible-inventory-')
            ansible_inventory_content = ''

        # rendering the Bash script for running the Ansible playbook
        template_file = os.path.join(os.path.dirname(__file__), 'templates/ansible.sh.j2')
        with open(template_file) as handle:
            template = handle.read()
            config.script = render(template, debug=config.debug,
                                   ansible_playbook_content=ansible_playbook_content,
                                   ansible_playbook=ansible_playbook,
                                   ansible_inventory_content=ansible_inventory_content,
                                   ansible_inventory=ansible_inventory,
                                   limit=entry['limit'])

        return Ansible(config)

def update_gol(self):
        
        updated_grid = [[self.update_cell(row, col) \
                            for col in range(self.get_grid_width())] \
                            for row in range(self.get_grid_height())]
        
        self.replace_grid(updated_grid)

def update_cell(self, row, col):
        # compute number of living neighbors
        neighbors = self.eight_neighbors(row, col)
        living_neighbors = 0
        for neighbor in neighbors:
            if not self.is_empty(neighbor[0], neighbor[1]):
                living_neighbors += 1
            
        # logic for Game of life        
        if (living_neighbors == 3) or (living_neighbors == 2 and not self.is_empty(row, col)):
            return mod_grid.FULL
        else:
            return mod_grid.EMPTY

def random_offset(self, lst):
        res = []
        x = random.randint(4,self.max_x - 42)
        y = random.randint(4,self.max_y - 10)
        for itm in lst:
            res.append([itm[0] + y, itm[1] + x])
        return res

def get_random(self, size=10):
        bin_i = np.random.choice(np.arange(len(self.bin_centers)), size=size, p=self.normalized_histogram)
        return self.bin_centers[bin_i] + np.random.uniform(-0.5, 0.5, size=size) * self.bin_volumes()[bin_i]

def std(self, bessel_correction=True):
        return self.bin_centers[np.argmin(np.abs(self.cumulative_density * 100 - percentile))]

def _data_to_hist(self, data, **kwargs):
        if self.axis_names is None:
            return None
        return itemgetter(*self.other_axes(axis))(self.axis_names)

def bin_centers(self, axis=None):
        Inclusive on both endpoints
        return tuple([self.get_axis_bin_index(values[ax_i], ax_i)
                      for ax_i in range(self.dimensions)])

def all_axis_bin_centers(self, axis):
        axis = self.get_axis_number(axis)
        if self.dimensions == 2:
            new_hist = Hist1d
        else:
            new_hist = Histdd
        return new_hist.from_histogram(np.sum(self.histogram, axis=axis),
                                       bin_edges=itemgetter(*self.other_axes(axis))(self.bin_edges),
                                       axis_names=self.axis_names_without(axis))

def slicesum(self, start, stop=None, axis=0):
        axis = self.get_axis_number(axis)
        projected_hist = np.sum(self.histogram, axis=self.other_axes(axis))
        return Hist1d.from_histogram(projected_hist, bin_edges=self.bin_edges[axis])

def cumulate(self, axis):
        result = self.cumulative_density(axis)
        result.histogram = 1 - 2 * np.abs(result.histogram - 0.5)
        return result

def lookup_hist(self, mh):
        result = mh.similar_blank_histogram()
        points = np.stack([mh.all_axis_bin_centers(i)
                           for i in range(mh.dimensions)]).reshape(mh.dimensions, -1)
        values = self.lookup(*points)
        result.histogram = values.reshape(result.histogram.shape)
        return result

def create_roadmap_doc(dat, opFile):
    op = format_title('Roadmap for AIKIF')
    for h1 in dat['projects']:
        op += format_h1(h1)
        if dat[h1] is None:
            op += '(No details)\n'
        else:
            for h2 in dat[h1]:
                op += '\n' + format_h2(h2)
                if dat[h1][h2] is None:
                    op += '(blank text)\n'
                else:
                    for txt in dat[h1][h2]:
                        op += '  - ' + txt + '\n'
        op += '\n'

    with open(opFile, 'w') as f:
        f.write(op)

def clear(self):
        self.grid = [[EMPTY for dummy_col in range(self.grid_width)] for dummy_row in range(self.grid_height)]

def save(self, fname):
        
        # get height and width of grid from file
        self.grid_width = 4
        self.grid_height = 4
        
        # re-read the file and load it
        self.grid = [[0 for dummy_l in range(self.grid_width)] for dummy_l in range(self.grid_height)]
        with open(fname, 'r') as f:
            for row_num, row in enumerate(f):
                if row.strip('\n') == '':
                    break
                for col_num, col in enumerate(row.strip('\n')):   
                    self.set_tile(row_num, col_num, col)

def extract_col(self, col):
        new_col = [row[col] for row in self.grid]
        return new_col

def extract_row(self,  row):
        new_row = []
        for col in range(self.get_grid_width()):
            new_row.append(self.get_tile(row, col))    
        return new_row

def replace_row(self, line, ndx):
        for col in range(len(line)):
            self.set_tile(ndx, col, line[col])

def replace_col(self, line, ndx):
        for row in range(len(line)):
            self.set_tile(row, ndx, line[row])

def new_tile(self, num=1):
        for _ in range(num):                
            if random.random() > .5: 
                new_tile = self.pieces[0]
            else:
                new_tile = self.pieces[1]
            
            # check for game over
            blanks = self.count_blank_positions()
            
            if blanks == 0:
                print ("GAME OVER")
            else:
                res = self.find_random_blank_cell()
                row = res[0]
                col = res[1]
                self.set_tile(row, col, new_tile)

def set_tile(self, row, col, value):
        #print('set_tile: y=', row, 'x=', col)
        if col < 0:
            print("ERROR - x less than zero", col)
            col = 0
            #return
            
        if col > self.grid_width - 1 :
            print("ERROR - x larger than grid", col)
            col = self.grid_width - 1
            #return
            
        if row < 0:
            print("ERROR - y less than zero", row)
            row = 0
            #return
            
        if row > self.grid_height - 1:
            print("ERROR - y larger than grid", row)
            row = self.grid_height - 1

        self.grid[row][col] = value

def replace_grid(self, updated_grid):
        for col in range(self.get_grid_width()):
            for row in range(self.get_grid_height()):
                if updated_grid[row][col] == EMPTY:
                    self.set_empty(row, col)
                else:
                    self.set_full(row, col)

def find_safe_starting_point(self):
        y = random.randint(2,self.grid_height-4)
        x = random.randint(2,self.grid_width-4)
        return y, x

def resize(fname, basewidth, opFilename):
    stat = ImageStat.Stat(img)
    print("extrema    : ", stat.extrema)
    print("count      : ", stat.count)
    print("sum        : ", stat.sum)
    print("sum2       : ", stat.sum2)
    print("mean       : ", stat.mean)
    print("median     : ", stat.median)
    print("rms        : ", stat.rms)
    print("var        : ", stat.var)
    print("stddev     : ", stat.stddev)

def print_all_metadata(fname):
    imgdict = {}
    try:
        imgdict['filename'] = fname
        imgdict['size'] = str(os.path.getsize(fname)) 
        imgdict['basename'] = os.path.basename(fname)
        imgdict['path'] = os.path.dirname(fname)
        img = Image.open(fname)
        # get the image's width and height in pixels
        width, height = img.size
        imgdict['width'] = str(width)
        imgdict['height'] = str(height)
        imgdict['format'] = str(img.format) 
        imgdict['palette'] = str(img.palette)
        stat = ImageStat.Stat(img)
         
        #res = res + q + str(stat.extrema) + q + d
        imgdict['count'] =  List2String(stat.count, ",")
        imgdict['sum'] =  List2String(stat.sum, ",")
        imgdict['sum2'] =  List2String(stat.sum2, ",")
        imgdict['mean'] =  List2String(stat.mean, ",") 
        imgdict['median'] =  List2String(stat.median, ",") 
        imgdict['rms'] =  List2String(stat.rms, ",") 
        imgdict['var'] =  List2String(stat.var, ",")
        imgdict['stddev'] =  List2String(stat.stddev, ",") 

        exif_data = get_exif_data(img)
        print('exif_data = ', exif_data)
        (lat, lon) = get_lat_lon(exif_data)
        print('(lat, lon)', (lat, lon))
        imgdict['lat'] =  str(lat)
        imgdict['lon'] =  str(lon)
    except Exception as ex:
        print('problem reading image file metadata in ', fname, str(ex))
        imgdict['lat'] =  'ERROR'
        imgdict['lon'] =  'ERROR'
    return imgdict

def get_metadata_as_csv(fname):
    ft = ImageFont.load("T://user//dev//src//python//_AS_LIB//timR24.pil")
    #wh = ft.getsize(txt)
    print("Adding text ", txt, " to ", fname, " pixels wide to file " , opFilename)
    im = Image.open(fname)
    draw = ImageDraw.Draw(im)
    draw.text((0, 0), txt, fill=(0, 0, 0), font=ft)
    del draw  
    im.save(opFilename)

def add_crosshair_to_image(fname, opFilename):
    im = Image.open(imageFile)
    im1 = im.filter(ImageFilter.CONTOUR)
    im1.save(opFile)

def get_img_hash(image, hash_size = 8):
    with open(fname, "rb") as f:
        i = Image.open(fname)
        #i.load()
        return i

def dump_img(fname):
    Normalizes intensities of a gene in two samples

    :param df: dataframe output of GetData()
    :param sampleA: column header of sample A
    :param sampleB: column header of sample B

    :returns: normalized intensities
    Testing given number to be a prime.

    >>> [n for n in range(100+1) if is_prime(n)]
    [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]
        Returns a dict of QMED methods using all available methods.


        Available methods are defined in :attr:`qmed_methods`. The returned dict keys contain the method name, e.g.
        `amax_record` with value representing the corresponding QMED estimate in m³/s.

        :return: dict of QMED estimates
        :rtype: dict
        Return QMED estimate based on annual maximum flow records.

        :return: QMED in m³/s
        :rtype: float
        Return a list of 12 sets. Each sets contains the years included in the POT record period.

        :param pot_dataset: POT dataset (records and meta data)
        :type pot_dataset: :class:`floodestimation.entities.PotDataset`
        Return QMED estimate based on catchment area.

        TODO: add source of method

        :return: QMED in m³/s
        :rtype: float
        Return QMED estimation based on FEH catchment descriptors, 1999 methodology.

        Methodology source: FEH, Vol. 3, p. 14

        :param as_rural: assume catchment is fully rural. Default: false.
        :type as_rural: bool
        :return: QMED in m³/s
        :rtype: float
        Return QMED estimation based on FEH catchment descriptors, 2008 methodology.

        Methodology source: Science Report SC050050, p. 36

        :param as_rural: assume catchment is fully rural. Default: false.
        :type as rural: bool

        :param donor_catchments: override donor catchment to improve QMED catchment. If `None` (default),
        donor catchment will be searched automatically, if empty list, no donors will be used.
        :type donor_catchments: :class:`Catchment`
        :return: QMED in m³/s
        :rtype: float
        Return percentage runoff urban adjustment factor.

        Methodology source: eqn. 6, Kjeldsen 2010
        Generic distance-decaying correlation function

        :param dist: Distance between catchment centrolds in km
        :type dist: float
        :param phi1: Decay function parameters 1
        :type phi1: float
        :param phi2: Decay function parameters 2
        :type phi2: float
        :param phi3: Decay function parameters 3
        :type phi3: float
        :return: Correlation coefficient, r
        :rtype: float
        Return vector ``b`` of model error covariances to estimate weights

        Methodology source: Kjeldsen, Jones and Morris, 2009, eqs 3 and 10

        :param donor_catchments: Catchments to use as donors
        :type donor_catchments: list of :class:`Catchment`
        :return: Model error covariance vector
        :rtype: :class:`numpy.ndarray`
        Return beta, the GLO scale parameter divided by loc parameter estimated using simple regression model

        Methodology source: Kjeldsen & Jones, 2009, table 2

        :param catchment: Catchment to estimate beta for
        :type catchment: :class:`Catchment`
        :return: beta
        :rtype: float
        Return model error coveriance matrix Sigma eta

        Methodology source: Kjelsen, Jones & Morris 2014, eqs 2 and 3

        :param donor_catchments: Catchments to use as donors
        :type donor_catchments: list of :class:`Catchment`
        :return: 2-Dimensional, symmetric covariance matrix
        :rtype: :class:`numpy.ndarray`
        Return sampling error coveriance matrix Sigma eta

        Methodology source: Kjeldsen & Jones 2009, eq 9

        :param donor_catchments: Catchments to use as donors
        :type donor_catchments: list of :class:`Catchment`
        :return: 2-Dimensional, symmetric covariance matrix
        :rtype: :class:`numpy.ndarray`
        Return vector alpha which is the weights for donor model errors

        Methodology source: Kjeldsen, Jones & Morris 2014, eq 10

        :param donor_catchments: Catchments to use as donors
        :type donor_catchments: list of :class:`Catchment`
        :return: Vector of donor weights
        :rtype: :class:`numpy.ndarray`
        Return a suitable donor catchment to improve a QMED estimate based on catchment descriptors alone.

        :param limit: maximum number of catchments to return. Default: 6. Set to `None` to return all available
                      catchments.
        :type limit: int
        :param dist_limit: maximum distance in km. between subject and donor catchment. Default: 500 km. Increasing the
                           maximum distance will increase computation time!
        :type dist_limit: float or int
        :return: list of nearby catchments
        :rtype: :class:`floodestimation.entities.Catchment`
        Calculate L-CV and L-SKEW from a single catchment or a pooled group of catchments.

        Methodology source: Science Report SC050050, para. 6.4.1-6.4.2
        Calculate L-CV and L-SKEW for a gauged catchment. Uses `lmoments3` library.

        Methodology source: Science Report SC050050, para. 6.7.5
        Return L-CV weighting for a donor catchment.

        Methodology source: Science Report SC050050, eqn. 6.18 and 6.22a
        Return multiplier for L-CV weightings in case of enhanced single site analysis.

        Methodology source: Science Report SC050050, eqn. 6.15a and 6.15b
        Return L-SKEW weighting for donor catchment.

        Methodology source: Science Report SC050050, eqn. 6.19 and 6.22b
        Return flood growth curve function based on `amax_records` from the subject catchment only.

        :return: Inverse cumulative distribution function with one parameter `aep` (annual exceedance probability)
        :type: :class:`.GrowthCurve`
        Return flood growth curve function based on `amax_records` from a pooling group.

        :return: Inverse cumulative distribution function with one parameter `aep` (annual exceedance probability)
        :type: :class:`.GrowthCurve`
        :param as_rural: assume catchment is fully rural. Default: false.
        :type as rural: bool
        content = json.dumps(document)

        versions = {}
        versions.update({'Spline': Version(VERSION)})
        versions.update(self.get_version("Bash", self.BASH_VERSION))

        if content.find('"docker(container)":') >= 0 or content.find('"docker(image)":') >= 0:
            versions.update(VersionsCheck.get_version("Docker", self.DOCKER_VERSION))
        if content.find('"packer":') >= 0:
            versions.update(VersionsCheck.get_version("Packer", self.PACKER_VERSION))
        if content.find('"ansible(simple)":') >= 0:
            versions.update(VersionsCheck.get_version('Ansible', self.ANSIBLE_VERSION))

        return versions

def get_version(tool_name, tool_command):
        result = {}
        for line in Bash(ShellConfig(script=tool_command, internal=True)).process():
            if line.find("command not found") >= 0:
                VersionsCheck.LOGGER.error("Required tool '%s' not found (stopping pipeline)!", tool_name)
                sys.exit(1)
            else:
                version = list(re.findall(r'(\d+(\.\d+)+)+', line))[0][0]
                result = {tool_name: Version(str(version))}
            break
        return result

def process(self, versions):

        Args:
            *names (str): Name or names of the events to register

        Note:
            If a listener returns :obj:`False`, the event will stop dispatching to
            other listeners. Any other return value is ignored.

        Args:
            name (str): The name of the :class:`Event` to dispatch
            *args (Optional): Positional arguments to be sent to listeners
            **kwargs (Optional): Keyword arguments to be sent to listeners

        Args:
            name (str): The name of the :class:`Event` or
                :class:`~pydispatch.properties.Property` object to retrieve

        Returns:

            The :class:`Event` instance for the event or property definition

        .. versionadded:: 0.1.0

        The context manager returned will store the last event data called by
        :meth:`emit` and prevent callbacks until it exits. On exit, it will
        dispatch the last event captured (if any)::

            class Foo(Dispatcher):
                _events_ = ['my_event']


            def on_my_event(value):
                print(value)

            foo = Foo()
            foo.bind(my_event=on_my_event)

            with foo.emission_lock('my_event'):
                foo.emit('my_event', 1)
                foo.emit('my_event', 2)

            >>> 2

        Args:
            name (str): The name of the :class:`Event` or
                :class:`~pydispatch.properties.Property`

        Returns:
            A context manager to be used by the :keyword:`with` statement.

            If available, this will also be an async context manager to be used
            with the :keyword:`async with` statement (see `PEP 492`_).

        Note:
            The context manager is re-entrant, meaning that multiple calls to
            this method within nested context scopes are possible.

        .. _PEP 492: https://www.python.org/dev/peps/pep-0492/#asynchronous-context-managers-and-async-with
    Test function to step through all functions in
    order to try and identify all features on a map
    This test function should be placed in a main 
    section later
        print('======================================================================')
        print(self)
        print('Table  = ',  str(len(self.header)) + ' cols x ' + str(len(self.arr)) + ' rows')
        print('HEADER = ', self.get_header())
        print('arr    = ', self.arr[0:2])

def get_distinct_values_from_cols(self, l_col_list):
        uniq_vals = []
        for l_col_name in l_col_list:
            #print('col_name: ' + l_col_name)   
            uniq_vals.append(set(self.get_col_data_by_name(l_col_name)))
            #print(' unique values = ', uniq_vals)    
        
        #print(' unique values[0] = ', uniq_vals[0])
        #print(' unique values[1] = ', uniq_vals[1])
        if len(l_col_list) == 0:
            return []
        elif len(l_col_list) == 1:
            return sorted([v for v in uniq_vals])
        elif len(l_col_list) == 2:
            res = []
            res = [(a, b) for a in uniq_vals[0] for b in uniq_vals[1]]
            return res
        else:
            print ("TODO ")
            return -44

def select_where(self, where_col_list, where_value_list, col_name=''):
        res = []        # list of rows to be returned
        col_ids = []    # ids of the columns to check
        #print('select_where  : arr = ',  len(self.arr), 'where_value_list = ',  where_value_list)
        for col_id, col in enumerate(self.header):
            if col in where_col_list:
                col_ids.append([col_id, col])
        #print('select_where    : col_ids = ',  col_ids)   # correctly prints [[0, 'TERM'], [2, 'ID']]
        
        for row_num, row in enumerate(self.arr):
            keep_this_row = True
            #print('col_ids=', col_ids, ' row = ', row_num, row)
            for ndx, where_col in enumerate(col_ids):
                #print('type where_value_list[ndx] = ', type(where_value_list[ndx]))
                #print('type row[where_col[0]] = ', type(row[where_col[0]]))
                
                if row[where_col[0]] != where_value_list[ndx]:
                    keep_this_row = False
            if keep_this_row is True:
                if col_name == '':
                    res.append([row_num, row])
                else:   # extracting a single column only
                    l_dat = self.get_col_by_name(col_name)
                    if l_dat is not None:
                        res.append(row[l_dat])
        return res

def update_where(self, col, value, where_col_list, where_value_list):
        if type(col) is str:
            col_ndx = self.get_col_by_name(col)
        else:
            col_ndx = col
        #print('col_ndx = ', col_ndx    )
        #print("updating " + col + " to " , value, " where " , where_col_list , " = " , where_value_list)
        new_arr = self.select_where(where_col_list, where_value_list)
        #print('new_arr', new_arr)
        for r in new_arr:
            self.arr[r[0]][col_ndx] = value

def percentile(self, lst_data, percent , key=lambda x:x):

        default is to save a file from list of lines

        save the default array as a CSV file
        drop the table, view or delete the file
        #print('get_col_data_by_name: col_name = ', col_name, ' WHERE = ', WHERE_Clause)
        col_key = self.get_col_by_name(col_name)
        if col_key is None:
            print('get_col_data_by_name: col_name = ', col_name, ' NOT FOUND')
            return []
        #print('get_col_data_by_name: col_key =', col_key)
        res = []
        for row in self.arr:
            #print('col_key=',col_key, ' len(row)=', len(row), ' row=', row)
            res.append(row[col_key])  # need to convert to int for calcs but leave as string for lookups
        return res

def format_rst(self):
        res = ''
        num_cols = len(self.header)
        col_width = 25
        for _ in range(num_cols):
            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' 
        res += '\n'
        for c in self.header:
            res += c.ljust(col_width) 
        res += '\n'
        for _ in range(num_cols):
            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' 
        res += '\n'
        for row in self.arr:
            for c in row:
                res += self.force_to_string(c).ljust(col_width)
            res += '\n' 
        for _ in range(num_cols):
            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' 
        res += '\n'
        return res

def getHomoloGene(taxfile="build_inputs/taxid_taxname",\
                  genefile="homologene.data",\
                  proteinsfile="build_inputs/all_proteins.data",\
                  proteinsclusterfile="build_inputs/proteins_for_clustering.data",\
                  baseURL="http://ftp.ncbi.nih.gov/pub/HomoloGene/current/"):


    def getDf(inputfile):
        if os.path.isfile(inputfile):
            df=pd.read_table(inputfile,header=None)
        else:
            df = urllib2.urlopen(baseURL+inputfile)
            df=df.read().split("\n")
            df=[ s for s in df if len(s) > 0 ]
            df=[s.split("\t") for s in df]
            df=pd.DataFrame(df)
        return df

    taxdf=getDf(taxfile)
    taxdf.set_index([0],inplace=True)
    taxdi=taxdf.to_dict().get(1)

    genedf=getDf(genefile)
    genecols=["HID","Taxonomy ID","Gene ID","Gene Symbol","Protein gi","Protein accession"]
    genedf.columns=genecols
    genedf["organism"]=genedf["Taxonomy ID"].apply(lambda x:taxdi.get(x))

    proteinsdf=getDf(proteinsfile)
    proteinscols=["taxid","entrez GeneID","gene symbol","gene description","protein accession.ver","mrna accession.ver",\
                 "length of protein  listed in column 5","-11) contains data about gene location on the genome",\
                  "starting position of gene in 0-based coordinate",\
                  "end position of the gene in 0-based coordinate","strand","nucleotide gi of genomic sequence where this gene is annotated"]
    proteinsdf.columns=proteinscols
    proteinsdf["organism"]=proteinsdf["taxid"].apply(lambda x:taxdi.get(x))

    protclusdf=getDf(proteinsclusterfile)
    protclustercols=["taxid","entrez GeneID","gene symbol","gene description","protein accession.ver","mrna accession.ver",\
                 "length of protein  listed in column 5","-11) contains data about gene location on the genome",\
                  "starting position of gene in 0-based coordinate",\
                  "end position of the gene in 0-based coordinate","strand","nucleotide gi of genomic sequence where this gene is annotated"]
    protclusdf.columns=proteinscols
    protclusdf["organism"]=protclusdf["taxid"].apply(lambda x:taxdi.get(x))

    return genedf, protclusdf, proteinsdf

def getFasta(opened_file, sequence_name):

    lines = opened_file.readlines()
    seq=str("")
    for i in range(0, len(lines)):
        line = lines[i]
        if line[0] == ">":
            fChr=line.split(" ")[0].split("\n")[0]
            fChr=fChr[1:]
            if fChr == sequence_name:
                s=i
                code=['N','A','C','T','G']
                firstbase=lines[s+1][0]
                while firstbase in code:
                    s=s + 1
                    seq=seq+lines[s]
                    firstbase=lines[s+1][0]

    if len(seq)==0:
        seq=None
    else:
        seq=seq.split("\n")
        seq="".join(seq)

    return seq

def writeFasta(sequence, sequence_name, output_file):
    i=0
    f=open(output_file,'w')
    f.write(">"+str(sequence_name)+"\n")
    while i <= len(sequence):
        f.write(sequence[i:i+60]+"\n")
        i=i+60
    f.close()

def rewriteFasta(sequence, sequence_name, fasta_in, fasta_out):
    f=open(fasta_in, 'r+')
    f2=open(fasta_out,'w')
    lines = f.readlines()
    i=0
    while i < len(lines):
        line = lines[i]
        if line[0] == ">":
            f2.write(line)
            fChr=line.split(" ")[0]
            fChr=fChr[1:]
            if fChr == sequence_name:
                code=['N','A','C','T','G']
                firstbase=lines[i+1][0]
                while firstbase in code:
                    i=i+1
                    firstbase=lines[i][0]
                s=0
                while s <= len(sequence):
                    f2.write(sequence[s:s+60]+"\n")
                    s=s+60
            else:
                i=i+1
        else:
            f2.write(line)
            i=i+1

    f2.close
    f.close

def _get_tool_str(self, tool):
        res = tool['file'] 
        try:
            res += '.' + tool['function']
        except Exception as ex:

            print('Warning - no function defined for tool ' + str(tool))
        res += '\n'
        return res

def get_tool_by_name(self, nme):
        for t in self.lstTools:
            if 'name' in t:
                if t['name'] == nme:
                    return t
            if 'file' in t:
                if t['file'] == nme:
                    return t
        return None

def save(self, fname=''):
        if fname != '':
            with open(fname, 'w') as f:
                for t in self.lstTools:
                    self.verify(t)
                    f.write(self.tool_as_string(t))

def verify(self, tool):
        if os.path.isfile(tool['file']):
            print('Toolbox: program exists = TOK  :: ' + tool['file'])
            return True
        else:
            print('Toolbox: program exists = FAIL :: ' + tool['file'])
            return False

def run(self, tool, args, new_import_path=''):
        if new_import_path != '':
            #print('APPENDING PATH = ', new_import_path)
            sys.path.append(new_import_path)
        
        #if silent == 'N':
        print('main called ' + tool['file'] + '->' + tool['function'] + ' with ', args, ' = ', tool['return'])
        mod = __import__( os.path.basename(tool['file']).split('.')[0]) # for absolute folder names
        # mod = __import__( tool['file'][:-2]) # for aikif folders (doesnt work)
        func = getattr(mod, tool['function'])
        tool['return'] = func(args)
        return tool['return']

def main(**kwargs):
        is_custom_logging = len(self.options.logging_config) > 0
        is_custom_logging = is_custom_logging and os.path.isfile(self.options.logging_config)
        is_custom_logging = is_custom_logging and not self.options.dry_run

        if is_custom_logging:
            Logger.configure_by_file(self.options.logging_config)
        else:
            logging_format = "%(asctime)-15s - %(name)s - %(message)s"
            if self.options.dry_run:
                logging_format = "%(name)s - %(message)s"

            Logger.configure_default(logging_format, self.logging_level)

def validate_document(self, definition):
        initial_document = {}
        try:

            initial_document = Loader.load(definition)
        except RuntimeError as exception:
            self.logger.error(str(exception))
            sys.exit(1)

        document = Validator().validate(initial_document)
        if document is None:

            self.logger.info("Schema validation for '%s' has failed", definition)
            sys.exit(1)

        self.logger.info("Schema validation for '%s' succeeded", definition)
        return document

def run_matrix(self, matrix_definition, document):

        matrix = Matrix(matrix_definition, 'matrix(parallel)' in document)

        process_data = MatrixProcessData()
        process_data.options = self.options
        process_data.pipeline = document['pipeline']
        process_data.model = {} if 'model' not in document else document['model']
        process_data.hooks = Hooks(document)

        return matrix.process(process_data)

def shutdown(self, collector, success):
        if len(self.options.temporary_scripts_path) > 0:
            if os.path.isfile(self.options.temporary_scripts_path):
                self.logger.error("Error: configured script path seems to be a file!")
                # it's ok to leave because called before the collector runs
                sys.exit(1)

            if not os.path.isdir(self.options.temporary_scripts_path):
                os.makedirs(self.options.temporary_scripts_path)

def create_and_run_collector(document, options):
    Transform dictionary of environment variables into Docker -e parameters.

    >>> result = docker_environment({'param1': 'val1', 'param2': 'val2'})
    >>> result in ['-e "param1=val1" -e "param2=val2"', '-e "param2=val2" -e "param1=val1"']
    True
    Retrieves download location for FEH data zip file from hosted json configuration file.

    :return: URL for FEH data file
    :rtype: str
    Check whether updated NRFA data is available.

    :param after_days: Only check if not checked previously since a certain number of days ago
    :type after_days: float
    :return: `True` if update available, `False` if not, `None` if remote location cannot be reached.
    :rtype: bool or None
    Downloads complete station dataset including catchment descriptors and amax records. And saves it into a cache
    folder.
    Save NRFA metadata to local config file using retrieved config data

    :param remote_config: Downloaded JSON data, not a ConfigParser object!
    Return metadata on the NRFA data.

    Returned metadata is a dict with the following elements:

    - `url`: string with NRFA data download URL
    - `version`: string with NRFA version number, e.g. '3.3.4'
    - `published_on`: datetime of data release/publication (only month and year are accurate, rest should be ignored)
    - `downloaded_on`: datetime of last download

    :return: metadata
    :rtype: dict
    Extract all files from downloaded FEH data zip file.
    return a dictionary of statistics about an 
    XML file including size in bytes, num lines,
    number of elements, count by elements
    makes a random xml file mainly for testing the xml_split
    Lists all organisms present in the KEGG database.

    :returns: a dataframe containing one organism per row.

    Finds KEGG database identifiers for a respective organism given example ensembl ids.


    :param organism: an organism as listed in organismsKEGG()
    :param ens_ids: a list of ensenbl ids of the respective organism

    :returns: nothing if no database was found, or a string if a database was found

    Looks up KEGG mappings of KEGG ids to ensembl ids

    :param organism: an organisms as listed in organismsKEGG()
    :param kegg_db: a matching KEGG db as reported in databasesKEGG

    :returns: a Pandas dataframe of with 'KEGGid' and 'ENSid'.

    Uses KEGG to retrieve all ids and respective ecs for a given KEGG organism

    :param organism: an organisms as listed in organismsKEGG()

    :returns: a Pandas dataframe of with 'ec' and 'KEGGid'.

    Uses KEGG to retrieve all ids for a given KEGG organism

    :param organism: an organism as listed in organismsKEGG()

    :returns: a Pandas dataframe of with 'gene_name' and 'KEGGid'.

    Transforms a pandas dataframe with the columns 'ensembl_gene_id','kegg_enzyme'
    to dataframe ready for use in ...

    :param df: a pandas dataframe with the following columns: 'ensembl_gene_id','kegg_enzyme'

    :returns: a pandas dataframe with the following columns: 'ensembl_gene_id','kegg_enzyme'
    Gets all KEGG pathways for an organism

    :param organism: an organism as listed in organismsKEGG()
    :param names_KEGGids: a Pandas dataframe with the columns 'gene_name': and  'KEGGid' as reported from idsKEGG(organism) (or a subset of it).

    :returns df: a Pandas dataframe with 'KEGGid','pathID(1):pathNAME(1)', 'pathID(n):pathNAME(n)'
    :returns paths: a list of retrieved KEGG pathways
    Lists BioMart databases through a RPY2 connection.


    :param host: address of the host server, default='www.ensembl.org'

    :returns: nothing

    Lists BioMart datasets through a RPY2 connection.

    :param database: a database listed in RdatabasesBM()

    :param host: address of the host server, default='www.ensembl.org'

    :returns: nothing

    Lists BioMart filters through a RPY2 connection.

    :param dataset: a dataset listed in RdatasetsBM()
    :param database: a database listed in RdatabasesBM()

    :param host: address of the host server, default='www.ensembl.org'

    :returns: nothing

    Lists BioMart attributes through a RPY2 connection.

    :param dataset: a dataset listed in RdatasetsBM()
    :param database: a database listed in RdatabasesBM()

    :param host: address of the host server, default='www.ensembl.org'

    :returns: nothing

    Get list of applications
        if name in self._dyn_fields:
            raise AttributeError('Field already added to the form.')
        else:
            self._dyn_fields[name] = {'label': label, 'type': field_type,
                                      'args': args, 'kwargs': kwargs}

def add_validator(self, name, validator, *args, **kwargs):
        if name in self._dyn_fields:
            if 'validators' in self._dyn_fields[name]:
                self._dyn_fields[name]['validators'].append(validator)
                self._dyn_fields[name][validator.__name__] = {}
                if args:
                    self._dyn_fields[name][validator.__name__]['args'] = args
                if kwargs:
                    self._dyn_fields[name][validator.__name__]['kwargs'] = kwargs
            else:
                self._dyn_fields[name]['validators'] = []
                self.add_validator(name, validator, *args, **kwargs)
        else:
            raise AttributeError('Field "{0}" does not exist. '
                                 'Did you forget to add it?'.format(name))

def process(self, form, post):

        if not isinstance(form, FormMeta):
            raise TypeError('Given form is not a valid WTForm.')

        re_field_name = re.compile(r'\%([a-zA-Z0-9_]*)\%')

        class F(form):
            pass

        for field, data in post.iteritems():
            if field in F():
                # Skip it if the POST field is one of the standard form fields.
                continue
            else:
                if field in self._dyn_fields:
                    # If we can find the field name directly, it means the field
                    # is not a set so just set the canonical name and go on.
                    field_cname = field
                    # Since we are not in a set, (re)set the current set.
                    current_set_number = None
                elif (field.split('_')[-1].isdigit()
                      and field[:-(len(field.split('_')[-1]))-1] in self._dyn_fields.keys()):
                    # If the field can be split on underscore characters,
                    # the last part contains only digits and the 
                    # everything *but* the last part is found in the
                    # field configuration, we are good to go.
                    # (Cowardly refusing to use regex here).
                    field_cname = field[:-(len(field.split('_')[-1]))-1]
                    # Since we apparently are in a set, remember the
                    # the set number we are at.
                    current_set_number = str(field.split('_')[-1])
                else:
                    # The field did not match to a canonical name
                    # from the fields dictionary or the name
                    # was malformed, throw it out.
                    continue

            # Since the field seems to be a valid one, let us
            # prepare the validator arguments and, if we are in a set
            # replace the %field_name% convention where we find it.
            validators = []
            if 'validators' in self._dyn_fields[field_cname]:
                for validator in self._dyn_fields[field_cname]['validators']:
                    args = []
                    kwargs = {}
                    if 'args' in self._dyn_fields[field_cname]\
                       [validator.__name__]:
                        if not current_set_number:
                            args = self._dyn_fields[field_cname]\
                                   [validator.__name__]['args']
                        else:
                            # If we are currently in a set, append the set number
                            # to all the words that are decorated with %'s within
                            # the arguments.
                            for arg in self._dyn_fields[field_cname]\
                                [validator.__name__]['args']:
                                try:
                                    arg = re_field_name.sub(r'\1'+'_'+current_set_number,
                                                            arg)
                                except:
                                    # The argument does not seem to be regex-able
                                    # Probably not a string, thus we can skip it.
                                    pass
                                args.append(arg)
                    if 'kwargs' in self._dyn_fields[field_cname]\
                       [validator.__name__]:
                        if not current_set_number:
                            kwargs = self._dyn_fields[field_cname]\
                                     [validator.__name__]['kwargs']
                        else:
                            # If we are currently in a set, append the set number
                            # to all the words that are decorated with %'s within
                            # the arguments.
                            for key, arg in self.iteritems(self._dyn_fields[field_cname]\
                                [validator.__name__]['kwargs']):
                                try:
                                    arg = re_field_name.sub(r'\1'+'_'+current_set_number,
                                                            arg)
                                except:
                                    # The argument does not seem to be regex-able
                                    # Probably not a string, thus we can skip it.
                                    pass
                                kwargs[key] = arg
                    # Finally, bind arguments to the validator
                    # and add it to the list
                    validators.append(validator(*args, **kwargs))

            # The field is setup, it is time to add it to the form.
            field_type = self._dyn_fields[field_cname]['type']
            field_label = self._dyn_fields[field_cname]['label']
            field_args = self._dyn_fields[field_cname]['args']
            field_kwargs = self._dyn_fields[field_cname]['kwargs']

            setattr(F, field, field_type(field_label,
                                         validators=validators,
                                         *field_args,
                                         **field_kwargs))

        # Create an instance of the form with the newly
        # created fields and give it back to the caller.
        if self.flask_wtf:
            # Flask WTF overrides the form initialization
            # and already injects the POST variables.
            form = F()
        else:
            form = F(post)
        return form

def GetBEDnarrowPeakgz(URL_or_PATH_TO_file):

    if os.path.isfile(URL_or_PATH_TO_file):
        response=open(URL_or_PATH_TO_file, "r")
        compressedFile = StringIO.StringIO(response.read())
    else:
        response = urllib2.urlopen(URL_or_PATH_TO_file)
        compressedFile = StringIO.StringIO(response.read())
    decompressedFile = gzip.GzipFile(fileobj=compressedFile)
    out=decompressedFile.read().split("\n")
    out=[ s.split("\t") for s in out]
    out=pd.DataFrame(out)
    out.columns=["chrom","chromStart","chromEnd","name","score","strand","signalValue","-log10(pValue)","-log10(qvalue)","peak"]
    out["name"]=out.index.tolist()
    out["name"]="Peak_"+out["name"].astype(str)
    out=out[:-1]
    return out

def dfTObedtool(df):

    df=df.astype(str)
    df=df.drop_duplicates()
    df=df.values.tolist()
    df=["\t".join(s) for s in df ]
    df="\n".join(df)
    df=BedTool(df, from_string=True)
    return df

def configure(**kwargs):
        self.finished = datetime.now()
        self.status = 'failed'
        self.information.update(kwargs)
        self.logger.info("Failed - took %f seconds.", self.duration())
        self.update_report_collector(int(time.mktime(self.finished.timetuple())))

def update_report_collector(self, timestamp):
    test function.
    creates a SQL loader script to load a text file into a database
    and then executes it.
    Note that src_file is 

    Args:
        iterable: An async iterable.

        default: An optional default value to return if the iterable is empty.

    Return:
        The next value of the iterable.

    Raises:
        TypeError: The iterable given is not async.

    This function will return the next value form an async iterable. If the
    iterable is empty the StopAsyncIteration will be propogated. However, if

    a default value is given as a second argument the exception is silenced and

    the default value is returned instead.
    if times is None:

        return AsyncIterWrapper(sync_itertools.repeat(obj))

    return AsyncIterWrapper(sync_itertools.repeat(obj, times))

def _async_callable(func):
        return func(*args, **kwargs)


    return _async_def_wrapper

def tee(iterable, n=2):
    tees = tuple(AsyncTeeIterable(iterable) for _ in range(n))
    for tee in tees:

        tee._siblings = tees

    return tees

def _on_change(self, obj, old, value, **kwargs):
        kwargs['property'] = self
        obj.emit(self.name, obj, value, old=old, **kwargs)

def parse_str(self, s):
        self.object = self.parsed_class()
        in_section = None  # Holds name of FEH file section while traversing through file.
        for line in s.split('\n'):
            if line.lower().startswith('[end]'):
                # Leave section
                in_section = None
            elif line.startswith('['):
                # Enter section, sanitise `[Section Name]` to `section_name`
                in_section = line.strip().strip('[]').lower().replace(' ', '_')
            elif in_section:
                try:
                    # Call method `_section_section_name(line)`
                    getattr(self, '_section_' + in_section)(line.strip())
                except AttributeError:
                    pass  # Skip unsupported section
        return self.object

def parse(self, file_name):
        self.object = self.parsed_class()
        with open(file_name, encoding='utf-8') as f:
            self.parse_str(f.read())
        return self.object

def has_next(self):
        try:
            next_item = self.paginator.object_list[self.paginator.per_page]
        except IndexError:
            return False
        return True

def parse_miss_cann(node, m, c):
    if node[2]:
        m1 = node[0]
        m2 = m-node[0]
        c1 = node[1]
        c2 = c-node[1]
    else:
        m1=m-node[0]
        m2=node[0]
        c1=c-node[1]
        c2=node[1]
    
    return m1, c1, m2, c2

def solve(m,c):
    G={ (m,c,1):[] }
    frontier=[ (m,c,1) ]  # 1 as boat starts on left bank 
    while len(frontier) > 0:
        hold=list(frontier)
        for node in hold:
            newnode=[]
            frontier.remove(node)
            newnode.extend(pick_next_boat_trip(node, m,c, frontier))
            for neighbor in newnode:
                if neighbor not in G:
                    G[node].append(neighbor)
                    G[neighbor]=[node]
                    frontier.append(neighbor)
    return mod_plan.find_path_BFS(G,(m,c,1),(0,0,0))

def create_script_fact(self):
        self.ddl_text += '---------------------------------------------\n'
        self.ddl_text += '-- CREATE Fact Table - ' + self.fact_table + '\n'
        self.ddl_text += '---------------------------------------------\n'
        self.ddl_text += 'DROP TABLE ' + self.fact_table + ' CASCADE CONSTRAINTS;\n'
        self.ddl_text += 'CREATE TABLE ' + self.fact_table + ' (\n'
        self.ddl_text += ' '.join([col + ' VARCHAR2(200), \n' for col in self.col_list])
        self.ddl_text += ' ' + self.date_updated_col + ' DATE \n' # + src_table + '; \n'
        self.ddl_text += ');\n'

def create_script_staging_table(self, output_table, col_list):
        self.ddl_text += '---------------------------------------------\n'
        self.ddl_text += '-- CREATE Staging Table - ' + output_table + '\n'
        self.ddl_text += '---------------------------------------------\n'
        self.ddl_text += 'DROP TABLE ' + output_table + ' CASCADE CONSTRAINTS;\n'
        self.ddl_text += 'CREATE TABLE ' + output_table + ' (\n  '
        self.ddl_text += '  '.join([col + ' VARCHAR2(200), \n' for col in col_list])
        self.ddl_text += '  ' + self.date_updated_col + ' DATE \n' # + src_table + '; \n'
        self.ddl_text += ');\n'

def distinct_values(t_old, t_new):   
    res = []
    res.append([' -- NOT IN check -- '])
    for new_col in t_new.header:
        dist_new = t_new.get_distinct_values_from_cols([new_col])
        #print('NEW Distinct values for ' + new_col + ' = ' + str(dist_new))
        for old_col in t_old.header:
            if old_col == new_col:
                dist_old = t_old.get_distinct_values_from_cols([old_col])
                #print('OLD Distinct values for ' + old_col + ' = ' + str(dist_old))
                
                # Now compare the old and new values to see what is different
                not_in_new = [x for x in dist_old[0] if x not in dist_new[0]]
                if not_in_new != []:
                    #print(old_col + ' not_in_new = ' , not_in_new)
                    res.append(['Not in New', old_col, not_in_new])
                    
                not_in_old = [x for x in dist_new[0] if x not in dist_old[0]]
                if not_in_old != []:
                    #print(new_col + ' not_in_old = ' , not_in_old)
                    res.append(['Not in Old', new_col, not_in_old])
                    
                    
    
    return sorted(res)

def aikif_web_menu(cur=''):
    This generates the research document based on the results of 
    the various programs and includes RST imports for introduction
    and summary
        returns a list of records containing text
        return Schema({
            'stage': And(str, len),
            'timestamp': int,
            'status': And(str, lambda s: s in ['started', 'succeeded', 'failed']),
            # optional matrix

            Optional('matrix', default='default'): And(str, len),
            # optional information

            Optional('information', default={}): {
                Optional(Regex(r'([a-z][_a-z]*)')): object
            }
        })

def schema_event_items():
        return Schema({
            'stage': And(str, len),
            'status': And(str, lambda s: s in ['started', 'succeeded', 'failed']),

            Optional('events', default=[]): And(len, [CollectorStage.schema_event_items()])
        })

def add(self, timestamp, information):
        try:
            item = Schema(CollectorStage.schema_event_items()).validate({
                'timestamp': timestamp, 'information': information
            })
            self.events.append(item)
        except SchemaError as exception:
            Logger.get_logger(__name__).error(exception)
            raise RuntimeError(str(exception))

def duration(self):
        duration = 0.0
        if len(self.events) > 0:
            first = datetime.fromtimestamp(self.events[0]['timestamp'])
            last = datetime.fromtimestamp(self.events[-1]['timestamp'])
            duration = (last - first).total_seconds()
        return duration

def count_stages(self, matrix_name):
        return len(self.data[matrix_name]) if matrix_name in self.data else 0

def get_stage(self, matrix_name, stage_name):
        found_stage = None
        if matrix_name in self.data:
            result = Select(self.data[matrix_name]).where(
                lambda entry: entry.stage == stage_name).build()
            found_stage = result[0] if len(result) > 0 else None
        return found_stage

def get_duration(self, matrix_name):
        duration = 0.0
        if matrix_name in self.data:
            duration = sum([stage.duration() for stage in self.data[matrix_name]])
        return duration

def update(self, item):
        if item.matrix not in self.data:
            self.data[item.matrix] = []

        result = Select(self.data[item.matrix]).where(
            lambda entry: entry.stage == item.stage).build()

        if len(result) > 0:
            stage = result[0]
            stage.status = item.status
            stage.add(item.timestamp, item.information)
        else:
            stage = CollectorStage(stage=item.stage, status=item.status)
            stage.add(item.timestamp, item.information)
            self.data[item.matrix].append(stage)

def run(self):
    reads a saved text file to list
        reads a saved grid file and paints it on the canvas
        draw a cell as position row, col containing val
        paint an agent trail as ONE pixel to allow for multiple agent
        trails to be seen in the same cell
        gets a colour for agent 0 - 9
    create a list of people with randomly generated names and stats
        if self.data.hooks and len(self.data.hooks.cleanup) > 0:
            env = self.data.env_list[0].copy()
            env.update({'PIPELINE_RESULT': 'SUCCESS', 'PIPELINE_SHELL_EXIT_CODE': '0'})
            config = ShellConfig(script=self.data.hooks.cleanup, model=self.model,
                                 env=env, dry_run=self.options.dry_run,
                                 debug=self.options.debug, strict=self.options.strict,
                                 temporary_scripts_path=self.options.temporary_scripts_path)
            cleanup_shell = Bash(config)
            for line in cleanup_shell.process():
                yield line

def process(self, pipeline):
        Top level function to process the command, mainly
        depending on mode.

        This should work by using the function name defined
        in all_commamnds
        Enter add mode - all text entered now will be 
        processed as adding information until cancelled
        search and query the AIKIF
        if not self.__integrity_check:
            if not self.__appid:

                raise Exception('U2F_APPID was not defined! Please define it in configuration file.')

            if self.__facets_enabled and not len(self.__facets_list):

            # Injection
            

            undefined_message = 'U2F {name} handler is not defined! Please import {name} through {method}!'

            if not self.__get_u2f_devices:

                raise Exception(undefined_message.format(name='Read', method='@u2f.read'))

            if not self.__save_u2f_devices:

                raise Exception(undefined_message.format(name='Save', method='@u2f.save'))


            if not self.__call_success_enroll:

                raise Exception(undefined_message.format(name='enroll onSuccess', method='@u2f.enroll_on_success'))

            if not self.__call_success_sign:

                raise Exception(undefined_message.format(name='sign onSuccess', method='@u2f.sign_on_success'))

            self.__integrity_check = True

        return True

def devices(self):
        self.verify_integrity()

        if self.__facets_enabled:
            data = json.dumps({
                'trustedFacets' : [{
                    'version': { 'major': 1, 'minor' : 0 },
                    'ids': self.__facets_list
                }]
            }, sort_keys=True, indent=2, separators=(',', ': '))

            mime = 'application/fido.trusted-apps+json'
            resp = Response(data, mimetype=mime)

            return resp, 200
        else:
            return jsonify({}), 404

def get_enroll(self):

        seed = session.pop('_u2f_enroll_')
        try:
            new_device, cert = complete_register(seed, response, self.__facets_list)
        except Exception as e:
            if self.__call_fail_enroll:
                self.__call_fail_enroll(e)

            return {
                'status' : 'failed', 
                'error'  : 'Invalid key handle!'
            }

        finally:
            pass

        

        devices = self.__get_u2f_devices()

        # Setting new device counter to 0
        new_device['counter'] = 0
        new_device['index']   = 0

        for device in devices:
            if new_device['index'] <= device['index']:
                new_device['index'] = device['index'] + 1

        devices.append(new_device)

        self.__save_u2f_devices(devices)
        
        self.__call_success_enroll()

        return {'status': 'ok', 'message': 'Successfully enrolled new U2F device!'}

def get_signature_challenge(self):
        
        devices = self.__get_u2f_devices()

        for i in range(len(devices)):
            if devices[i]['keyHandle'] == request['id']:
                del devices[i]
                self.__save_u2f_devices(devices)
                
                return {
                    'status'  : 'ok', 
                    'message' : 'Successfully deleted your device!'
                }

        return {
            'status' : 'failed', 
            'error'  : 'No device with such an id been found!'
        }

def verify_counter(self, signature, counter):
        Validate data against the schema.

        Args:
            data(dict): data structure to validate.

        Returns:

            dict: data as provided and defaults where defined in schema.
        result = None
        if isinstance(node, ScalarNode):
            result = Loader.include_file(self.construct_scalar(node))
        else:
            raise RuntimeError("Not supported !include on type %s" % type(node))
        return result

def load(filename):
        transposes rows and columns
        convert list to key value pairs
        
        This should also create unique id's to allow for any
        dataset to be transposed, and then later manipulated
        r1c1,r1c2,r1c3
        r2c1,r2c2,r2c3
        
        should be converted to 
        ID  COLNUM  VAL
        r1c1, 
        This is the reverse of data_to_links and takes a links table and 
        generates a data table as follows
        Input Table                         Output Table
        Cat_Name,CAT_val,Person_a,person_b  NAME,Location  
        Location,Perth,John,Fred            John,Perth
        Location,Perth,John,Cindy           Cindy,Perth
        Location,Perth,Fred,Cindy           Fred,Perth
        try each strategy with different amounts
    print('Loading ' + fname + ' to redis')
    r = redis.StrictRedis(host = '127.0.0.1', port = 6379, db = 0);
    with open(fname, 'r') as f:
        for line_num, row in enumerate(f): 
            if row.strip('') != '':
                if line_num < 100000000:
                    l_key, l_val = parse_n3(row, 'csv')
                    if line_num % 1000 == 0: 
                        print('loading line #', line_num, 'key=', l_key, ' = ', l_val)
                    if l_key != '':
                        r.set(l_key, l_val)

def parse_n3(row, src='csv'):
    if row.strip() == '':
        return '',''
    l_root = 'opencyc'
    key = ''
    val = ''
    if src == 'csv': 
        cols = row.split(',')
        if len(cols) < 3:
            #print('PARSE ISSUE : ', row)
            return '',''
        key = ''
        val = ''
        key = l_root + ':' + cols[1].strip('"').strip() + ':' + cols[2].strip('"').strip()
        try:
            val = cols[3].strip('"').strip()
        except Exception:
            val = "Error parsing " + row
    elif src == 'n3':
        pass
    return key, val

def summarise_file_as_html(fname):
    txt = '<H1>' + fname + '</H1>'
    num_lines = 0
    print('Reading OpenCyc file - ', fname)
    with open(ip_folder + os.sep + fname, 'r') as f:
        txt += '<PRE>'
        for line in f: 
            if line.strip() != '':
                num_lines += 1
                if num_lines < 80:
                    txt += str(num_lines) + ': ' + escape_html(line) + ''
        txt += '</PRE>'
        txt += 'Total lines = ' + str(num_lines) + '<BR><BR>'
    
    return txt

def main():
    iterations  = 9     # how many simulations to run
    years       = 3    # how many times to run each simulation
    width       = 22     # grid height
    height      = 78     # grid width
    time_delay  = 0.03   # delay when printing on screen
    lg = mod_log.Log('test')
    lg.record_process('Game of Life', 'game_of_life_console.py')
    for _ in range(iterations):
        s,e = run_game_of_life(years, width, height, time_delay, 'N') 
        lg.record_result("Started with " +  str(s) + " cells and ended with " + str(e) + " cells")

def run_game_of_life(years, width, height, time_delay, silent="N"):
    lfe = mod_grid.GameOfLife(width, height, ['.', 'x'], 1)
    set_random_starting_grid(lfe)
    lg.record_source(lfe, 'game_of_life_console.py')
    print(lfe)
    start_cells = lfe.count_filled_positions()
    for ndx, dummy_idx in enumerate(range(years)):
        lfe.update_gol()
        if silent == "N":
            print_there(1,1, "Game of Life - Iteration # " + str(ndx))
            print_there(1, 2, lfe)
            time.sleep(time_delay)
    end_cells = lfe.count_filled_positions()
    return start_cells, end_cells

def print_there(x, y, text):
    sys.stdout.write("\x1b7\x1b[%d;%df%s\x1b8" % (x, y, text))
    sys.stdout.flush()

def identify_col_pos(txt):


	res = []

	#res.append(0)

	lines = txt.split('\n')

	prev_ch = ''

	for col_pos, ch in enumerate(lines[0]):

		if _is_white_space(ch) is False and _is_white_space(prev_ch) is True:

			res.append(col_pos)

		prev_ch = ch

	res.append(col_pos)

	return res

def load_tbl_from_csv(fname):


	import csv



	rows_to_load = []



	with open(fname, 'r', encoding='cp1252', errors='ignore') as csvfile:

		csvreader = csv.reader(csvfile, delimiter = ',' )



		reader = csv.reader(csvfile)



		rows_to_load = list(reader)



	return rows_to_load

def _get_dict_char_count(txt):


	dct = {}

	for letter in txt:

		if letter in dct:

			dct[letter] += 1

		else:

			dct[letter] = 1

	return dct

def creator(entry, config):
        # writing Dockerfile
        dockerfile = render(config.script, model=config.model, env=config.env,
                            variables=config.variables, item=config.item)
        filename = "dockerfile.dry.run.see.comment"

        if not config.dry_run:
            temp = tempfile.NamedTemporaryFile(
                prefix="dockerfile-", mode='w+t', delete=False)
            temp.writelines(dockerfile)
            temp.close()
            filename = temp.name
            dockerfile = ''

        # rendering the Bash script for generating the Docker image
        name = entry['name'] + "-%s" % os.getpid() if entry['unique'] else entry['name']
        tag = render(entry['tag'], model=config.model, env=config.env, item=config.item)
        template_file = os.path.join(os.path.dirname(__file__), 'templates/docker-image.sh.j2')

        with open(template_file) as handle:
            template = handle.read()
            config.script = render(template, name=name, tag=tag,
                                   dockerfile_content=dockerfile,
                                   dockerfile_filename=filename)

        return Image(config)

def stdout_redirector():
    old_stdout = sys.stdout
    sys.stdout = Stream()
    try:
        yield sys.stdout
    finally:
        sys.stdout.close()
        sys.stdout = old_stdout

def write_temporary_file(content, prefix='', suffix=''):
    temp = tempfile.NamedTemporaryFile(prefix=prefix, suffix=suffix, mode='w+t', delete=False)
    temp.writelines(content)
    temp.close()
    return temp.name

def print_new(ctx, name, migration_type):
        Starts an agent with standard logging
        set coords of agent in an arbitrary world
    Load catchment object from a ``.CD3`` or ``.xml`` file.

    If there is also a corresponding ``.AM`` file (annual maximum flow data) or
    a ``.PT`` file (peaks over threshold data) in the same folder as the CD3 file, these datasets will also be loaded.

    :param file_path: Location of CD3 or xml file
    :type file_path: str
    :return: Catchment object with the :attr:`amax_records` and :attr:`pot_dataset` attributes set (if data available).
    :rtype: :class:`.entities.Catchment`
    :param incl_pot: Whether to load the POT (peaks-over-threshold) data. Default: ``True``.
    :type incl_pot: bool
    Load catchment object into the database.

    A catchment/station number (:attr:`catchment.id`) must be provided. If :attr:`method` is set to `update`, any
    existing catchment in the database with the same catchment number will be updated.

    :param catchment: New catchment object to replace any existing catchment in the database
    :type catchment: :class:`.entities.Catchment`
    :param session: Database session to use, typically `floodestimation.db.Session()`
    :type session: :class:`sqlalchemy.orm.session.Session`
    :param method: - ``create``: only new catchments will be loaded, it must not already exist in the database.
                   - ``update``: any existing catchment in the database will be updated. Otherwise it will be created.
    :type method: str
    :param autocommit: Whether to commit the database session immediately. Default: ``False``.
    :type autocommit: bool
    Add catchments from a user folder to the database.

    The user folder is specified in the ``config.ini`` file like this::

        [import]
        folder = path/to/import/folder

    If this configuration key does not exist this will be silently ignored.

    :param session: database session to use, typically `floodestimation.db.Session()`
    :type session: :class:`sqlalchemy.orm.session.Session`
    :param method: - ``create``: only new catchments will be loaded, it must not already exist in the database.
                   - ``update``: any existing catchment in the database will be updated. Otherwise it will be created.
    :type method: str
    :param autocommit: Whether to commit the database session immediately. Default: ``False``.
    :type autocommit: bool
    sends the text 'txt' to the window handle hwnd using SendMessage
    start an app
    use shell to bring the application with caption to front

        Return a list of catchments sorted by hydrological similarity defined by `similarity_distance_function`

        :param subject_catchment: subject catchment to find similar catchments for
        :type subject_catchment: :class:`floodestimation.entities.Catchment`
        :param similarity_dist_function: a method returning a similarity distance measure with 2 arguments, both
                                         :class:`floodestimation.entities.Catchment` objects
        :param include_subject_catchment: - `auto`: include subject catchment if suitable for pooling and if urbext < 0.03
                                          - `force`: always include subject catchment having at least 10 years of data
                                          - `exclude`: do not include the subject catchment
        :type include_subject_catchment: str
        :return: list of catchments sorted by similarity
        :type: list of :class:`floodestimation.entities.Catchment`
    Reads and parses a sam file.

    :param SAMfile: /path/to/file.sam
    :param header: logical, if True, reads the header information

    :returns: a pandas dataframe with the respective SAM columns: 'QNAME','FLAG','RNAME','POS','MAPQ','CIGAR','RNEXT','PNEXT','TLEN','SEQ','QUAL' and a list of the headers if header=True

    Explains a SAM flag.

    :param x: flag

    :returns: complete SAM flag explanaition
        returns a string representation of the bias details
        read the bias file based on the short_filename
        and return as a dictionary
    returns the home folder and program root depending on OS
    read a simple text file from a private location to get
    username and password
    module intended to be imported in most AIKIF utils
    to manage folder paths, user settings, etc.
    Modify the parameters at the top of this file to suit
    Selectes motifs from a meme file based on the number of sites.

    :param memeFile: MEME file to be read
    :param outFile: MEME file to be written
    :param minSites: minimum number of sites each motif needs to have to be valid

    :returns: nothing
        reads the file and cleans into standard text ready for parsing

        Restore the default configuration and remove the user's config file.
        Write data to user config file.
    with open(filename) as f:
        data = json.load(f)
    headers = [_create_puremagic(x) for x in data['headers']]
    footers = [_create_puremagic(x) for x in data['footers']]
    return headers, footers

def _max_lengths():
    results = []
    for match in matches:
        con = (0.8 if len(match.extension) > 9 else
               float("0.{0}".format(len(match.extension))))
        if ext == match.extension:
            con = 0.9
        results.append(
            PureMagicWithConfidence(confidence=con, **match._asdict()))
    return sorted(results, key=lambda x: x.confidence, reverse=True)

def _identify_all(header, footer, ext=None):
    if not header:
        raise ValueError("Input was empty")
    info = _identify_all(header, footer, ext)[0]
    if mime:
        return info.mime_type
    return info.extension if not \
        isinstance(info.extension, list) else info[0].extension

def _file_details(filename):

    :param filename: string of the filename
    :return: the extension off the end (empty string if it can't find one)
    off magic number and will return the file extension.
    If mime is True it will return the mime type instead.

    :param filename: path to file
    :param mime: Return mime, not extension
    :return: guessed extension or mime
    off magic number and will return the file extension.
    If mime is True it will return the mime type instead.
    If filename is provided it will be used in the computation.

    :param string: string representation to check
    :param mime: Return mime, not extension
    :param filename: original filename
    :return: guessed extension or mime
    Returns a field of choice from the attribute column of the GTF

    :param field: field to be retrieved
    :returns: a Pandas dataframe with one columns containing the field of choice

    List the type of attributes in a the attribute section of a GTF file

    :param inGTF: GTF dataframe to be analysed
    :returns: a list of attributes present in the attribute section

    Reads an extracts all attributes in the attributes section of a GTF and constructs a new dataframe wiht one collumn per attribute instead of the attributes column

    :param inGTF: GTF dataframe to be parsed
    :returns: a dataframe of the orignal input GTF with attributes parsed.

    Write a GTF dataframe into a file

    :param inGTF: GTF dataframe to be written. It should either have 9 columns with the last one being the "attributes" section or more than 9 columns where all columns after the 8th will be colapsed into one.
    :param file_path: path/to/the/file.gtf
    :returns: nothing
    Transform a GTF dataframe into a bed dataframe

    :param inGTF: GTF dataframe for transformation
    :param name: field of the GTF data frame to be use for the bed 'name' positon

    returns: a bed dataframe with the corresponding bed fiels: 'chrom','chromStart','chromEnd','name','score','strand'
    Gets all positions of all bases in an exon

    :param df: a Pandas dataframe with 'start','end', and 'strand' information for each entry.
                df must contain 'seqname','feature','start','end','strand','frame','gene_id',
                'transcript_id','exon_id','exon_number']
    :param feature: feature upon wich to generate the map, eg. 'exon' or 'transcript'

    :returns: a string with the comma separated positions of all bases in the exon
    Maps a genome position to transcript positon"

    :param df: a Pandas dataframe
    :param field: the head of the column containing the genomic position
    :param dic: a dictionary containing for each transcript the respective bases eg. {ENST23923910:'234,235,236,1021,..'}
    :param refCol: header of the reference column with IDs, eg. 'transcript_id'

    :returns: position on transcript
    having problems with urllib on a specific site so trying requests
    exp = pd.read_table(filename)
    expected_columns = {'File', 'Exists', 'Size', 'Data format', 'Parameter group', 'Experiment', 'Fraction'}
    found_columns = set(exp.columns)
    if len(expected_columns - found_columns) > 0:
        message = '\n'.join(['The raw files table has the wrong format!',
            'It should contain columns:',
            ', '.join(sorted(expected_columns)),
            'Found columns:',
            ', '.join(sorted(found_columns))])
        raise ValueError(message)
    exp['Raw file'] = exp['File'].apply(path.basename).apply(path.splitext).str.get(0)
    exp['Experiment'] = exp['Experiment'].astype(str)
    return exp

def add_method(self, m, **kwargs):
        if isinstance(m, types.FunctionType):
            self['function', id(m)] = m
        else:
            f, obj = get_method_vars(m)
            wrkey = (f, id(obj))
            self[wrkey] = obj

def del_method(self, m):
        if isinstance(m, types.FunctionType) and not iscoroutinefunction(m):
            wrkey = ('function', id(m))
        else:
            f, obj = get_method_vars(m)
            wrkey = (f, id(obj))
        if wrkey in self:
            del self[wrkey]

def del_instance(self, obj):
        to_remove = set()
        for wrkey, _obj in self.iter_instances():
            if obj is _obj:
                to_remove.add(wrkey)
        for wrkey in to_remove:
            del self[wrkey]

def iter_instances(self):
        for wrkey in set(self.keys()):
            obj = self.get(wrkey)
            if obj is None:
                continue
            yield wrkey, obj

def iter_methods(self):
        for wrkey, obj in self.iter_instances():
            f, obj_id = wrkey
            if f == 'function':
                yield self[wrkey]
            else:
                yield getattr(obj, f.__name__)

def load_data_subject_areas(subject_file):
    lst = []
    if os.path.exists(subject_file):
        with open(subject_file, 'r') as f:
            for line in f:
                lst.append(line.strip())
    else:
        print('MISSING DATA FILE (subject_file) ' , subject_file)
        print('update your config.py or config.txt')
    return lst

def check_ontology(fname):    
    with open(fname, 'r') as stream:
        y = yaml.safe_load(stream)
    import pprint
    pprint.pprint(y)

def find_type(self, txt):
        searchString = txt.upper()
        match = 'Unknown'
        for i in self.lst_type:
            if searchString in i:
                match = i
        return match

def get_full_filename(self, dataType, subjectArea):
        return dataPath + os.sep + 'core' + os.sep + dataType + '_' + subjectArea + '.CSV'

def load_plan(self, fname):    
        adds a constraint for the plan
        calculates basic stats on the MapRule elements of the maps
        to give a quick overview.
        save the rules to file after web updates or program changes 
        uses the MapRule 'm' to run through the 'dict'
        and extract data based on the rule
        uses type to format the raw information to a dictionary
        usable by the mapper
        takes a string and parses via NLP, ready for mapping
        process the file according to the mapping rules.
        The cols list must match the columns in the filename
        reads the data_filename into a matrix and calls the main
        function '' to generate a  .rule file based on the data in the map
        

        For all datafiles mapped, there exists a .rule file to define it
        
        loops until exit command given
        takes a question and returns the best answer based on known skills
    txt = '<H2>' + fname + '</H2>'
    print (fname)
    #try:
    txt += web.read_csv_to_html_table(fname, 'Y')  # it is ok to use a table for actual table data
    #except:
    #	txt += '<H2>ERROR - cant read file</H2>'
    #txt += web.read_csv_to_html_list(fname)  # only use this for single column lists
    
    txt += '</div>\n'
    return txt

def managed_process(process):
        Get path for temporary scripts.

        Returns:
            str: path for temporary scripts or None if not set
        Create a temporary, executable bash file.

        It also does render given script (string) with the model and
        the provided environment variables and optional also an item
        when using the B{with} field.

        Args:
            script (str): either pather and filename or Bash code.

        Returns:
            str: path and filename of a temporary file.
        options = ''
        if self.config.debug:
            options += "set -x\n"
        if self.config.strict:
            options += "set -euo pipefail\n"
        return options

def process_file(self, filename):
        try:
            key = self._fd_to_key.pop(self._fileobj_lookup(fileobj))
        except KeyError:
            raise KeyError("{0!r} is not registered".format(fileobj))

        # Getting the fileno of a closed socket on Windows errors with EBADF.
        except socket.error as err:
            if err.errno != errno.EBADF:
                raise
            else:
                for key in self._fd_to_key.values():
                    if key.fileobj is fileobj:
                        self._fd_to_key.pop(key.fd)
                        break
                else:
                    raise KeyError("{0!r} is not registered".format(fileobj))
        return key

def prepare(self):
        try:
            assert(type(self.sender) is Channel)
            assert(type(self.receiver) is Channel)
            return True
        except:
            return False

def send(self):
        #print('sending message to ' + self.receiver)
        if self.prepare():
            ## TODO - send message via library
            print('sending message')
            lg.record_process('comms.py', 'Sending message ' + self.title)

            return True
        else:
            return False

def buildIndex(ipFile, ndxFile, append='Y', silent='N', useShortFileName='Y'):
    if silent == 'N':
        pass
    if append == 'N':
        try:
            os.remove(ndxFile)
        except Exception as ex:
            print('file already deleted - ignore' + str(ex))
            
    delims = [',', chr(31), '', '$', '&', '"', '%', '/', '\\', '.', ';', ':', '!', '?', '-', '_', ' ', '\n', '*', '\'', '(', ')', '[', ']', '{', '}']
    # 1st pass - index the ontologies, including 2 depths up (later - TODO)
    #buildIndex(ipFile, ndxFile, ' ', 1, 'Y')

    # 2nd pass - use ALL delims to catch each word as part of hyphenated - eg AI Build py
    totWords, totLines, uniqueWords = getWordList(ipFile, delims)
    
    AppendIndexDictionaryToFile(uniqueWords, ndxFile, ipFile, useShortFileName)
    if silent == 'N':
        print(format_op_row(ipFile, totLines, totWords, uniqueWords))
   
        show('uniqueWords', uniqueWords, 5)
    DisplayIndexAsDictionary(uniqueWords)

def format_op_row(ipFile, totLines, totWords, uniqueWords):
    txt = os.path.basename(ipFile).ljust(36) + ' '
    txt += str(totLines).rjust(7) + ' '
    txt += str(totWords).rjust(7) + ' '
    txt += str(len(uniqueWords)).rjust(7) + ' '
    return txt

def format_op_hdr():
    txt = 'Base Filename'.ljust(36) + ' '
    txt += 'Lines'.rjust(7) + ' '
    txt += 'Words'.rjust(7) + '  '
    txt += 'Unique'.ljust(8) + ''
    return txt

def AppendIndexDictionaryToFile(uniqueWords, ndxFile, ipFile, useShortFileName='Y'):
    if useShortFileName == 'Y':
        f = os.path.basename(ipFile)
    else:
        f = ipFile
    with open(ndxFile, "a", encoding='utf-8', errors='replace') as ndx:
        word_keys = uniqueWords.keys()
        #uniqueWords.sort()
        for word in sorted(word_keys):
            if word != '':
                line_nums = uniqueWords[word]
                ndx.write(f + ', ' + word + ', ')
                for line_num in line_nums:
                    ndx.write(str(line_num))
                ndx.write('\n')

def DisplayIndexAsDictionary(word_occurrences):
    word_keys = word_occurrences.keys()
    for num, word in enumerate(word_keys):
        line_nums = word_occurrences[word]
        print(word + " ")
        if num > 3:
            break

def show(title, lst, full=-1):
    txt = title + ' (' + str(len(lst)) + ') items :\n '
    num = 0
    for i in lst:
        if full == -1 or num < full:
            if type(i) is str:
                txt = txt + i + ',\n '
            else:
                txt = txt + i + ', ['
                for j in i:
                    txt = txt + j + ', '
                txt = txt + ']\n'
        num = num + 1
    try:
        print(txt)
    except Exception as ex:
        print('index.show() - cant print line, error ' + str(ex))

def getWordList(ipFile, delim):
    indexedWords = {}
    totWords = 0
    totLines = 0
    with codecs.open(ipFile, "r",encoding='utf-8', errors='replace') as f:
        for line in f:
            totLines = totLines + 1
            words = multi_split(line, delim)
            totWords = totWords + len(words)
            for word in words:
                cleanedWord = word.lower().strip()
                if cleanedWord not in indexedWords:
                    indexedWords[cleanedWord] =  str(totLines)
                else:
                    indexedWords[cleanedWord] = indexedWords[cleanedWord] + ' ' + str(totLines)
    return totWords, totLines, indexedWords

def multi_split(txt, delims):
    res = [txt]
    for delimChar in delims:
        txt, res = res, []
        for word in txt:
            if len(word) > 1:
                res += word.split(delimChar)
    return res

def creator(entry, config):
    converts and unknown type to string for display purposes.
    
        For a log session you can add as many watch points 
        which are used in the aggregation and extraction of
        key things that happen.
        Each watch point has a rating (up to you and can range 
        from success to total failure and an importance for 
        finer control of display
        calculates a rough guess of runtime based on product of parameters 
        converts seconds to a string in terms of 
        seconds -> years to show complexity of algorithm
        logs an entry to fname along with standard date and user details
        function to collect raw data from the web and hard drive
        Examples - new source file for ontologies, email contacts list, folder for xmas photos
        record the command passed - this is usually the name of the program
        being run or task being run
        record the output of the command. Records the result, can have 
        multiple results, so will need to work out a consistent way to aggregate this
        read a logfile and return entries for a program
        takes the logfiles and produces an event summary matrix
            date        command result  process source
            20140421    9       40      178     9
            20140423    0       0       6       0
            20140424    19      1       47      19
            20140425    24      0       117     24
            20140426    16      0       83      16
            20140427    1       0       6       1
            20140429    0       0       0       4

        reads a logfile and returns a dictionary by date
        showing the count of log entries
        provides a mapping from the CSV file to the 
        aikif data structures.
    Returns the string of a variable name.
    Performs a blast query online.

    As in https://ncbi.github.io/blast-cloud/

    :param query: Search query. Allowed values: Accession, GI, or FASTA.
    :param database: BLAST database. Allowed values: nt, nr, refseq_rna, refseq_protein, swissprot, pdbaa, pdbnt
    :param program: BLAST program. Allowed values:  blastn, megablast, blastp, blastx, tblastn, tblastx
    :param filter: Low complexity filtering. Allowed values: F to disable. T or L to enable. Prepend "m" for mask at lookup (e.g., mL)

    :param format_type: Report type. Allowed values: HTML, Text, XML, XML2, JSON2, or Tabular. HTML is the default.
    :param expect: Expect value. Allowed values: Number greater than zero.
    :param nucl_reward: Reward for matching bases (BLASTN and megaBLAST). Allowed values: Integer greater than zero.
    :param nucl_penalty: Cost for mismatched bases (BLASTN and megaBLAST). Allowed values: Integer less than zero.
    :param gapcosts: Gap existence and extension costs. Allowed values: Pair of positive integers separated by a space such as "11 1".
    :param matrix: Scoring matrix name. Allowed values: One of BLOSUM45, BLOSUM50, BLOSUM62, BLOSUM80, BLOSUM90, PAM250, PAM30 or PAM70. Default: BLOSUM62 for all applicable programs.
    :param hitlist_size: Number of databases sequences to keep. Allowed values: Integer greater than zero.
    :param descriptions: Number of descriptions to print (applies to HTML and Text). Allowed values: Integer greater than zero.
    :param alignments: Number of alignments to print (applies to HTML and Text). Allowed values: Integer greater than zero.
    :param ncbi_gi: Show NCBI GIs in report. Allowed values: T or F.

    :param threshold: Neighboring score for initial words. Allowed values: Positive integer (BLASTP default is 11). Does not apply to BLASTN or MegaBLAST).
    :param word_size: Size of word for initial matches. Allowed values: Positive integer.
    :param composition_based_statistics: Composition based statistics algorithm to use. Allowed values: One of 0, 1, 2, or 3. See comp_based_stats command line option in the BLAST+ user manual for details.
    :param organism: an organism as in https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome
    :param others: here you can add other parameters as seen in a blast bookmarked page. Define you query in https://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn&PAGE_TYPE=BlastSearch&LINK_LOC=blasthome

            Once your query is defined click on "Bookmark" on right upper side of the page. You can copy fragments of the URL

            which define the query. Eg. For organism "Homo sapiens (taxid:9606)" you will see the string "EQ_MENU=Homo%20sapiens%20%28taxid%3A9606%29" - this is
            the string you can use here in others.

    :param num_threads: Number of virtual CPUs to use. Allowed values: Integer greater than zero (default is 1). Supported only on the cloud.
    :param verbose: print more

    :returns: BLAST search request identifier
    Checks the status of a query.

    :param rid: BLAST search request identifier. Allowed values: The Request ID (RID) returned when the search was submitted
    :param baseURL: server url. Default=http://blast.ncbi.nlm.nih.gov

    :returns status: status for the query.
    :returns therearehist: yes or no for existing hits on a finished query.
    Retrieves results for an RID.

    :param rid: BLAST search request identifier. Allowed values: The Request ID (RID) returned when the search was submitted

    :param format_type: Report type. Allowed values: HTML, Text, XML, XML2, JSON2, or Tabular. Tabular is the default.
    :param hitlist_size: Number of databases sequences to keep. Allowed values: Integer greater than zero.
    :param alignments: Number of alignments to print (applies to HTML and Text). Allowed values: Integer greater than zero.
    :param ncbi_gi: Show NCBI GIs in report. Allowed values: T or F.
    :param format_object: Object type. Allowed values: SearchInfo (status check) or Alignment (report formatting).
    :param baseURL: server url. Default=http://blast.ncbi.nlm.nih.gov

    :returns: the result of a BLAST query. If format_type="Tabular" it will parse the content into a Pandas dataframe.
    Generating HTML report.

    Args:
        store (Store): report data.

    Returns:
        str: rendered HTML template.
        self.list_level += 1
        if self.list_level == 1:
            self.final_ast_tokens.append(ast_token)

def __end_of_list(self, ast_token):
        if self.list_level == 1:
            if self.list_entry is None:
                self.list_entry = ast_token
            elif not isinstance(ast_token, type(self.list_entry)):
                self.final_ast_tokens.append(ast_token)
        elif self.list_level == 0:
            self.final_ast_tokens.append(ast_token)

def compress(self):
        Get AST tokens for Python condition.

        Returns:
            list: list of AST tokens
        Verify that each token in order does match the expected types.

        The list provided by `get_tokens` does have three more elements
        at the beginning of the list which should be always the same
        for a condition (Module and Expr). Those are automatically
        added first to the final list of expected types so you don't have
        to specify it yourself each time.

        >>> tokens = Condition.get_tokens('2 == 3')
        >>> Condition.match_tokens(tokens, [ast.Compare, ast.Num, ast.Eq, ast.Num])
        True

        Args:
            ast_entries (list): list of AST tokens parsers previously.
            ast_types (list): list of expected AST types.

        Returns:
            bool: when all tokes match the expected types
        Find rule for given condition.

        Args:
            condition (str): Python condition as string.

        Returns:
            str, list, function: found rule name, list of AST tokens for condition
                                 and verification function.
        Evaluate simple condition.

        >>> Condition.evaluate('  2  ==  2  ')
        True
        >>> Condition.evaluate('  not  2  ==  2  ')
        False
        >>> Condition.evaluate('  not  "abc"  ==  "xyz"  ')
        True
        >>> Condition.evaluate('2 in [2, 4, 6, 8, 10]')
        True
        >>> Condition.evaluate('5 in [2, 4, 6, 8, 10]')
        False
        >>> Condition.evaluate('"apple" in ["apple", "kiwi", "orange"]')
        True
        >>> Condition.evaluate('5 not in [2, 4, 6, 8, 10]')
        True
        >>> Condition.evaluate('"apple" not in ["kiwi", "orange"]')
        True

        Args:
            condition (str): Python condition as string.

        Returns:
            bool: True when condition evaluates to True.
    starts the web interface and possibly other processes
    Get creator function by name.

    Args:
        name (str): name of the creator function.

    Returns:
        function: creater function.
    creator = get_creator_by_name(data['creator'])
    shell = creator(data['entry'],
                    ShellConfig(script=data['entry']['script'],
                                title=data['entry']['title'] if 'title' in data['entry'] else '',
                                model=data['model'], env=data['env'], item=data['item'],
                                dry_run=data['dry_run'], debug=data['debug'], strict=data['strict'],
                                variables=data['variables'],
                                temporary_scripts_path=data['temporary_scripts_path']))
    output = []
    for line in shell.process():
        output.append(line)
        Logger.get_logger(__name__ + '.worker').info(" | %s", line)
    return {'id': data['id'], 'success': shell.success, 'output': output}

def get_merged_env(self, include_os=False):
        env = {}
        if include_os:
            env.update(os.environ.copy())
        for level in range(3):
            env.update(self.pipeline.data.env_list[level].copy())
        return env

def prepare_shell_data(self, shells, key, entry):
        self.logger.info("Processing group of tasks (parallel=%s)", self.get_parallel_mode())
        self.pipeline.data.env_list[2] = {}

        output, shells = [], []
        result = Adapter({'success': True, 'output': []})
        for task_entry in document:
            key, entry = list(task_entry.items())[0]

            if (not self.parallel or key == 'env') and len(shells) > 0:
                result = Adapter(self.process_shells(shells))
                output += result.output
                shells = []
                if not result.success:
                    break

            if key == 'env':
                self.pipeline.data.env_list[2].update(entry)

            elif key in ['shell', 'docker(container)', 'docker(image)', 'python',
                         'packer', 'ansible(simple)']:
                self.prepare_shell_data(shells, key, entry)

        if result.success:
            result = Adapter(self.process_shells(shells))
            output += result.output

        self.event.delegate(result.success)
        return {'success': result.success, 'output': output}

def process_shells_parallel(self, shells):
        output = []
        for shell in shells:
            entry = shell['entry']
            config = ShellConfig(script=entry['script'], title=entry['title'] if 'title' in entry else '',
                                 model=shell['model'], env=shell['env'], item=shell['item'],
                                 dry_run=shell['dry_run'], debug=shell['debug'], strict=shell['strict'],
                                 variables=shell['variables'],
                                 temporary_scripts_path=shell['temporary_scripts_path'])
            result = Adapter(self.process_shell(get_creator_by_name(shell['creator']), entry, config))
            output += result.output
            self.__handle_variable(entry, result.output)
            if not result.success:
                return {'success': False, 'output': output}
        return {'success': True, 'output': output}

def process_shells(self, shells):
        self.logger.info("Processing Bash code: start")

        output = []
        shell = creator(entry, config)
        for line in shell.process():
            output.append(line)
            self.logger.info(" | %s", line)

        if shell.success:
            self.logger.info("Processing Bash code: finished")
            return {'success': True, 'output': output}

        for line in self.run_cleanup(config.env, shell.exit_code):
            output.append(line)

        self.logger.error("Pipeline has failed: leaving as soon as possible!")
        self.event.failed()
        return {'success': False, 'output': output}

def run_cleanup(self, env, exit_code):
        Saving output for configured variable name.

        Args:
            shell_entry(dict): shell based configuration (shell, docker container or Python).
            output: list of strings representing output of last shell
    This is the main body of the process that does the work.

    Summary:
    - load the raw data
    - read in rules list
    - create log events for AIKIF according to rules [map]
    - create new facts / reports based on rules [report]
    
    OUTPUT = 
                AIKIF mapping  : Date_of_transaction => event
                AIKIF mapping  : Amount => fact
                AIKIF mapping  : Details => location
                New column     : trans_type = DB WHERE amount > 0 ELSE CR
                summing        : details contains "CALTEX" into Travel Expense
                Done    
    
        Returns the distance between the centroids of two catchments in kilometers.

        :param other_catchment: Catchment to calculate distance to
        :type other_catchment: :class:`.Catchment`
        :return: Distance between the catchments in km.
        :rtype: float
        Estimate the `urbext2000` parameter for a given year assuming a nation-wide urbanisation curve.

        Methodology source: eqn 5.5, report FD1919/TR

        :param year: Year to provide estimate for
        :type year: float
        :return: Urban extent parameter
        :rtype: float
        Return a list of continuous data periods by removing the data gaps from the overall record.
        Adding all files from given path to the object.

        Args:
            path (str): valid, existing directory
        Convert JSON into a in memory file storage.

        Args:
            data (str): valid JSON with path and filenames and
                        the base64 encoding of the file content.

        Returns:
            InMemoryFiles: in memory file storage
	delete a single file
	delete all files in folder 'fldr'
	copy single file
	copies all the files from src to dest folder
    script to setup folder structures for AIKIF 
    and prepare data tables.
    print("reading RDF from " + fname + "....")
    store = Graph()
    store.parse(fname, format="n3")
    print("Loaded " + str(len(store)) + " tuples")
    return store

def show_graph_summary(g):
    with open(csv_fname, "w") as f:
        num_tuples = 0
        f.write('"num","subject","predicate","object"\n')
        for subj, pred, obj in g:
            num_tuples += 1
            f.write('"' + str(num_tuples) + '",')
            f.write('"' + get_string_from_rdf(subj) + '",')
            f.write('"' + get_string_from_rdf(pred) + '",')
            f.write('"' + get_string_from_rdf(obj) + '"\n')
    print("Finished exporting " , num_tuples, " tuples")

def get_string_from_rdf(src):
    with open(ip, "rb") as f:
        with open(op, "wb") as fout:
            for _ in range(num_lines):
                fout.write(f.readline() )

def flatten(*sequence):
        result = []
        for entry in self.sequence:
            ignore = False
            for filter_function in self.filter_functions:
                if not filter_function(entry):
                    ignore = True
                    break
            if not ignore:
                value = entry
                for transform_function in self.transform_functions:
                    value = transform_function(value)
                result.append(value)
        return result

def extract_all(zipfile, dest_folder):
    z = ZipFile(zipfile)
    print(z)
    z.extract(dest_folder)

def create_zip_from_file(zip_file, fname):
    with zipfile.ZipFile(zip_file, 'w') as myzip:
        myzip.write(fname)

def create_zip_from_folder(zip_file, fldr, mode="r"):
    #print('zip from folder - adding folder : ', fldr)
    zipf = zipfile.ZipFile(zip_file, 'w')
    for root, dirs, files in os.walk(fldr):
        for file in files:
            fullname = os.path.join(root, file)
            #print('zip - adding file : ', fullname)
            zipf.write(fullname)
    
    
    zipf.close()

def add_method(self, loop, callback):
        f, obj = get_method_vars(callback)
        wrkey = (f, id(obj))
        self[wrkey] = obj
        self.event_loop_map[wrkey] = loop

def iter_methods(self):
        for wrkey, obj in self.iter_instances():
            f, obj_id = wrkey
            loop = self.event_loop_map[wrkey]
            m = getattr(obj, f.__name__)
            yield loop, m

def submit_coroutine(self, coro, loop):

        async def _do_call(_coro):
            with _IterationGuard(self):
                await _coro
        asyncio.run_coroutine_threadsafe(_do_call(coro), loop=loop)

def launch(self):
        if self.fullname != "":
            try:
                os.remove(self.fullname)
            except IOError:
                print("Cant delete ",self.fullname)

def count_lines_in_file(self, fname=''):
        if fname == '':
            fname = self.fullname
        loc = 0    
        try:
            with open(fname) as f:
                for l in f:
                    if l.strip() != '':
                        loc += 1
            return loc    
        except Exception as ex:
            print('cant count lines of code in "', fname, '":', str(ex))
            return 0

def get_file_sample(self, numLines=10):
        with open(self.fullname, "a") as myfile:
            myfile.write(txt)

def load_file_to_string(self):
        lst = []
        try:
            with open(self.fullname, 'r') as f:
                for line in f:
                    lst.append(line) 
            return lst  
        except IOError:
            return lst

def get_program_list():
    colList = ['FileName','FileSize','Functions', 'Imports']

    txt = '<TABLE width=90% border=0>'
    txt += format_file_table_header(colList)
    fl = web.GetFileList(aikif_folder, ['*.py'], 'N')
    for f in fl:
        if '__init__.py' in f:
            txt += '<TR><TD colspan=4><HR><H3>' + get_subfolder(f) + '</h3></td></tr>\n'
        else:
            txt += format_file_to_html_row(f, colList)
    txt += '</TABLE>\n\n'
    return txt

def get_subfolder(txt):
    root_folder = os.sep + 'aikif' + os.sep
    ndx = txt.find(root_folder, 1)
    return txt[ndx:].replace('__init__.py', '')

def get_functions(fname):
    if junk in txt:
        return txt[:txt.find(junk)]
    else:
        return txt

def get_imports(fname):
    This is a sample program to show how a learning agent can
    be logged using AIKIF. 
    The idea is that this main function is your algorithm, which
    will run until it finds a successful result. The result is 
    returned and the time taken is logged.
    
    There can optionally be have additional functions 
    to call to allow for easy logging access
        res = self.connection.get(key)
        print(res)
        return res

def creator(_, config):

    :param bytes_in: the input image's bytes

    :param quality: the output JPEG quality (default 95)

    :returns: Optimized JPEG bytes
    :rtype: bytes

    :raises ValueError: Guetzli was not able to decode the image (the image is
                        probably corrupted or is not a JPEG)

    .. code:: python

        import pyguetzli

        input_jpeg_bytes = open("./test/image.jpg", "rb").read()
        optimized_jpeg = pyguetzli.process_jpeg_bytes(input_jpeg_bytes)

    :param bytes bytes_in: the input image's bytes
    :param int width: the width of the input image
    :param int height: the height of the input image

    :param int quality: the output JPEG quality (default 95)

    :returns: Optimized JPEG bytes
    :rtype: bytes

    :raises ValueError: the given width and height is not coherent with the
                        ``bytes_in`` length.

    .. code:: python

        import pyguetzli

        # 2x2px RGB image
        #                |    red    |   green   |
        image_pixels  = b"\\xFF\\x00\\x00\\x00\\xFF\\x00"
        image_pixels += b"\\x00\\x00\\xFF\\xFF\\xFF\\xFF"
        #                |   blue    |   white   |

        optimized_jpeg = pyguetzli.process_rgb_bytes(image_pixels, 2, 2)
    Decorator for a class to make a singleton out of it.

    @type the_class: class
    @param the_class: the class that should work as a singleton
    @rtype: decorator
    @return: decorator
        Creating or just return the one and only class instance.

        The singleton depends on the parameters used in __init__
        @type args: list
        @param args: positional arguments of the constructor.
        @type kwargs: dict
        @param kwargs: named parameters of the constructor.
        @rtype: decorated class type
        @return: singleton instance of decorated class.
    Printing Grid
     0     0     0     2
     0     0     4     0
     0     0     0     0
     0     0     0     0

    Printing Grid
     0     B     0     B     0     B     0     B
     B     0     B     0     B     0     B     0
     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0
     0     0     0     0     0     0     0     0
     0     W     0     W     0     W     0     W
     W     0     W     0     W     0     W     0
    grd = Grid(4,4, [2,4])
    grd.new_tile()
    grd.new_tile()
    print(grd)
    print("There are ", grd.count_blank_positions(), " blanks in grid 1\n")

    grd2 = Grid(5,5, ['A','B'])
    grd2.new_tile(26)
    print(grd2)
    build_board_checkers()

    print("There are ", grd2.count_blank_positions(), " blanks in grid 2")

def url(self, name):
        key = blobstore.create_gs_key('/gs' + name)
        return images.get_serving_url(key)

def process(self, stage):
        return self._fetch('trading fees', self.market.code)(self._trading_fees)()

def fetch_ticker(self) -> Ticker:
        return self._fetch('order book', self.market.code)(self._order_book)()

def fetch_trades_since(self, since: int) -> List[Trade]:
        return self._transactions(self._deposits, 'deposits', limit)

def fetch_deposits_since(self, since: int) -> List[Deposit]:
        return self._transactions(self._withdrawals, 'withdrawals', limit)

def fetch_withdrawals_since(self, since: int) -> List[Withdrawal]:
        self.log.debug(f'Requesting {self.currency} withdrawal from {self.name} to {address}')
        amount = self._parse_money(amount)

        if self.dry_run:

            withdrawal = Withdrawal.create_default(TxType.WITHDRAWAL, self.currency, amount, address)
            self.log.warning(f'DRY RUN: Withdrawal requested on {self.name}: {withdrawal}')
            return withdrawal

        try:
            withdrawal = self._withdraw(amount, address, subtract_fee, **params)
        except Exception as e:
            msg = f'Failed requesting withdrawal on {self.name}!: amount={amount}, address={address}'
            raise self.exception(InvalidWithdrawal, msg, e) from e

        self.log.info(f'Withdrawal requested on {self.name}: {withdrawal}')
        return withdrawal

def fetch_order(self, order_id: str) -> Order:
        return self._fetch_orders_limit(self._open_orders, limit)

def fetch_closed_orders(self, limit: int) -> List[Order]:
        return self._fetch_orders_since(self._closed_orders_since, since)

def cancel_order(self, order_id: str) -> str:
        orders_to_cancel = order_ids
        self.log.debug(f'Canceling orders on {self.name}: ids={orders_to_cancel}')
        cancelled_orders = []

        if self.dry_run:  # Don't cancel if dry run
            self.log.warning(f'DRY RUN: Orders cancelled on {self.name}: {orders_to_cancel}')
            return orders_to_cancel

        try:  # Iterate and cancel orders
            if self.has_batch_cancel:
                self._cancel_orders(orders_to_cancel)
                cancelled_orders.append(orders_to_cancel)
                orders_to_cancel.clear()
            else:
                for i, order_id in enumerate(orders_to_cancel):
                    self._cancel_order(order_id)
                    cancelled_orders.append(order_id)
                    orders_to_cancel.pop(i)
        except Exception as e:
            msg = f'Failed to cancel {len(orders_to_cancel)} orders on {self.name}: ids={orders_to_cancel}'
            raise self.exception(OrderNotFound, msg, e) from e

        self.log.info(f'Orders cancelled on {self.name}: ids={cancelled_orders}')
        return cancelled_orders

def cancel_all_orders(self) -> List[str]:
        return self._fetch('minimum order amount', self.market.code)(self._min_order_amount)()

def place_market_order(self, side: Side, amount: Number) -> Order:
    This is the main module for the script.  The script will accept a file, or a directory, and then
    encrypt it with a provided key before pushing it to S3 into a specified bucket.
        Queries S3 to identify the region hosting the provided bucket.
    A wrapper for the entire rna alignment subgraph.

    :param list fastqs: The input fastqs for alignment
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict star_options: Options specific to star
    :return: Dict containing input bam and the generated index (.bam.bai)
    :rtype: dict
    Align a pair of fastqs with STAR.

    :param list fastqs: The input fastqs for alignment
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict star_options: Options specific to star
    :return: Dict containing output genome bam, genome bai, and transcriptome bam
                 output_files:
                    |- 'rnaAligned.toTranscriptome.out.bam': fsID
                    +- 'rnaAligned.out.bam': fsID
                    +- 'rnaChimeric.out.junction': fsID
    :rtype: dict
    A wrapper for sorting and indexing the genomic star bam generated by run_star. It is required
    since run_star returns a dict of 2 bams

    :param dict star_bams: The bams from run_star
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict star_options: Options specific to star
    :return: Dict containing input bam and the generated index (.bam.bai)
                     output_files:
                        |- 'rna_transcriptome.bam': fsID
                        +- 'rna_genome':
                                 |- 'rna_sorted.bam': fsID
                                 +- 'rna_sorted.bam.bai': fsID
                        +- 'rnaChimeric.out.junction': fsID
    :rtype: dict
        self.expr = []
        self.matcher = None
        self.last_matcher = None
        self.description = None

def clone(self):

        # If we still have an uninitialized matcher init it now
        if self.matcher:
            self._init_matcher()

        # Evaluate the current set of matchers forming the expression
        matcher = self.evaluate()

        try:
            value = self._transform(value)
            self._assertion(matcher, value)
        except AssertionError as ex:
            # By re-raising here the exception we reset the traceback
            raise ex
        finally:
            # Reset the state of the object so we can use it again

            if self.deferred:
                self.reset()

def _assertion(self, matcher, value):
        # To support the syntax `any_of(subject) | should ...` we check if the
        # value to check is an Expectation object and if it is we use the descriptor
        # protocol to bind the value's assertion logic to this expectation.
        if isinstance(value, Expectation):
            assertion = value._assertion.__get__(self, Expectation)
            assertion(matcher, value.value)
        else:
            hc.assert_that(value, matcher)

def _transform(self, value):
        if self.transform:
            try:
                value = self.transform(value)
            except:
                import sys
                exc_type, exc_obj, exc_tb = sys.exc_info()
                raise AssertionError('Error applying transformation <{0}>: {2}: {3}'.format(
                    self.transform.__name__, value, exc_type.__name__, exc_obj))

        return value

def evaluate(self):

        # Apply Shunting Yard algorithm to convert the infix expression
        # into Reverse Polish Notation. Since we have a very limited
        # set of operators and binding rules, the implementation becomes
        # really simple. The expression is formed of hamcrest matcher instances
        # and operators identifiers (ints).
        ops = []
        rpn = []
        for token in self.expr:
            if isinstance(token, int):
                while len(ops) and token <= ops[-1]:
                    rpn.append(ops.pop())
                ops.append(token)
            else:
                rpn.append(token)

        # Append the remaining operators
        while len(ops):
            rpn.append(ops.pop())

        # Walk the RPN expression to create AllOf/AnyOf matchers
        stack = []
        for token in rpn:
            if isinstance(token, int):
                # Handle the NOT case in a special way since it's unary
                if token == OPERATOR.NOT:
                    stack[-1] = IsNot(stack[-1])
                    continue

                # Our operators always need two operands
                if len(stack) < 2:
                    raise RuntimeError('Unable to build a valid expression. Not enough operands available.')

                # Check what kind of matcher we need to create
                if token == OPERATOR.OR:
                    matcher = hc.any_of(*stack[-2:])
                else:  # AND, BUT
                    matcher = hc.all_of(*stack[-2:])

                stack[-2:] = [matcher]
            else:
                stack.append(token)

        if len(stack) != 1:
            raise RuntimeError('Unable to build a valid expression. The RPN stack should have just one item.')

        matcher = stack.pop()

        # If a description has been given include it in the matcher
        if self.description:
            matcher = hc.described_as(self.description, matcher)

        return matcher

def _find_matcher(self, alias):
        matcher = lookup(alias)
        if not matcher:
            msg = 'Matcher "%s" not found' % alias

            # Try to find similarly named matchers to help the user
            similar = suggest(alias, max=3, cutoff=0.5)
            if len(similar) > 1:
                last = similar.pop()
                msg += '. Perhaps you meant to use %s or %s?' % (', '.join(similar), last)
            elif len(similar) > 0:
                msg += '. Perhaps you meant to use %s?' % similar.pop()

            raise KeyError(msg)

        return matcher

def _init_matcher(self, *args, **kwargs):
        if len(args):
            description = description.format(*args)
        self.description = description
        return self

def make_dbsource(**kwargs):

        Arguments:
        queryset -- QuerySet for Layer
        Keyword args:
        stylename -- str name of style to apply

        Arguments:
        bbox -- OGRGeometry polygon to zoom map extent
        style = mapnik.Style()
        rule = mapnik.Rule()
        self._symbolizer = self.symbolizer()
        rule.symbols.append(self._symbolizer)
        style.rules.append(rule)
        return style

def wrap_fusion(job,
                fastqs,
                star_output,
                univ_options,
                star_fusion_options,
                fusion_inspector_options):
    # Give user option to skip fusion calling
    if not star_fusion_options['run']:
        job.fileStore.logToMaster('Skipping STAR-Fusion on %s' % univ_options['patient'])
        return

    fusion = job.wrapJobFn(run_fusion, fastqs, star_output['rnaChimeric.out.junction'],
                               univ_options, star_fusion_options, fusion_inspector_options,
                               cores=star_fusion_options['n'],
                               memory=PromisedRequirement(lambda x: int(1.85 * x.size),
                                                          star_fusion_options['index']),
                               disk=PromisedRequirement(fusion_disk,
                                                        fastqs,
                                                        star_fusion_options['index'])).encapsulate()
    job.addChild(fusion)
    return fusion.rv()

def parse_star_fusion(infile):
    reader = csv.reader(infile, delimiter='\t')
    header = reader.next()
    header = {key: index for index, key in enumerate(header)}

    features = ['LeftGene', 'LeftLocalBreakpoint', 'LeftBreakpoint',
                'RightGene', 'RightLocalBreakpoint', 'RightBreakpoint',
                'LargeAnchorSupport', 'JunctionReadCount', 'SpanningFragCount']

    for line in reader:
        yield Expando(dict((feature, line[header[feature]]) for feature in features))

def get_transcripts(transcript_file):
    with open(transcript_file, 'r') as fa:
        transcripts = {}
        regex_s = r"(?P<ID>TRINITY.*)\s(?P<fusion>.*--.*):(?P<left_start>\d+)-(?P<right_start>\d+)"
        regex = re.compile(regex_s)
        while True:
            # Usually the transcript is on one line
            try:
                info = fa.next()
                seq = fa.next()

                assert info.startswith('>')

                m = regex.search(info)
                if m:
                    transcripts[m.group('ID')] = seq.strip()

            except StopIteration:
                break

            except AssertionError:
                print("WARNING: Malformed fusion transcript file")
    return transcripts

def split_fusion_transcript(annotation_path, transcripts):

    annotation = collections.defaultdict(dict)

    forward = 'ACGTN'
    reverse = 'TGCAN'
    trans = string.maketrans(forward, reverse)

    # Pull in assembled transcript annotation

    five_pr_splits = collections.defaultdict(dict)

    three_pr_splits = collections.defaultdict(dict)

    regex = re.compile(r'ID=(?P<ID>.*);Name=(?P<Name>.*);Target=(?P<Target>.*)\s(?P<start>\d+)\s(?P<stop>\d+)')

    with open(annotation_path, 'r') as gff:
        for line in gff:
            print(line)
            if line.startswith('#'):
                _, eyd, fusion = line.strip().split()
                fusion, start_stop = fusion.split(':')
                left_break, right_break = start_stop.split('-')

                annotation[fusion][eyd] = {}
                annotation[fusion][eyd]['left_break'] = left_break
                annotation[fusion][eyd]['right_break'] = right_break

            else:
                line = line.strip().split('\t')
                fusion = line[0]
                strand = line[6]
                block_start = line[3]
                block_stop = line[4]
                attr = line[8]
                m = regex.search(attr)
                if m:
                    transcript_id = m.group('Name')

                    rb = any([block_start == annotation[fusion][transcript_id]['right_break'],
                              block_stop == annotation[fusion][transcript_id]['right_break']])

                    lb = any([block_start == annotation[fusion][transcript_id]['left_break'],
                              block_stop == annotation[fusion][transcript_id]['left_break']])

                    if strand == '-' and rb:
                        transcript_split = int(m.group('stop')) + 1   # Off by one
                        # Take the reverse complement to orient transcripts from 5' to 3'
                        five_seq = transcripts[transcript_id][transcript_split:]
                        five_pr_splits[fusion][transcript_id] = five_seq.translate(trans)[::-1]
                        three_seq = transcripts[transcript_id][:transcript_split]
                        three_pr_splits[fusion][transcript_id] = three_seq.translate(trans)[::-1]

                    elif strand == '+' and lb:
                        transcript_split = int(m.group('stop'))
                        s1 = transcripts[transcript_id][:transcript_split]
                        five_pr_splits[fusion][transcript_id] = s1
                        s2 = transcripts[transcript_id][transcript_split:]
                        three_pr_splits[fusion][transcript_id] = s2

    return five_pr_splits, three_pr_splits

def get_gene_ids(fusion_bed):
    with open(fusion_bed, 'r') as f:
        gene_to_id = {}
        regex = re.compile(r'(?P<gene>ENSG\d*)')
        for line in f:
            line = line.split('\t')
            transcript, gene_bit, name = line[3].split(';')
            m = regex.search(gene_bit)
            if m:
                gene_to_id[name] = m.group('gene')
    return gene_to_id

def reformat_star_fusion_output(job,
                                fusion_annot,
                                fusion_file,
                                transcript_file,
                                transcript_gff_file,
                                univ_options):
    input_files = {'results.tsv': fusion_file,
                   'fusion.bed': fusion_annot}

    if transcript_file and transcript_gff_file:
        input_files['transcripts.fa'] = transcript_file
        input_files['transcripts.gff'] = transcript_gff_file

    work_dir = job.fileStore.getLocalTempDir()
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)

    # Pull in assembled transcript file
    hugo_to_gene_ids = get_gene_ids(input_files['fusion.bed'])

    if transcript_file and transcript_gff_file:
        transcripts = get_transcripts(input_files['transcripts.fa'])
        five_pr_splits, three_pr_splits = split_fusion_transcript(input_files['transcripts.gff'],
                                                                  transcripts)

    else:

        five_pr_splits = collections.defaultdict(dict)

        three_pr_splits = collections.defaultdict(dict)

    # Pull in assembled transcript annotation

    # Header for BEDPE file
    header = ['# chr1', 'start1', 'end1',
              'chr2', 'start2', 'end2',
              'name', 'score',
              'strand1', 'strand2',
              'junctionSeq1', 'junctionSeq2',
              'hugo1', 'hugo2']

    output_path = os.path.join(work_dir, 'fusion_results.bedpe')
    with open(input_files['results.tsv'], 'r') as in_f, open(output_path, 'w') as out_f:
        writer = csv.writer(out_f, delimiter='\t')
        writer.writerow(header)
        for record in parse_star_fusion(in_f):

            left_chr, left_break, left_strand = record.LeftBreakpoint.split(':')

            right_chr, right_break, right_strand = record.RightBreakpoint.split(':')

            fusion = ''.join([record.LeftGene, '--', record.RightGene])
            name = '-'.join([hugo_to_gene_ids[record.LeftGene], hugo_to_gene_ids[record.RightGene]])
            score = 'Junction:%s-Spanning:%s' % (record.JunctionReadCount, record.SpanningFragCount)

            # Add empty sequences in case Trinity doesn't output one
            if len(five_pr_splits[fusion].keys()) == 0:
                five_pr_splits[fusion]['N/A'] = '.'

            if len(three_pr_splits[fusion].keys()) == 0:
                three_pr_splits[fusion]['N/A'] = '.'

            for transcript_id in five_pr_splits[fusion].keys():
                five_prime_seq = five_pr_splits[fusion][transcript_id]
                three_prime_seq = three_pr_splits[fusion][transcript_id]

                writer.writerow([left_chr,
                                 '.',  # Donor start position is not necessary
                                 left_break,
                                 right_chr,
                                 right_break,
                                 '.',  # Acceptor end position is not necessary
                                 name,
                                 score,
                                 left_strand,
                                 right_strand,
                                 five_prime_seq,
                                 three_prime_seq,
                                 record.LeftGene,
                                 record.RightGene])

    bedpe_id = job.fileStore.writeGlobalFile(output_path)
    export_results(job, bedpe_id, 'fusion.bedpe', univ_options, subfolder='mutations/fusions')
    job.fileStore.logToMaster('Reformatted STAR-Fusion output for %s successfully'
                              % univ_options['patient'])
    return bedpe_id

def _ensure_patient_group_is_ok(patient_object, patient_name=None):
    from protect.addons.common import TCGAToGTEx
    assert isinstance(patient_object, (set, dict)), '%s,%s' % (patient_object, patient_name)
    # set(dict) = set of keys of the dict
    test_set = set(patient_object)
    if 'tumor_type' not in patient_object:
        raise ParameterError(('The patient entry for sample %s ' % patient_name) +
                             'does not contain a Tumor type.')
    elif patient_object['tumor_type'] not in TCGAToGTEx:
        raise ParameterError(('The patient entry for sample %s ' % patient_name) +
                             'does contains an invalid Tumor type. Please use one of the '
                             'valid TCGA tumor types.')
    if {'tumor_dna_fastq_1', 'normal_dna_fastq_1', 'tumor_rna_fastq_1'}.issubset(test_set):
        # Best case scenario, we get all fastqs
        pass
    else:
        # We have less than 3 fastqs so we have to have a haplotype.
        if 'hla_haplotype_files' not in test_set:
            raise ParameterError(('The patient entry for sample %s ' % patient_name) +
                                 'does not contain a hla_haplotype_files entry.\nCannot haplotype '
                                 'patient if all the input sequence files are not fastqs.')
        # Either we have a fastq and/or bam for the tumor and normal, or we need to be given a vcf
        if (({re.search('tumor_dna_((bam)|(fastq_1)).*', x) for x in test_set} == {None} or
                {re.search('normal_dna_((bam)|(fastq_1)).*', x) for x in test_set} == {None}) and
                ('mutation_vcf' not in test_set and 'fusion_bedpe' not in test_set)):
            raise ParameterError(('The patient entry for sample %s ' % patient_name) +
                                 'does not contain a mutation_vcf or fusion_bedpe entry. If both '
                                 'tumor and normal DNA sequences (fastqs or bam) are not provided, '
                                 'a pre-computed vcf and/or bedpe must be provided.')
        # We have to be given a tumor rna fastq or bam unless we are processing ONLY fusions
        if {re.search('tumor_rna_((bam)|(fastq_1)).*', x) for x in test_set} == {None}:
            if 'mutation_vcf' not in test_set and 'fusion_bedpe' in test_set:
                # The only case where it is ok to not have the genome mapped rna.
                pass
            else:
                raise ParameterError(('The patient entry for sample %s ' % patient_name) +
                                     'does not contain a tumor rna sequence data entry. We require '
                                     'either tumor_rna_fastq_1 or tumor_rna_bam.')
        # If we are given an RNA bam then it needs to have a corresponding transcriptome bam unless
        # we have also been provided expression values.
        if 'tumor_rna_bam' in test_set and 'tumor_rna_transcriptome_bam' not in test_set:
            if 'expression_files' not in test_set:
                raise ParameterError(('The patient entry for sample %s ' % patient_name +
                                      'was provided a tumor rna bam with sequences mapped to the '
                                      'genome but was not provided a matching rna bam for the '
                                      'transcriptome or a tar containing expression values. '
                                      'We require either a matching transcriptome bam to estimate'
                                      'expression, or the precomputed expression values.'))

def _add_default_entries(input_dict, defaults_dict):

    for key, value in defaults_dict.iteritems():
        if key == 'patients':

            print('Cannot default `patients`.')
            continue
        if isinstance(value, dict):
            if key not in input_dict or input_dict[key] is None:
                # User didn't specify anython for the tool, but the entry was still in there so we

                # just copy over the whole defaults dict
                input_dict[key] = value
            else:

                r = _add_default_entries(input_dict.get(key, {}), value)
                input_dict[key] = r
        else:
            # Only write if not in input_dict
            if key not in input_dict or input_dict[key] is None:
                # Either the user didn't have the entry, or had it without a value
                input_dict[key] = value
    return input_dict

def _process_group(input_group, required_group, groupname, append_subgroups=None):
    if append_subgroups is None:
        append_subgroups = []
    tool_options = {}
    for key in input_group:
        _ensure_set_contains(input_group[key], required_group.get(key, {}), groupname + '::' + key)
        if key in append_subgroups:
            continue
        else:
            tool_options[key] = input_group[key]
    for key in input_group:
        if key in append_subgroups:
            continue
        else:
            for yek in append_subgroups:
                tool_options[key].update(input_group[yek])
    return tool_options

def get_fastq_2(job, patient_id, sample_type, fastq_1):
    prefix, extn = fastq_1, 'temp'
    final_extn = ''
    while extn:
        prefix, extn = os.path.splitext(prefix)
        final_extn = extn + final_extn
        if prefix.endswith('1'):
            prefix = prefix[:-1]
            job.fileStore.logToMaster('"%s" prefix for "%s" determined to be %s'
                                      % (sample_type, patient_id, prefix))
            break
    else:
        raise ParameterError('Could not determine prefix from provided fastq (%s). Is it '
                             'of the form <fastq_prefix>1.[fq/fastq][.gz]?' % fastq_1)
    if final_extn not in ['.fastq', '.fastq.gz', '.fq', '.fq.gz']:
        raise ParameterError('If and _2 fastq path is not specified, only .fastq, .fq or '
                             'their gzippped extensions are accepted. Could not process '
                             '%s:%s.' % (patient_id, sample_type + '_fastq_1'))
    return ''.join([prefix, '2', final_extn])

def parse_config_file(job, config_file, max_cores=None):
    sample_set, univ_options, processed_tool_inputs = _parse_config_file(job, config_file,
                                                                         max_cores)
    # Start a job for each sample in the sample set
    for patient_id in sample_set.keys():
        job.addFollowOnJobFn(launch_protect, sample_set[patient_id], univ_options,
                             processed_tool_inputs)
    return None

def get_all_tool_inputs(job, tools, outer_key='', mutation_caller_list=None):
    for tool in tools:
        for option in tools[tool]:
            if isinstance(tools[tool][option], dict):
                tools[tool][option] = get_all_tool_inputs(
                    job, {option: tools[tool][option]},
                    outer_key=':'.join([outer_key, tool]).lstrip(':'))[option]
            else:
                # If a file is of the type file, vcf, tar or fasta, it needs to be downloaded from
                # S3 if reqd, then written to job store.
                if option.split('_')[-1] in ['file', 'vcf', 'index', 'fasta', 'fai', 'idx', 'dict',
                                             'tbi', 'beds', 'gtf', 'config']:
                    tools[tool][option] = job.addChildJobFn(
                        get_pipeline_inputs, ':'.join([outer_key, tool, option]).lstrip(':'),
                        tools[tool][option]).rv()
                elif option == 'version':
                    tools[tool][option] = str(tools[tool][option])
    if mutation_caller_list is not None:
        # Guaranteed to occur only in the outermost loop
        indexes = tools.pop('indexes')
        indexes['chromosomes'] = parse_chromosome_string(job, indexes['chromosomes'])
        for mutation_caller in mutation_caller_list:
            if mutation_caller == 'indexes':
                continue
            tools[mutation_caller].update(indexes)
    return tools

def get_pipeline_inputs(job, input_flag, input_file, encryption_key=None, per_file_encryption=False,
                        gdc_download_token=None):
    work_dir = os.getcwd()
    job.fileStore.logToMaster('Obtaining file (%s) to the file job store' % input_flag)
    if input_file.startswith(('http', 'https', 'ftp')):
        input_file = get_file_from_url(job, input_file, encryption_key=encryption_key,
                                       per_file_encryption=per_file_encryption,
                                       write_to_jobstore=True)
    elif input_file.startswith(('S3', 's3')):
        input_file = get_file_from_s3(job, input_file, encryption_key=encryption_key,
                                      per_file_encryption=per_file_encryption,
                                      write_to_jobstore=True)
    elif input_file.startswith(('GDC', 'gdc')):
        input_file = get_file_from_gdc(job, input_file, gdc_download_token=gdc_download_token,
                                       write_to_jobstore=True)
    else:
        assert os.path.exists(input_file), 'Bogus Input : ' + input_file
        input_file = job.fileStore.writeGlobalFile(input_file)
    return input_file

def prepare_samples(job, patient_dict, univ_options):
    job.fileStore.logToMaster('Downloading Inputs for %s' % univ_options['patient'])
    # For each sample type, check if the prefix is an S3 link or a regular file
    # Download S3 files.
    output_dict = {}
    for input_file in patient_dict:
        if not input_file.endswith(('bam', 'bai', '_1', '_2', 'files', 'vcf', 'bedpe')):
            output_dict[input_file] = patient_dict[input_file]
            continue
        output_dict[input_file] = get_pipeline_inputs(
            job, ':'.join([univ_options['patient'], input_file]), patient_dict[input_file],
            encryption_key=(univ_options['sse_key'] if patient_dict['ssec_encrypted'] else None),
            per_file_encryption=univ_options['sse_key_is_master'],
            gdc_download_token=univ_options['gdc_download_token'])
    return output_dict

def get_patient_bams(job, patient_dict, sample_type, univ_options, bwa_options, mutect_options):
    output_dict = {}
    if 'dna' in sample_type:
        sample_info = 'fix_pg_sorted'
        prefix = sample_type + '_' + sample_info
    else:
        sample_info = 'genome_sorted'
        prefix = 'rna_' + sample_info
    if sample_type + '_bam' in patient_dict['gdc_inputs']:
        output_dict[prefix + '.bam'] = patient_dict[sample_type + '_bam'][0]
        output_dict[prefix + '.bam.bai'] = patient_dict[sample_type + '_bam'][1]
    elif sample_type + '_bai' in patient_dict:
        output_dict[prefix + '.bam'] = patient_dict[sample_type + '_bam']
        output_dict[prefix + '.bam.bai'] = patient_dict[sample_type + '_bai']
    else:
        from protect.alignment.dna import index_bamfile, index_disk
        output_job = job.wrapJobFn(index_bamfile, patient_dict[sample_type + '_bam'],
                                   'rna' if sample_type == 'tumor_rna' else sample_type,
                                   univ_options, bwa_options['samtools'],
                                   sample_info=sample_info, export=False,
                                   disk=PromisedRequirement(index_disk,
                                                            patient_dict[sample_type + '_bam']))
        job.addChild(output_job)
        output_dict = output_job.rv()
    if sample_type == 'tumor_rna':
        if 'tumor_rna_transcriptome_bam' not in patient_dict:
            patient_dict['tumor_rna_transcriptome_bam'] = None
        return{'rna_genome': output_dict,
               'rna_transcriptome.bam': patient_dict['tumor_rna_transcriptome_bam']}
    else:
        return output_dict

def get_patient_vcf(job, patient_dict):
    temp = job.fileStore.readGlobalFile(patient_dict['mutation_vcf'],
                                        os.path.join(os.getcwd(), 'temp.gz'))
    if is_gzipfile(temp):
        outfile = job.fileStore.writeGlobalFile(gunzip(temp))
        job.fileStore.deleteGlobalFile(patient_dict['mutation_vcf'])
    else:
        outfile = patient_dict['mutation_vcf']
    return outfile

def get_patient_mhc_haplotype(job, patient_dict):
    haplotype_archive = job.fileStore.readGlobalFile(patient_dict['hla_haplotype_files'])
    haplotype_archive = untargz(haplotype_archive, os.getcwd())
    output_dict = {}
    for filename in 'mhci_alleles.list', 'mhcii_alleles.list':
        output_dict[filename] = job.fileStore.writeGlobalFile(os.path.join(haplotype_archive,
                                                                           filename))
    return output_dict

def get_patient_expression(job, patient_dict):
    expression_archive = job.fileStore.readGlobalFile(patient_dict['expression_files'])
    expression_archive = untargz(expression_archive, os.getcwd())
    output_dict = {}
    for filename in 'rsem.genes.results', 'rsem.isoforms.results':
        output_dict[filename] = job.fileStore.writeGlobalFile(os.path.join(expression_archive,
                                                                           filename))
    return output_dict

def generate_config_file():
    shutil.copy(os.path.join(os.path.dirname(__file__), 'input_parameters.yaml'),
                os.path.join(os.getcwd(), 'ProTECT_config.yaml'))

def main():
    parser = argparse.ArgumentParser(prog='ProTECT',
                                     description='Prediction of T-Cell Epitopes for Cancer Therapy',
                                     epilog='Contact Arjun Rao (aarao@ucsc.edu) if you encounter '
                                     'any problems while running ProTECT')
    inputs = parser.add_mutually_exclusive_group(required=True)
    inputs.add_argument('--config_file', dest='config_file', help='Config file to be used in the '

                        'run.', type=str, default=None)
    inputs.add_argument('--generate_config', dest='generate_config', help='Generate a config file '
                        'in the current directory that is pre-filled with references and flags for '

                        'an hg19 run.', action='store_true', default=False)
    parser.add_argument('--max-cores-per-job', dest='max_cores', help='Maximum cores to use per '
                        'job. Aligners and Haplotypers ask for cores dependent on the machine that '
                        'the launchpad gets assigned to -- In a heterogeneous cluster, this can '
                        'lead to problems. This value should be set to the number of cpus on the '
                        'smallest node in a cluster.',

                        type=int, required=False, default=None)
    # We parse the args once to see if the user has asked for a config file to be generated.  In
    # this case, we don't need a jobstore.  To handle the case where Toil arguments are passed to
    # ProTECT, we parse known args, and if the used specified config_file instead of generate_config
    # we re-parse the arguments with the added Toil parser.
    params, others = parser.parse_known_args()
    if params.generate_config:
        generate_config_file()
    else:
        Job.Runner.addToilOptions(parser)
        params = parser.parse_args()
        params.config_file = os.path.abspath(params.config_file)
        if params.maxCores:
            if not params.max_cores:
                params.max_cores = int(params.maxCores)
            else:
                if params.max_cores > int(params.maxCores):
                    print("The value provided to max-cores-per-job (%s) was greater than that "
                          "provided to maxCores (%s). Setting max-cores-per-job = maxCores." %
                          (params.max_cores, params.maxCores), file=sys.stderr)
                    params.max_cores = int(params.maxCores)
        start = Job.wrapJobFn(parse_config_file, params.config_file, params.max_cores)
        Job.Runner.startToil(start, params)
    return None

def poll(self):
        if select.select([self.tn], [], [], 0) == ([self.tn], [], []):
            response = urllib.unquote(self.tn.read_until(b"\n").decode())
            if self.debug: print "Telnet Poll: %s" % (response[:-1])
            # TODO Keep track of which screen is displayed
            return response
        else:
            return None

def module_to_dict(module, omittable=lambda k: k.startswith('_')):
    return dict([(k, repr(v)) for k, v in module.__dict__.items() if not omittable(k)])

def run_snpeff(job, merged_mutation_file, univ_options, snpeff_options):
    work_dir = os.getcwd()
    input_files = {
        'merged_mutations.vcf': merged_mutation_file,
        'snpeff_index.tar.gz': snpeff_options['index']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)
    input_files['snpeff_index'] = untargz(input_files['snpeff_index.tar.gz'], work_dir)
    input_files = {key: docker_path(path) for key, path in input_files.items()}

    parameters = ['eff',
                  '-dataDir', input_files['snpeff_index'],
                  '-c', '/'.join([input_files['snpeff_index'],
                                  'snpEff_' + univ_options['ref'] + '_gencode.config']),
                  '-no-intergenic',
                  '-no-downstream',
                  '-no-upstream',
                  # '-canon',
                  '-noStats',
                  univ_options['ref'] + '_gencode',
                  input_files['merged_mutations.vcf']]
    xmx = snpeff_options['java_Xmx'] if snpeff_options['java_Xmx'] else univ_options['java_Xmx']
    with open('/'.join([work_dir, 'mutations.vcf']), 'w') as snpeff_file:
        docker_call(tool='snpeff', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], java_xmx=xmx, outfile=snpeff_file,
                    tool_version=snpeff_options['version'])
    output_file = job.fileStore.writeGlobalFile(snpeff_file.name)
    export_results(job, output_file, snpeff_file.name, univ_options, subfolder='mutations/snpeffed')
    job.fileStore.logToMaster('Ran snpeff on %s successfully' % univ_options['patient'])
    return output_file

def paths_in_directory(input_directory):
    paths = []
    for base_path, directories, filenames in os.walk(input_directory):
        relative_path = os.path.relpath(base_path, input_directory)
        path_components = relative_path.split(os.sep)
        if path_components[0] == ".":
            path_components = path_components[1:]
        if path_components and path_components[0].startswith("."):
            # hidden dir
            continue
        path_components = filter(bool, path_components)  # remove empty components
        for filename in filenames:
            if filename.startswith("."):
                # hidden file
                continue
            paths.append(path_components + [filename])
    return paths

def run_car_t_validity_assessment(job, rsem_files, univ_options, reports_options):
    return job.addChildJobFn(assess_car_t_validity, rsem_files['rsem.genes.results'],
                             univ_options, reports_options).rv()

def align_dna(job, fastqs, sample_type, univ_options, bwa_options):
    # The mkdup and regroup steps use picard that allots heap space using the Xmx key in the
    # univ_options dictionary. This should reflect in the job allotment. Since We want all these
    # jobs to occur on the same node, we ened to give them all the same memory requirements.

    bwa = job.wrapJobFn(run_bwa, fastqs, sample_type, univ_options, bwa_options,
                        disk=PromisedRequirement(bwa_disk, fastqs, bwa_options['index']),
                        memory=univ_options['java_Xmx'],
                        cores=bwa_options['n'])
    sam2bam = job.wrapJobFn(bam_conversion, bwa.rv(), sample_type, univ_options,
                            bwa_options['samtools'],
                            disk=PromisedRequirement(sam2bam_disk, bwa.rv()),
                            memory=univ_options['java_Xmx'])
    # reheader takes the same disk as sam2bam so we can serialize this on the same worker.
    reheader = job.wrapJobFn(fix_bam_header, sam2bam.rv(), sample_type, univ_options,
                             bwa_options['samtools'],
                             disk=PromisedRequirement(sam2bam_disk, bwa.rv()),
                             memory=univ_options['java_Xmx'])
    regroup = job.wrapJobFn(add_readgroups, reheader.rv(), sample_type, univ_options,
                            bwa_options['picard'],
                            disk=PromisedRequirement(regroup_disk, reheader.rv()),
                            memory=univ_options['java_Xmx'])
    mkdup = job.wrapJobFn(mark_duplicates, regroup.rv(), sample_type, univ_options,
                          bwa_options['picard'],
                          disk=PromisedRequirement(mkdup_disk, regroup.rv()),
                          memory=univ_options['java_Xmx'])
    index = job.wrapJobFn(index_bamfile, mkdup.rv(), sample_type, univ_options,
                          bwa_options['samtools'], sample_info='fix_pg_sorted',
                          disk=PromisedRequirement(index_disk, mkdup.rv()),
                          memory=univ_options['java_Xmx'])
    job.addChild(bwa)
    bwa.addChild(sam2bam)
    sam2bam.addChild(reheader)
    reheader.addChild(regroup)
    regroup.addChild(mkdup)
    mkdup.addChild(index)
    return index.rv()

def run_bwa(job, fastqs, sample_type, univ_options, bwa_options):
    work_dir = os.getcwd()
    input_files = {
        'dna_1.fastq': fastqs[0],
        'dna_2.fastq': fastqs[1],
        'bwa_index.tar.gz': bwa_options['index']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)
    # Handle gzipped file
    gz = '.gz' if is_gzipfile(input_files['dna_1.fastq']) else ''
    if gz:
        for read_file in 'dna_1.fastq', 'dna_2.fastq':
            os.symlink(read_file, read_file + gz)
            input_files[read_file + gz] = input_files[read_file] + gz
    # Untar the index
    input_files['bwa_index'] = untargz(input_files['bwa_index.tar.gz'], work_dir)
    input_files = {key: docker_path(path) for key, path in input_files.items()}

    parameters = ['mem',
                  '-t', str(bwa_options['n']),
                  '-v', '1',  # Don't print INFO messages to the stderr
                  '/'.join([input_files['bwa_index'], univ_options['ref']]),
                  input_files['dna_1.fastq' + gz],
                  input_files['dna_2.fastq' + gz]]
    with open(''.join([work_dir, '/', sample_type, '.sam']), 'w') as samfile:
        docker_call(tool='bwa', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], outfile=samfile,
                    tool_version=bwa_options['version'])
    # samfile.name retains the path info
    output_file = job.fileStore.writeGlobalFile(samfile.name)
    job.fileStore.logToMaster('Ran bwa on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return output_file

def bam_conversion(job, samfile, sample_type, univ_options, samtools_options):
    work_dir = os.getcwd()
    input_files = {
        sample_type + '.sam': samfile}
    input_files = get_files_from_filestore(job, input_files, work_dir,
                                           docker=True)
    bamfile = '/'.join([work_dir, sample_type + '.bam'])
    parameters = ['view',
                  '-bS',
                  '-o', docker_path(bamfile),
                  input_files[sample_type + '.sam']
                  ]
    docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], tool_version=samtools_options['version'])
    output_file = job.fileStore.writeGlobalFile(bamfile)
    # The samfile is no longer useful so delete it
    job.fileStore.deleteGlobalFile(samfile)
    job.fileStore.logToMaster('Ran sam2bam on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return output_file

def fix_bam_header(job, bamfile, sample_type, univ_options, samtools_options, retained_chroms=None):
    if retained_chroms is None:
        retained_chroms = []

    work_dir = os.getcwd()
    input_files = {
        sample_type + '.bam': bamfile}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=True)
    parameters = ['view',
                  '-H',
                  input_files[sample_type + '.bam']]
    with open('/'.join([work_dir, sample_type + '_input_bam.header']), 'w') as headerfile:
        docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], outfile=headerfile,
                    tool_version=samtools_options['version'])
    with open(headerfile.name, 'r') as headerfile, \
            open('/'.join([work_dir, sample_type + '_output_bam.header']), 'w') as outheaderfile:
        for line in headerfile:
            if line.startswith('@PG'):
                line = '\t'.join([x for x in line.strip().split('\t') if not x.startswith('CL')])
            if retained_chroms and line.startswith('@SQ'):
                if line.strip().split()[1].lstrip('SN:') not in retained_chroms:
                    continue
            print(line.strip(), file=outheaderfile)
    parameters = ['reheader',
                  docker_path(outheaderfile.name),
                  input_files[sample_type + '.bam']]
    with open('/'.join([work_dir, sample_type + '_fixPG.bam']), 'w') as fixpg_bamfile:
        docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], outfile=fixpg_bamfile,
                    tool_version=samtools_options['version'])
    output_file = job.fileStore.writeGlobalFile(fixpg_bamfile.name)
    # The old bam file is now useless.
    job.fileStore.deleteGlobalFile(bamfile)
    job.fileStore.logToMaster('Ran reheader on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return output_file

def add_readgroups(job, bamfile, sample_type, univ_options, picard_options):
    work_dir = os.getcwd()
    input_files = {
        sample_type + '.bam': bamfile}
    get_files_from_filestore(job, input_files, work_dir, docker=True)
    parameters = ['AddOrReplaceReadGroups',
                  'CREATE_INDEX=false',
                  'I=/data/' + sample_type + '.bam',
                  'O=/data/' + sample_type + '_reheader.bam',
                  'SO=coordinate',
                  'ID=1',
                  ''.join(['LB=', univ_options['patient']]),
                  'PL=ILLUMINA',
                  'PU=12345',
                  ''.join(['SM=', sample_type.rstrip('_dna')])]
    docker_call(tool='picard', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], java_xmx=univ_options['java_Xmx'],
                tool_version=picard_options['version'])
    output_file = job.fileStore.writeGlobalFile(
        '/'.join([work_dir, sample_type + '_reheader.bam']))
    # Delete the old bam file
    job.fileStore.deleteGlobalFile(bamfile)
    job.fileStore.logToMaster('Ran add_read_groups on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return output_file

def weekday(cls, year, month, day):
        functions.check_valid_bs_range(NepDate(year, month, 1))
        return values.NEPALI_MONTH_DAY_DATA[year][month - 1]

def itermonthdays(cls, year, month):
        for day in NepCal.itermonthdates(year, month):
            if day.month == month:
                yield day.day
            else:
                yield 0

def itermonthdays2(cls, year, month):
        for day in NepCal.itermonthdates(year, month):
            if day.month == month:
                yield (day.day, day.weekday())
            else:
                yield (0, day.weekday())

def monthdatescalendar(cls, year, month):
    A wrapper for the the entire SomaticSniper sub-graph.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :return: fsID to the merged SomaticSniper calls
    :rtype: toil.fileStore.FileID
    Run the SomaticSniper subgraph on the DNA bams.  Optionally split the results into
    per-chromosome vcfs.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :param bool split: Should the results be split into perchrom vcfs?
    :return: Either the fsID to the genome-level vcf or a dict of results from running SomaticSniper
             on every chromosome
             perchrom_somaticsniper:
                 |- 'chr1': fsID
                 |- 'chr2' fsID
                 |
                 |-...
                 |
                 +- 'chrM': fsID
    :rtype: toil.fileStore.FileID|dict
    Run SomaticSniper on the DNA bams.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :return: fsID to the genome-level vcf
    :rtype: toil.fileStore.FileID
    Filter SomaticSniper calls.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param toil.fileStore.FileID somaticsniper_output: SomaticSniper output vcf
    :param toil.fileStore.FileID tumor_pileup: Pileup generated for the tumor bam
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :returns: fsID for the filtered genome-level vcf
    :rtype: toil.fileStore.FileID
    Runs a samtools pileup on the tumor bam.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict somaticsniper_options: Options specific to SomaticSniper
    :return: fsID for the pileup file
    :rtype: toil.fileStore.FileID
    tokens = [str(name)]
    if argument:
        tokens.append(str(argument))
    return '::'.join(tokens)

def removed_or_inserted_action(mapper, connection, target):
    action_history = get_history(target, 'action')
    argument_history = get_history(target, 'argument')
    owner_history = get_history(
        target,
        'user' if isinstance(target, ActionUsers) else
        'role' if isinstance(target, ActionRoles) else 'role_name')

    if action_history.has_changes() or argument_history.has_changes() \
       or owner_history.has_changes():
        current_access.delete_action_cache(
            get_action_cache_key(target.action, target.argument))
        current_access.delete_action_cache(
            get_action_cache_key(
                action_history.deleted[0] if action_history.deleted
                else target.action,
                argument_history.deleted[0] if argument_history.deleted
                else target.argument)
        )

def allow(cls, action, **kwargs):
        return cls.create(action, exclude=False, **kwargs)

def deny(cls, action, **kwargs):
        return cls.create(action, exclude=True, **kwargs)

def query_by_action(cls, action, argument=None):
        query = cls.query.filter_by(action=action.value)
        argument = argument or getattr(action, 'argument', None)
        if argument is not None:
            query = query.filter(db.or_(
                cls.argument == str(argument),
                cls.argument.is_(None),
            ))
        else:
            query = query.filter(cls.argument.is_(None))
        return query

def predict_mhci_binding(job, peptfile, allele, peplen, univ_options, mhci_options):
    work_dir = os.getcwd()
    input_files = {
        'peptfile.faa': peptfile}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=True)
    peptides = read_peptide_file(os.path.join(os.getcwd(), 'peptfile.faa'))
    if not peptides:
        return job.fileStore.writeGlobalFile(job.fileStore.getLocalTempFile())
    parameters = [mhci_options['pred'],
                  allele,
                  peplen,
                  input_files['peptfile.faa']]
    with open('/'.join([work_dir, 'predictions.tsv']), 'w') as predfile:
        docker_call(tool='mhci', tool_parameters=parameters, work_dir=work_dir,
                    dockerhub=univ_options['dockerhub'], outfile=predfile, interactive=True,
                    tool_version=mhci_options['version'])
    output_file = job.fileStore.writeGlobalFile(predfile.name)
    job.fileStore.logToMaster('Ran mhci on %s:%s:%s successfully'
                              % (univ_options['patient'], allele, peplen))
    return output_file

def iter_and_close(file_like, block_size):

    This uses pkg_resources.resource_filename which is not the
    recommended way, since it extracts the files.

    I think this works fine unless you have some _very_ serious
    requirements for static content, in which case you probably
    shouldn't be serving it through a WSGI app, IMHO. YMMV.
        if (path.abspath(full_path) + path.sep)\
                .startswith(path.abspath(self.root) + path.sep):
            return True
        else:
            return False

def _match_magic(self, full_path):
        full_path = self.root + path_info
        if path.exists(full_path):
            return full_path
        else:
            for magic in self.magics:
                if path.exists(magic.new_path(full_path)):
                    return magic.new_path(full_path)
            else:
                return full_path

def _guess_type(self, full_path):
        magic = self._match_magic(full_path)
        if magic is not None:
            return magic.conditions(full_path, environ)
        else:
            mtime = stat(full_path).st_mtime
            return str(mtime), rfc822.formatdate(mtime)

def _file_like(self, full_path):
        if self.matches(full_path):
            return full_path[:-len(self.extension)]
        else:
            raise MagicError("Path does not match this magic.")

def body(self, environ, file_like):
        variables = environ.copy()
        variables.update(self.variables)
        template = string.Template(file_like.read())
        if self.safe is True:
            return [template.safe_substitute(variables)]
        else:
            return [template.substitute(variables)]

def get_rate_for(self, currency: str, to: str, reverse: bool=False) -> Number:
        rate = self.get_rate_for(currency, to, reverse)
        if self.return_decimal:
            amount = Decimal(amount)
        return amount * rate

def convert_money(self, money: Money, to: str, reverse: bool=False) -> Money:
        
        if ref not in self.widgets:   
            widget = IconWidget(screen=self, ref=ref, x=x, y=y, name=name)
            self.widgets[ref] = widget
            return self.widgets[ref]

def add_scroller_widget(self, ref, left=1, top=1, right=20, bottom=1, direction="h", speed=1, text="Message"):     
        

        .. note::

            If fails this falls back to a restricted interface, which can only be used by approved apps.

        :rtype: str

        :rtype: datetime
        Use this context manager to add arguments to an argparse object with the add_argument

        method. Arguments must be defined before the command is defined. Note that
        no-clean and resume are added upon exit and should not be added in the context manager. For

        more info about these default arguments see below.
    A wrapper for boost_ranks.

    :param dict rsem_files: Dict of results from rsem
    :param dict merged_mhc_calls: Dict of results from merging mhc peptide binding predictions
    :param dict transgene_out: Dict of results from running Transgene
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict rankboost_options: Options specific to rankboost
    :return: Dict of concise and detailed results for mhci and mhcii
             output_files:
                |- 'mhcii_rankboost_concise_results.tsv': fsID
                |- 'mhcii_rankboost_detailed_results.txt': fsID
                |- 'mhci_rankboost_concise_results.tsv': fsID
                +- 'mhci_rankboost_detailed_results.txt': fsID
    :rtype: dict
        # Convert paths to list because Python's _NamespacePath doesn't support
        # indexing.
        paths = list(getattr(module, '__path__', []))
        if len(paths) != 1:
            filename = getattr(module, '__file__', None)
            if filename is not None:
                paths = [os.path.dirname(filename)]
            else:
                # For unknown reasons, sometimes the list returned by __path__
                # contains duplicates that must be removed.
                paths = list(set(paths))
        if len(paths) > 1:
            raise ImproperlyConfigured(
                "The bot module %r has multiple filesystem locations (%r); "
                "you must configure this bot with an AppConfig subclass "
                "with a 'path' class attribute." % (module, paths))
        elif not paths:
            raise ImproperlyConfigured(
                "The bot module %r has no filesystem location, "
                "you must configure this bot with an AppConfig subclass "
                "with a 'path' class attribute." % (module,))
        return paths[0]

def create(cls, entry):
        # trading_bots.example.bot.ExampleBot
        try:
            # If import_module succeeds, entry is a path to a bot module,

            # which may specify a bot class with a default_bot attribute.
            # Otherwise, entry is a path to a bot class or an error.
            module = import_module(entry)

        except ImportError:
            # Track that importing as a bot module failed. If importing as a
            # bot class fails too, we'll trigger the ImportError again.
            module = None

            mod_path, _, cls_name = entry.rpartition('.')

            # Raise the original exception when entry cannot be a path to an
            # bot config class.
            if not mod_path:
                raise

        else:
            try:
                # If this works, the bot module specifies a bot class.

                entry = module.default_bot
            except AttributeError:

                # Otherwise, it simply uses the default bot registry class.
                return cls(f'{entry}.Bot', module)
            else:
                mod_path, _, cls_name = entry.rpartition('.')

        # If we're reaching this point, we must attempt to load the bot
        # class located at <mod_path>.<cls_name>
        mod = import_module(mod_path)
        try:
            bot_cls = getattr(mod, cls_name)
        except AttributeError:
            if module is None:
                # If importing as an bot module failed, that error probably
                # contains the most informative traceback. Trigger it again.
                import_module(entry)
            raise

        # Check for obvious errors. (This check prevents duck typing, but
        # it could be removed if it became a problem in practice.)
        if not issubclass(bot_cls, Bot):
            raise ImproperlyConfigured(
                "'%s' isn't a subclass of Bot." % entry)

        # Entry is a path to an bot config class.
        return cls(entry, mod, bot_cls.label)

def get_config(self, config_name, require_ready=True):
        if require_ready:
            self.bots.check_configs_ready()
        else:
            self.bots.check_bots_ready()

        return self.configs.get(config_name.lower(), {})

def get_configs(self):
        self.bots.check_models_ready()
        for config in self.configs.values():
            yield config

def populate(self, installed_bots=None):
        if self.ready:
            return

        # populate() might be called by two threads in parallel on servers
        # that create threads before initializing the WSGI callable.
        with self._lock:
            if self.ready:
                return

            # An RLock prevents other threads from entering this section. The
            # compare and set operation below is atomic.
            if self.loading:
                # Prevent re-entrant calls to avoid running AppConfig.ready()
                # methods twice.
                raise RuntimeError("populate() isn't re-entrant")
            self.loading = True

            # Phase 1: Initialize bots
            for entry in installed_bots or {}:
                if isinstance(entry, Bot):
                    cls = entry
                    entry = '.'.join([cls.__module__, cls.__name__])
                bot_reg = BotRegistry.create(entry)
                if bot_reg.label in self.bots:
                    raise ImproperlyConfigured(
                        "Bot labels aren't unique, "
                        "duplicates: %s" % bot_reg.label)

                self.bots[bot_reg.label] = bot_reg
                bot_reg.bots = self

            # Check for duplicate bot names.
            counts = Counter(
                bot_reg.name for bot_reg in self.bots.values())
            duplicates = [
                name for name, count in counts.most_common() if count > 1]
            if duplicates:
                raise ImproperlyConfigured(
                    "Bot names aren't unique, "
                    "duplicates: %s" % ", ".join(duplicates))

            self.bots_ready = True

            # Phase 2: import config files
            for bot in self.bots.values():
                bot.import_configs()

            self.configs_ready = True

            self.ready = True

def get_bot(self, bot_label):
        self.check_bots_ready()
        try:
            return self.bots[bot_label]
        except KeyError:
            message = "No installed bot with label '%s'." % bot_label
            for bot_cls in self.get_bots():
                if bot_cls.name == bot_label:
                    message += " Did you mean '%s'?" % bot_cls.label
                    break
            raise LookupError(message)

def get_configs(self):
        self.check_configs_ready()

        result = []
        for bot in self.bots.values():
            result.extend(list(bot.get_models()))
        return result

def get_config(self, bot_label, config_name=None, require_ready=True):
        if require_ready:
            self.check_configs_ready()
        else:
            self.check_bots_ready()

        if config_name is None:

            config_name = defaults.BOT_CONFIG

        bot = self.get_bot(bot_label)

        if not require_ready and bot.configs is None:
            bot.import_configs()

        return bot.get_config(config_name, require_ready=require_ready)

def __to_float(val, digits):
    final_result = {SUCCESS: False,
                    MESSAGE: None,
                    CONTENT: None,
                    RAINCONTENT: None}

    log.info("Getting buienradar json data for latitude=%s, longitude=%s",
             latitude, longitude)
    result = __get_ws_data()

    if result[SUCCESS]:
        # store json data:
        final_result[CONTENT] = result[CONTENT]
        final_result[SUCCESS] = True
    else:
        if STATUS_CODE in result and MESSAGE in result:
            msg = "Status: %d, Msg: %s" % (result[STATUS_CODE],
                                           result[MESSAGE])
        elif MESSAGE in result:
            msg = "Msg: %s" % (result[MESSAGE])
        else:
            msg = "Something went wrong (reason unknown)."

        log.warning(msg)
        final_result[MESSAGE] = msg

    # load forecasted precipitation:
    result = __get_precipfc_data(latitude,
                                 longitude)
    if result[SUCCESS]:
        final_result[RAINCONTENT] = result[CONTENT]
    else:
        if STATUS_CODE in result and MESSAGE in result:
            msg = "Status: %d, Msg: %s" % (result[STATUS_CODE],
                                           result[MESSAGE])
        elif MESSAGE in result:
            msg = "Msg: %s" % (result[MESSAGE])
        else:
            msg = "Something went wrong (reason unknown)."

        log.warning(msg)
        final_result[MESSAGE] = msg

    return final_result

def __get_precipfc_data(latitude, longitude):
    log.info("Retrieving  weather data (%s)...", url)
    result = {SUCCESS: False, MESSAGE: None}
    try:
        r = requests.get(url)
        result[STATUS_CODE] = r.status_code
        result[HEADERS] = r.headers
        result[CONTENT] = r.text
        if (200 == r.status_code):
            result[SUCCESS] = True
        else:
            result[MESSAGE] = "Got http statuscode: %d." % (r.status_code)
        return result
    except requests.RequestException as ose:
        result[MESSAGE] = 'Error getting url data. %s' % ose
        log.error(result[MESSAGE])

    return result

def __parse_ws_data(jsondata, latitude=52.091579, longitude=5.119734):
    result[DATA] = {ATTRIBUTION: ATTRIBUTION_INFO,
                    FORECAST: [],
                    PRECIPITATION_FORECAST: None}

    for key, [value, func] in SENSOR_TYPES.items():
        result[DATA][key] = None
        try:
            sens_data = loc_data[value]
            if key == CONDITION:
                # update weather symbol & status text
                desc = loc_data[__WEATHERDESCRIPTION]
                result[DATA][CONDITION] = __cond_from_desc(desc)
                result[DATA][CONDITION][IMAGE] = loc_data[__ICONURL]
                continue
            if key == STATIONNAME:
                result[DATA][key] = __getStationName(loc_data[__STATIONNAME],
                                                     loc_data[__STATIONID])
                continue
            # update all other data:
            if func is not None:
                result[DATA][key] = func(sens_data)
            else:
                result[DATA][key] = sens_data
        except KeyError:
            if result[MESSAGE] is None:
                result[MESSAGE] = "Missing key(s) in br data: "
            result[MESSAGE] += "%s " % value
            log.warning("Data element with key='%s' "
                        "not loaded from br data!", key)
    result[SUCCESS] = True
    return result

def __parse_fc_data(fc_data):
    try:
        return float(section[name])
    except (ValueError, TypeError, KeyError):
        return float(0)

def __parse_precipfc_data(data, timeframe):
    # '{ 'code': 'conditon', 'detailed', 'exact', 'exact_nl'}
    for code, [condition, detailed, exact, exact_nl] in __BRCONDITIONS.items():
        if exact_nl == desc:
            return {CONDCODE: code,
                    CONDITION: condition,
                    DETAILED: detailed,
                    EXACT: exact,
                    EXACTNL: exact_nl
                    }
    return None

def __get_ws_distance(wstation, latitude, longitude):
    if wstation:
        try:
            wslat = float(wstation[__LAT])
            wslon = float(wstation[__LON])

            dist = vincenty((latitude, longitude), (wslat, wslon))
            log.debug("calc distance: %s (latitude: %s, longitude: "
                      "%s, wslat: %s, wslon: %s)", dist, latitude,
                      longitude, wslat, wslon)
            return dist
        except (ValueError, TypeError, KeyError):
            # value does not exist, or is not a float
            return None
    else:
        return None

def __getStationName(name, id):
        This method is used within urls.py to create unique formwizard
        instances for every request. We need to override this method because
        we add some kwargs which are needed to make the formwizard usable.
        Creates a dict with all needed parameters for the form wizard instances.

        * `form_list` - is a list of forms. The list entries can be single form
          classes or tuples of (`step_name`, `form_class`). If you pass a list
          of forms, the formwizard will convert the class list to
          (`zero_based_counter`, `form_class`). This is needed to access the
          form for a specific step.
        * `initial_dict` - contains a dictionary of initial data dictionaries.
          The key should be equal to the `step_name` in the `form_list` (or
          the str of the zero based counter - if no step_names added in the
          `form_list`)
        * `instance_dict` - contains a dictionary of instance objects. This list
          is only used when `ModelForm`s are used. The key should be equal to
          the `step_name` in the `form_list`. Same rules as for `initial_dict`
          apply.
        * `condition_dict` - contains a dictionary of boolean values or
          callables. If the value of for a specific `step_name` is callable it
          will be called with the formwizard instance as the only argument.
          If the return value is true, the step's form will be used.
        This method gets called by the routing engine. The first argument is
        `request` which contains a `HttpRequest` instance.
        The request is stored in `self.request` for later use. The storage
        instance is stored in `self.storage`.

        After processing the request using the `dispatch` method, the
        response gets updated by the storage engine (for example add cookies).
        This method handles GET requests.

        If a GET request reaches this point, the wizard assumes that the user
        just starts at the first step or wants to restart the process.
        The data of the wizard will be resetted before rendering the first step.
        This method handles POST requests.

        The wizard will render either the current step (if form validation
        wasn't successful), the next step (if the current step was stored
        successful) or the done view (if no more steps are available)
        This method gets called when all forms passed. The method should also
        re-validate all steps to prevent manipulation. If any form don't
        validate, `render_revalidation_failure` should get called.
        If everything is fine call `done`.
        Returns the prefix which will be used when calling the actual form for
        the given step. `step` contains the step-name, `form` the form which
        will be called with the returned prefix.

        If no step is given, the form_prefix will determine the current step
        automatically.

        Constructs the form for a given `step`. If no `step` is defined, the
        current step will be determined automatically.

        The form will be initialized using the `data` argument to prefill the
        new form. If needed, instance or queryset (for `ModelForm` or
        `ModelFormSet`) will be added too.
        Gets called when a form doesn't validate when rendering the done

        view. By default, it changed the current step to failing forms step
        and renders the form.
        Returns a merged dictionary of all step cleaned_data dictionaries.
        If a step contains a `FormSet`, the key will be prefixed with formset
        and contain a list of the formset' cleaned_data dictionaries.
        Returns the cleaned data for a given `step`. Before returning the
        cleaned data, the stored values are being revalidated through the
        form. If the data doesn't validate, None will be returned.
        Returns the index for the given `step` name. If no step is given,
        the current step will be used to get the index.
        Returns a ``HttpResponse`` containing a all needed context data.
        We require a url_name to reverse URLs later. Additionally users can
        pass a done_step_name to change the URL name of the "done" view.
        This renders the form or, if needed, does the http redirects.
        Do a redirect if user presses the prev. step button. The rest of this
        is super'd from FormWizard.
        When using the NamedUrlFormWizard, we have to redirect to update the
        browser's URL to match the shown step.
        When a step fails, we have to redirect the user to the first failing
        step.
    from trading_bots.conf import settings
    store_settings = settings.storage
    store = store_settings.get('name', 'json')
    if store == 'json':
        store = 'trading_bots.core.storage.JSONStore'
    elif store == 'redis':
        store = 'trading_bots.core.storage.RedisStore'
    store_cls = load_class_by_name(store)
    kwargs = store_cls.configure(store_settings)
    return store_cls(logger=logger, **kwargs)

def parse_request_headers(headers):
    request_header_keys = set(headers.keys(lower=True))
    request_meta_keys = set(XHEADERS_TO_ARGS_DICT.keys())
    data_header_keys = request_header_keys.intersection(request_meta_keys)
    return dict(([XHEADERS_TO_ARGS_DICT[key],
                  headers.get(key, None)] for key in data_header_keys))

def split_docstring(docstring):
    docstring_list = [line.strip() for line in docstring.splitlines()]
    description_list = list(
        takewhile(lambda line: not (line.startswith(':') or
                                    line.startswith('@inherit')), docstring_list))
    description = ' '.join(description_list).strip()
    first_field_line_number = len(description_list)

    fields = []
    if first_field_line_number >= len(docstring_list):
        return description, fields  # only description, without any field
    last_field_lines = [docstring_list[first_field_line_number]]

    for line in docstring_list[first_field_line_number + 1:]:
        if line.strip().startswith(':') or line.strip().startswith('@inherit'):
            fields.append(' '.join(last_field_lines))
            last_field_lines = [line]
        else:
            last_field_lines.append(line)

    fields.append(' '.join(last_field_lines))
    return description, fields

def get_method_docstring(cls, method_name):
    method = getattr(cls, method_name, None)
    if method is None:
        return
    docstrign = inspect.getdoc(method)
    if docstrign is None:
        for base in cls.__bases__:
            docstrign = get_method_docstring(base, method_name)
            if docstrign:
                return docstrign
        else:
            return None

    return docstrign

def condition_from_code(condcode):
        Validates a submission file

        :param file_path: path to file to be loaded.
        :param data: pre loaded YAML object (optional).
        :return: Bool to indicate the validity of the file.
    mod_path, _, cls_name = name.rpartition('.')
    mod = importlib.import_module(mod_path)
    cls = getattr(mod, cls_name)
    return cls

def load_yaml_file(file_path: str):
    A wrapper for assess_itx_resistance.

    :param dict rsem_files: Results from running rsem
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict reports_options: Options specific to reporting modules
    :return: The results of running assess_itx_resistance
    :rtype: toil.fileStore.FileID

        # allow specify directly
        configured = get('CELERY_RESULT_BACKEND', None)
        if configured:
            return configured

        if not self._redis_available():
            return None

        host, port = self.REDIS_HOST, self.REDIS_PORT

        if host and port:

            default = "redis://{host}:{port}/{db}".format(
                    host=host, port=port,
                    db=self.CELERY_REDIS_RESULT_DB)


        return default

def BROKER_TYPE(self):

        # also allow specify broker_url
        broker_url = get('BROKER_URL', None)
        if broker_url:
            log.info("Using BROKER_URL setting: {}".format(broker_url))
            return broker_url

        redis_available = self._redis_available()
        broker_type = self.BROKER_TYPE

        if broker_type == 'redis' and not redis_available:
            log.warn("Choosed broker type is redis, but redis not available. \
                    Check redis package, and REDIS_HOST, REDIS_PORT settings")

        if broker_type == 'redis' and redis_available:

            return 'redis://{host}:{port}/{db}'.format(
                    host=self.REDIS_HOST,
                    port=self.REDIS_PORT,
                    db=self.CELERY_REDIS_BROKER_DB)
        elif broker_type == 'rabbitmq':
            return 'amqp://{user}:{passwd}@{host}:{port}/{vhost}'.format(
                    user=self.RABBITMQ_USER,
                    passwd=self.RABBITMQ_PASSWD,
                    host=self.RABBITMQ_HOST,
                    port=self.RABBITMQ_PORT,
                    vhost=self.RABBITMQ_VHOST)
        else:
            return DEFAULT_BROKER_URL

def traverse_inventory(self, item_filter=None):
        not self._intentory_raw and self._get_inventory_raw()

        for item in self._intentory_raw['rgDescriptions'].values():
            tags = item['tags']
            for tag in tags:
                internal_name = tag['internal_name']
                if item_filter is None or internal_name == item_filter:

                    item_type = Item
                    if internal_name == TAG_ITEM_CLASS_CARD:
                        item_type = Card

                    appid = item['market_fee_app']
                    title = item['name']

                    yield item_type(appid, title)

def validate(self, **kwargs):


        default_data_schema = json.load(open(self.default_schema_file, 'r'))

        # even though we are using the yaml package to load,
        # it supports JSON and YAML
        data = kwargs.pop("data", None)
        file_path = kwargs.pop("file_path", None)

        if file_path is None:
            raise LookupError("file_path argument must be supplied")

        if data is None:

            try:
                data = yaml.load(open(file_path, 'r'), Loader=Loader)
            except Exception as e:
                self.add_validation_message(ValidationMessage(file=file_path, message=
                'There was a problem parsing the file.\n' + e.__str__()))
                return False

        try:

            if 'type' in data:
                custom_schema = self.load_custom_schema(data['type'])
                json_validate(data, custom_schema)
            else:

                json_validate(data, default_data_schema)

        except ValidationError as ve:

            self.add_validation_message(
                ValidationMessage(file=file_path,
                                    message=ve.message + ' in ' + str(ve.instance)))

        if self.has_errors(file_path):
            return False
        else:
            return True

def b(s):
        validatedfeatures = []
        for feature in features:
            if isinstance(feature, int) or isinstance(feature, float):
                validatedfeatures.append( str(feature) )
            elif self.delimiter in feature and not self.sklearn:
                raise ValueError("Feature contains delimiter: " + feature)
            elif self.sklearn and isinstance(feature, str): #then is sparse added together
                validatedfeatures.append(feature)
            else:
                validatedfeatures.append(feature)
        return validatedfeatures

def addinstance(self, testfile, features, classlabel="?"):
        options = "-F " + self.format + " " +  self.timbloptions + " -t cross_validate"
        print("Instantiating Timbl API : " + options,file=stderr)
        if sys.version < '3':
            self.api = timblapi.TimblAPI(b(options), b"")
        else:
            self.api = timblapi.TimblAPI(options, "")
        if self.debug:
            print("Enabling debug for timblapi",file=stderr)
            self.api.enableDebug()
        print("Calling Timbl Test : " + options,file=stderr)
        if sys.version < '3':
            self.api.test(b(foldsfile),b'',b'')
        else:
            self.api.test(u(foldsfile),'','')
        a = self.api.getAccuracy()
        del self.api
        return a

def leaveoneout(self):


        .. note:: The action is saved only if a cache system is defined.

        :param action_key: The unique action name.
        :param data: The action to be saved.


        .. note:: It returns the action if a cache system is defined.

        :param action_key: The unique action name.
        :returns: The action stored in cache or ``None``.


        .. note:: It returns the action if a cache system is defined.

        :param action_key: The unique action name.

        .. note:: A action can't be registered two times. If it happens, then
        an assert exception will be raised.

        :param action: The action to be registered.

        .. note:: A system role can't be registered two times. If it happens,
        then an assert exception will be raised.

        :param system_role: The system role to be registered.

        :param entry_point_group: The entrypoint for extensions.
    args = docopt(__doc__, argv=argv,
                  version=pkg_resources.require('buienradar')[0].version)

    level = logging.ERROR
    if args['-v']:
        level = logging.INFO
    if args['-v'] == 2:
        level = logging.DEBUG
    logging.basicConfig(level=level)

    log = logging.getLogger(__name__)
    log.info("Start...")

    latitude = float(args['--latitude'])
    longitude = float(args['--longitude'])
    timeframe = int(args['--timeframe'])

    usexml = False
    if args['--usexml']:
        usexml = True

    result = get_data(latitude, longitude, usexml)
    if result[SUCCESS]:
        log.debug("Retrieved data:\n%s", result)

        result = parse_data(result[CONTENT], result[RAINCONTENT],
                            latitude, longitude, timeframe, usexml)
        log.info("result: %s", result)
        print(result)
    else:
        log.error("Retrieving weather data was not successfull (%s)",
                  result[MESSAGE])

def global_unlock_percent(self):
        percent = CRef.cfloat()
        result = self._iface.get_ach_progress(self.name, percent)

        if not result:
            return 0.0

        return float(percent)

def unlocked(self):
        achieved = CRef.cbool()
        result = self._iface.get_ach(self.name, achieved)

        if not result:
            return False

        return bool(achieved)

def unlock(self, store=True):
        result = self._iface.ach_unlock(self.name)
        result and store and self._store()
        return result

def __parse_ws_data(content, latitude=52.091579, longitude=5.119734):
    result[DATA] = {ATTRIBUTION: ATTRIBUTION_INFO,
                    FORECAST: [],
                    PRECIPITATION_FORECAST: None}

    for key, [value, func] in SENSOR_TYPES.items():
        result[DATA][key] = None
        try:
            from buienradar.buienradar import condition_from_code
            sens_data = loc_data[value]
            if key == CONDITION:
                # update weather symbol & status text
                code = sens_data[__BRID][:1]
                result[DATA][CONDITION] = condition_from_code(code)
                result[DATA][CONDITION][IMAGE] = sens_data[__BRTEXT]
            else:
                if key == STATIONNAME:
                    name = sens_data[__BRTEXT].replace("Meetstation", "")
                    name = name.strip()
                    name += " (%s)" % loc_data[__BRSTATIONCODE]
                    result[DATA][key] = name
                else:
                    # update all other data
                    if func is not None:
                        result[DATA][key] = func(sens_data)
                    else:
                        result[DATA][key] = sens_data
        except KeyError:
            if result[MESSAGE] is None:
                result[MESSAGE] = "Missing key(s) in br data: "
            result[MESSAGE] += "%s " % value
            log.warning("Data element with key='%s' "
                        "not loaded from br data!", key)
    result[SUCCESS] = True
    return result

def __parse_fc_data(fc_data):

    wstation: weerstation section of buienradar xml (dict)
    latitude: our latitude
    longitude: our longitude
    Predict binding for each peptide in `peptfile` to `allele` using the IEDB mhcii binding
    prediction tool.

    :param toil.fileStore.FileID peptfile: The input peptide fasta
    :param str allele: Allele to predict binding against
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict mhcii_options: Options specific to mhcii binding prediction
    :return: tuple of fsID for file containing the predictions and the predictor used
    :rtype: tuple(toil.fileStore.FileID, str|None)
    Predict binding for each peptide in `peptfile` to `allele` using netMHCIIpan.

    :param toil.fileStore.FileID peptfile: The input peptide fasta
    :param str allele: Allele to predict binding against
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict netmhciipan_options: Options specific to netmhciipan binding prediction
    :return: tuple of fsID for file containing the predictions and the predictor used (netMHCIIpan)
    :rtype: tuple(toil.fileStore.FileID, str)
        self.needs.update(permission.needs)
        self.excludes.update(permission.excludes)

def _load_permissions(self):
    @wraps(f)

    def decorated(ctx, param, value):
        return LocalProxy(lambda: f(ctx, param, value))
    return decorated

def process_action(ctx, param, value):
    user = User.query.filter(User.email == value).first()
    if not user:
        raise click.BadParameter('User with email \'%s\' not found.', value)
    return user

def process_role(ctx, param, value):

    def processor(action, argument):
        db.session.add(
            ActionUsers.allow(action, argument=argument, user_id=user.id)
        )
    return processor

def allow_role(role):
    for processor in processors:
        processor(action, argument)
    db.session.commit()

def deny_user(user):

    def processor(action, argument):
        db.session.add(
            ActionRoles.deny(action, argument=argument, role_id=role.id)
        )
    return processor

def process_deny_action(processors, action, argument):

    def processor(action, argument):
        ActionUsers.query_by_action(action, argument=argument).filter(
            ActionUsers.user_id.is_(None)
        ).delete(synchronize_session=False)
    return processor

def remove_user(user):

    def processor(action, argument):
        ActionRoles.query_by_action(action, argument=argument).filter(
            ActionRoles.role_id == role.id
        ).delete(synchronize_session=False)
    return processor

def process_remove_action(processors, action, argument):
    for name, action in _current_actions.items():
        click.echo('{0}:{1}'.format(
            name, '*' if hasattr(action, 'argument') else ''
        ))

def show_actions(email, role):
    A wrapper for assess_mhc_genes.

    :param dict rsem_files: Results from running rsem
    :param str rna_haplotype: The job store id for the rna haplotype file
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict reports_options: Options specific to reporting modules
    :return: The results of running assess_mhc_genes
    :rtype: toil.fileStore.FileID
    This module will parse the config file withing params and set up the variables that will be
    passed to the various tools in the pipeline.

    ARGUMENTS
    config_file: string containing path to a config file.  An example config
                 file is available at
                        https://s3-us-west-2.amazonaws.com/pimmuno-references
                        /input_parameters.list

    RETURN VALUES
    None
    This module runs cutadapt on the input RNA fastq files and then calls the RNA aligners.

    ARGUMENTS
    1. fastqs: Dict of list of input RNA-Seq fastqs
         fastqs
            +- 'tumor_rna': [<JSid for 1.fastq> , <JSid for 2.fastq>]
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
              +- 'dockerhub': <dockerhub to use>
    3. cutadapt_options: Dict of parameters specific to cutadapt
         cutadapt_options
              |- 'a': <sequence of 3' adapter to trim from fwd read>
              +- 'A': <sequence of 3' adapter to trim from rev read>
    RETURN VALUES
    1. output_files: Dict of cutadapted fastqs
         output_files
             |- 'rna_cutadapt_1.fastq': <JSid>
             +- 'rna_cutadapt_2.fastq': <JSid>

    This module corresponds to node 2 on the tree
    This module uses STAR to align the RNA fastqs to the reference

    ARGUMENTS
    1. fastqs: REFER RETURN VALUE of run_cutadapt()
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
              +- 'dockerhub': <dockerhub to use>
    3. star_options: Dict of parameters specific to STAR
         star_options
             |- 'index_tar': <JSid for the STAR index tarball>
             +- 'n': <number of threads to allocate>
    RETURN VALUES
    1. output_files: Dict of aligned bams
         output_files
             |- 'rnaAligned.toTranscriptome.out.bam': <JSid>
             +- 'rnaAligned.sortedByCoord.out.bam': Dict of genome bam + bai
                                |- 'rna_fix_pg_sorted.bam': <JSid>
                                +- 'rna_fix_pg_sorted.bam.bai': <JSid>

    This module corresponds to node 9 on the tree
    This module aligns the SAMPLE_TYPE dna fastqs to the reference

    ARGUMENTS -- <ST> depicts the sample type. Substitute with 'tumor'/'normal'
    1. fastqs: Dict of list of input WGS/WXS fastqs
         fastqs
              +- '<ST>_dna': [<JSid for 1.fastq> , <JSid for 2.fastq>]
    2. sample_type: string of 'tumor_dna' or 'normal_dna'
    3. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    4. bwa_options: Dict of parameters specific to bwa
         bwa_options
              |- 'index_tar': <JSid for the bwa index tarball>
              +- 'n': <number of threads to allocate>

    RETURN VALUES
    1. output_files: Dict of aligned bam + reference (nested return)
         output_files
             |- '<ST>_fix_pg_sorted.bam': <JSid>
             +- '<ST>_fix_pg_sorted.bam.bai': <JSid>

    This module corresponds to nodes 3 and 4 on the tree
    This module converts SAMFILE from sam to bam

    ARGUMENTS
    1. samfile: <JSid for a sam file>
    2. sample_type: string of 'tumor_dna' or 'normal_dna'
    3. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    RETURN VALUES
    1. output_files: REFER output_files in run_bwa()
    This module modified the header in BAMFILE

    ARGUMENTS
    1. bamfile: <JSid for a bam file>
    2. sample_type: string of 'tumor_dna' or 'normal_dna'
    3. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    RETURN VALUES
    1. output_files: REFER output_files in run_bwa()
    This module will run rsem on the RNA Bam file.

    ARGUMENTS
    1. star_bams: Dict of input STAR bams
         star_bams
              +- 'rnaAligned.toTranscriptome.out.bam': <JSid>
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    3. rsem_options: Dict of parameters specific to rsem
         rsem_options
              |- 'index_tar': <JSid for the rsem index tarball>
              +- 'n': <number of threads to allocate>

    RETURN VALUES
    1. output_file: <Jsid of rsem.isoforms.results>

    This module corresponds to node 9 on the tree
    This module will merge the per-chromosome radia files created by spawn_radia into a genome vcf.
    It will make 2 vcfs, one for PASSing non-germline calls, and one for all calls.

    ARGUMENTS
    1. perchrom_rvs: REFER RETURN VALUE of spawn_radia()

    RETURN VALUES
    1. output_files: Dict of outputs
            output_files
                |- radia_calls.vcf: <JSid>
                +- radia_parsed_filter_passing_calls.vcf: <JSid>

    This module corresponds to node 11 on the tree
    This module will run radia on the RNA and DNA bams

    ARGUMENTS
    1. bams: Dict of bams and their indexes
        bams
         |- 'tumor_rna': <JSid>
         |- 'tumor_rnai': <JSid>
         |- 'tumor_dna': <JSid>
         |- 'tumor_dnai': <JSid>
         |- 'normal_dna': <JSid>
         +- 'normal_dnai': <JSid>
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    3. radia_options: Dict of parameters specific to radia
         radia_options
              |- 'dbsnp_vcf': <JSid for dnsnp vcf file>
              +- 'genome': <JSid for genome fasta file>
    4. chrom: String containing chromosome name with chr appended

    RETURN VALUES
    1. Dict of filtered radia output vcf and logfile (Nested return)
        |- 'radia_filtered_CHROM.vcf': <JSid>
        +- 'radia_filtered_CHROM_radia.log': <JSid>
    This module will run filterradia on the RNA and DNA bams.

    ARGUMENTS
    1. bams: REFER ARGUMENTS of run_radia()
    2. univ_options: REFER ARGUMENTS of run_radia()
    3. radia_file: <JSid of vcf generated by run_radia()>
    3. radia_options: REFER ARGUMENTS of run_radia()
    4. chrom: REFER ARGUMENTS of run_radia()

    RETURN VALUES
    1. Dict of filtered radia output vcf and logfile
        |- 'radia_filtered_CHROM.vcf': <JSid>
        +- 'radia_filtered_CHROM_radia.log': <JSid>
    This module will merge the per-chromosome mutect files created by spawn_mutect into a genome
    vcf.  It will make 2 vcfs, one for PASSing non-germline calls, and one for all calls.

    ARGUMENTS
    1. perchrom_rvs: REFER RETURN VALUE of spawn_mutect()

    RETURN VALUES
    1. output_files: <JSid for mutect_passing_calls.vcf>

    This module corresponds to node 11 on the tree
    This module will run mutect on the DNA bams

    ARGUMENTS
    1. tumor_bam: REFER ARGUMENTS of spawn_mutect()
    2. normal_bam: REFER ARGUMENTS of spawn_mutect()
    3. univ_options: REFER ARGUMENTS of spawn_mutect()
    4. mutect_options: REFER ARGUMENTS of spawn_mutect()
    5. chrom: String containing chromosome name with chr appended

    RETURN VALUES
    1. output_files: Dict of results of mutect for chromosome
            output_files
              |- 'mutect_CHROM.vcf': <JSid>
              +- 'mutect_CHROM.out': <JSid>

    This module corresponds to node 12 on the tree
    This module will run an indel caller on the DNA bams.  This module will be
    implemented in the future.

    This module corresponds to node 13 on the tree
    This module will run a fusion caller on DNA bams.  This module will be
    implemented in the future.

    This module corresponds to node 10 on the tree
    This module will aggregate all the mutations called in the previous steps and will then call
    snpeff on the results.

    ARGUMENTS
    1. fusion_output: <JSid for vcf generated by the fusion caller>
    2. radia_output: <JSid for vcf generated by radia>
    3. mutect_output: <JSid for vcf generated by mutect>
    4. indel_output: <JSid for vcf generated by the indel caller>

    RETURN VALUES
    1. output_file: <JSid for merged vcf>

    This module corresponds to node 15 on the tree
    This module will run snpeff on the aggregated mutation calls.  Currently the only mutations
    called are SNPs hence SnpEff suffices. This node will be replaced in the future with another
    translator.

    ARGUMENTS
    1. merged_mutation_file: <JSid for merged vcf>
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    3. snpeff_options: Dict of parameters specific to snpeff
         snpeff_options
                +- 'index_tar': <JSid for the snpEff index tarball>

    RETURN VALUES
    1. output_file: <JSid for the snpeffed vcf>

    This node corresponds to node 16 on the tree
    This module will run transgene on the input vcf file from the aggregator and produce the
    peptides for MHC prediction

    ARGUMENTS
    1. snpeffed_file: <JSid for snpeffed vcf>
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    3. transgene_options: Dict of parameters specific to transgene
         transgene_options
                +- 'gencode_peptide_fasta': <JSid for the gencode protein fasta>

    RETURN VALUES
    1. output_files: Dict of transgened n-mer peptide fastas
         output_files
                |- 'transgened_tumor_9_mer_snpeffed.faa': <JSid>
                |- 'transgened_tumor_10_mer_snpeffed.faa': <JSid>
                +- 'transgened_tumor_15_mer_snpeffed.faa': <JSid>

    This module corresponds to node 17 on the tree
    This module will run PHLAT on SAMPLE_TYPE fastqs.

    ARGUMENTS -- <ST> depicts the sample type. Substitute with 'tumor_dna',
                 'normal_dna', or 'tumor_rna'
    1. fastqs: Dict of list of input WGS/WXS fastqs
         fastqs
              +- '<ST>': [<JSid for 1.fastq> , <JSid for 2.fastq>]
    2. sample_type: string of 'tumor' or 'normal'
    3. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                +- 'dockerhub': <dockerhub to use>
    4. phlat_options: Dict of parameters specific to phlat
         phlat_options
              |- 'index_tar': <JSid for the PHLAT index tarball>
              +- 'n': <number of threads to allocate>

    RETURN VALUES
    1. output_file: <JSid for the allele predictions for ST>

    This module corresponds to nodes 5, 6 and 7 on the tree
    This module will merge the results form running PHLAT on the 3 input fastq
    pairs.

    ARGUMENTS
    1. tumor_phlat: <JSid for tumor DNA called alleles>
    2. normal_phlat: <JSid for normal DNA called alleles>
    3. rna_phlat: <JSid for tumor RNA called alleles>

    RETURN VALUES
    1. output_files: Dict of JSids for consensus MHCI and MHCII alleles
             output_files
                    |- 'mhci_alleles.list': <JSid>
                    +- 'mhcii_alleles.list': <JSid>

    This module corresponds to node 14 on the tree
    This is the final module in the pipeline.  It will call the rank boosting R
    script.

    This module corresponds to node 21 in the tree
    This is adapted from John Vivian's return_input_paths from the RNA-Seq pipeline.

    Returns the paths of files from the FileStore if they are not present.
    If docker=True, return the docker path for the file.
    If the file extension is tar.gz, then tar -zxvf it.

    files is a dict with:
        keys = the name of the file to be returned in toil space
        value = the input value for the file (can be toil temp file)
    work_dir is the location where the file should be stored
    cache indiciates whether caching should be used
    This module will accept the vcf files for mutect and radia read into memory in a dict object
    VCF_FILE and will merge the calls.  Merged calls are printed to MERGED_MUT_FILE.

    VCF_FILE is a dict with
    key : mutation caller (mutect or radia)
    value : dict with
            key: (chrom, pos, ref, alt)
            value: vcf line in list form (split by tab)
    Makes subprocess call of a command to a docker container. work_dir MUST BE AN ABSOLUTE PATH or
    the call will fail.  outfile is an open file descriptor to a writeable file.
    This module accepts a tar.gz archive and untars it.

    RETURN VALUE: path to the untar-ed directory/file

    NOTE: this module expects the multiple files to be in a directory before
          being tar-ed.
    split an input bam to paired fastqs.

    ARGUMENTS
    1. bamfile: Path to a bam file
    2. univ_options: Dict of universal arguments used by almost all tools
         univ_options
                |- 'dockerhub': <dockerhub to use>
                +- 'java_Xmx': value for max heap passed to java
    This is the main function for the UCSC Precision Immuno pipeline.
    A wrapper for the the entire strelka sub-graph.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict strelka_options: Options specific to strelka
    :return: fsID to the merged strelka calls
    :rtype: toil.fileStore.FileID
    Run the strelka subgraph on the DNA bams.  Optionally split the results into per-chromosome
    vcfs.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict strelka_options: Options specific to strelka
    :param bool split: Should the results be split into perchrom vcfs?
    :return: Either the fsID to the genome-level vcf or a dict of results from running strelka
             on every chromosome
             perchrom_strelka:
                 |- 'chr1':
                 |      |-'snvs': fsID
                 |      +-'indels': fsID
                 |- 'chr2':
                 |      |-'snvs': fsID
                 |      +-'indels': fsID
                 |-...
                 |
                 +- 'chrM':
                        |-'snvs': fsID
                        +-'indels': fsID
    :rtype: toil.fileStore.FileID|dict
    Run strelka on the DNA bams.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict strelka_options: Options specific to strelka
    :return: Dict of fsIDs snv and indel prediction files
             output_dict:
                 |-'snvs': fsID
                 +-'indels': fsID
    :rtype: dict
    A wwrapper to unmerge the strelka snvs and indels

    :param dict strelka_out: Results from run_strelka
    :param list chromosomes: List of chromosomes to retain
    :param dict strelka_options: Options specific to strelka
    :param dict univ_options: Dict of universal options used by almost all tools
    :return: Dict of dicts containing the fsIDs for the per-chromosome snv and indel calls
             output:
               |- 'snvs':
               |      |- 'chr1': fsID
               |      |- 'chr2': fsID
               |      |- ...
               |      +- 'chrM': fsID
               +- 'indels':
                      |- 'chr1': fsID
                      |- 'chr2': fsID
                      |- ...
                      +- 'chrM': fsID
    :rtype: dict
    if isinstance(timestamp, (int, float)):
        maya_dt = maya.MayaDT(timestamp)
    elif isinstance(timestamp, str):
        maya_dt = maya.when(timestamp)
    elif timestamp is None:
        maya_dt = maya.now()
    else:
        raise ValueError(f'`{type(timestamp)}` is not supported')
    return maya_dt.iso8601()

def truncate(value: Decimal, n_digits: int) -> Decimal:
    decimal_places = DECIMALS.get(currency.upper(), 2)
    return truncate(value, decimal_places)

def truncate_money(money: Money) -> Money:
    upper = value * (1 + spread_p)
    lower = value / (1 + spread_p)
    return lower, upper

def spread_money(money: Money, spread_p: Decimal) -> Tuple[Money, Money]:
    Checks if the english date is in valid range for conversion
    Checks if the nepali date is in valid range for conversion
    Convert a number to nepali
        added.
    A wrapper for the the entire MuSE sub-graph.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict muse_options: Options specific to MuSE
    :return: fsID to the merged MuSE calls
    :rtype: toil.fileStore.FileID
    Spawn a MuSE job for each chromosome on the DNA bams.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict muse_options: Options specific to MuSE
    :return: Dict of results from running MuSE on every chromosome
             perchrom_muse:
                 |- 'chr1': fsID
                 |- 'chr2' fsID
                 |
                 |-...
                 |
                 +- 'chrM': fsID
    :rtype: dict
    Run MuSE call on a single chromosome in the input bams.

    :param dict tumor_bam: Dict of bam and bai for tumor DNA-Seq
    :param dict normal_bam: Dict of bam and bai for normal DNA-Seq
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict muse_options: Options specific to MuSE
    :param str chrom: Chromosome to process
    :return: fsID for the chromsome vcf
    :rtype: toil.fileStore.FileID
    Run MuSE sump on the MuSE call generated vcf.

    :param toil.fileStore.FileID muse_output: vcf generated by MuSE call
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict muse_options: Options specific to MuSE
    :param str chrom: Chromosome to process
    :return: fsID for the chromsome vcf
    :rtype: toil.fileStore.FileID
        start, stop = limits or (self.minval, self.maxval)
        return np.linspace(start, stop, k)

def quantiles(self, k=5):
        if not value:
            return []
        return map(super(CommaSepFloatField, self).to_python, value.split(','))

def run_validators(self, values):
        value = super(BoundingBoxField, self).to_python(value)
        try:
            bbox = gdal.OGRGeometry.from_bbox(value).geos
        except (ValueError, AttributeError):
            return []
        bbox.srid = self.srid
        return bbox

def run_mutect_with_merge(job, tumor_bam, normal_bam, univ_options, mutect_options):
    spawn = job.wrapJobFn(run_mutect, tumor_bam, normal_bam, univ_options,
                          mutect_options).encapsulate()
    merge = job.wrapJobFn(merge_perchrom_vcfs, spawn.rv())
    job.addChild(spawn)
    spawn.addChild(merge)
    return merge.rv()

def run_mutect(job, tumor_bam, normal_bam, univ_options, mutect_options):
    # Get a list of chromosomes to handle
    if mutect_options['chromosomes']:
        chromosomes = mutect_options['chromosomes']
    else:
        chromosomes = sample_chromosomes(job, mutect_options['genome_fai'])

    perchrom_mutect = defaultdict()
    for chrom in chromosomes:
        perchrom_mutect[chrom] = job.addChildJobFn(
            run_mutect_perchrom, tumor_bam, normal_bam, univ_options, mutect_options, chrom,
            memory='6G', disk=PromisedRequirement(mutect_disk,
                                                  tumor_bam['tumor_dna_fix_pg_sorted.bam'],
                                                  normal_bam['normal_dna_fix_pg_sorted.bam'],
                                                  mutect_options['genome_fasta'],
                                                  mutect_options['dbsnp_vcf'],
                                                  mutect_options['cosmic_vcf'])).rv()
    return perchrom_mutect

def run_mutect_perchrom(job, tumor_bam, normal_bam, univ_options, mutect_options, chrom):
    work_dir = os.getcwd()
    input_files = {
        'tumor.bam': tumor_bam['tumor_dna_fix_pg_sorted.bam'],
        'tumor.bam.bai': tumor_bam['tumor_dna_fix_pg_sorted.bam.bai'],
        'normal.bam': normal_bam['normal_dna_fix_pg_sorted.bam'],
        'normal.bam.bai': normal_bam['normal_dna_fix_pg_sorted.bam.bai'],
        'genome.fa.tar.gz': mutect_options['genome_fasta'],
        'genome.fa.fai.tar.gz': mutect_options['genome_fai'],
        'genome.dict.tar.gz': mutect_options['genome_dict'],
        'cosmic.vcf.tar.gz': mutect_options['cosmic_vcf'],
        'cosmic.vcf.idx.tar.gz': mutect_options['cosmic_idx'],
        'dbsnp.vcf.gz': mutect_options['dbsnp_vcf'],
        'dbsnp.vcf.idx.tar.gz': mutect_options['dbsnp_idx']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)
    # dbsnp.vcf should be bgzipped, but all others should be tar.gz'd
    input_files['dbsnp.vcf'] = gunzip(input_files['dbsnp.vcf.gz'])
    for key in ('genome.fa', 'genome.fa.fai', 'genome.dict', 'cosmic.vcf', 'cosmic.vcf.idx',
                'dbsnp.vcf.idx'):
        input_files[key] = untargz(input_files[key + '.tar.gz'], work_dir)
    input_files = {key: docker_path(path) for key, path in input_files.items()}

    mutout = ''.join([work_dir, '/', chrom, '.out'])
    mutvcf = ''.join([work_dir, '/', chrom, '.vcf'])
    parameters = ['-R', input_files['genome.fa'],
                  '--cosmic', input_files['cosmic.vcf'],
                  '--dbsnp', input_files['dbsnp.vcf'],
                  '--input_file:normal', input_files['normal.bam'],
                  '--input_file:tumor', input_files['tumor.bam'],
                  # '--tumor_lod', str(10),
                  # '--initial_tumor_lod', str(4.0),
                  '-L', chrom,
                  '--out', docker_path(mutout),
                  '--vcf', docker_path(mutvcf)
                  ]
    java_xmx = mutect_options['java_Xmx'] if mutect_options['java_Xmx'] \
        else univ_options['java_Xmx']
    docker_call(tool='mutect', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], java_xmx=java_xmx,
                tool_version=mutect_options['version'])
    output_file = job.fileStore.writeGlobalFile(mutvcf)
    export_results(job, output_file, mutvcf, univ_options, subfolder='mutations/mutect')
    job.fileStore.logToMaster('Ran MuTect on %s:%s successfully' % (univ_options['patient'], chrom))
    return output_file

def process_mutect_vcf(job, mutect_vcf, work_dir, univ_options):
    mutect_vcf = job.fileStore.readGlobalFile(mutect_vcf)

    with open(mutect_vcf, 'r') as infile, open(mutect_vcf + 'mutect_parsed.tmp', 'w') as outfile:
        for line in infile:
            line = line.strip()
            if line.startswith('#'):
                print(line, file=outfile)
                continue
            line = line.split('\t')
            if line[6] != 'REJECT':
                print('\t'.join(line), file=outfile)
    return outfile.name

def get_universe(self, as_str=False):
        result = self._iface.get_connected_universe()

        if as_str:
            return Universe.get_alias(result)

        return result

def EXTRA_LOGGING(self):

        input_text = get('EXTRA_LOGGING', '')
        modules = input_text.split(',')
        if input_text:
            modules = input_text.split(',')
            modules = [x.split(':') for x in modules]
        else:
            modules = []
        return modules

def from_ad_date(cls, date):
        return NepDate(year, month, day).update()

def events_list(self):
        functions.check_valid_bs_range(self)
        # Here's a trick to find the gregorian date:
        # We find the number of days from earliest nepali date to the current
        # day. We then add the number of days to the earliest english date
        self.en_date = values.START_EN_DATE + \
            (
                self - NepDate(
                    values.START_NP_YEAR,
                    1,
                    1
                )
            )
        return self

def get_file_from_s3(job, s3_url, encryption_key=None, write_to_jobstore=True):
    work_dir = job.fileStore.getLocalTempDir()
    filename = '/'.join([work_dir, os.path.basename(s3_url)])
    # This is common to encrypted and unencrypted downloads
    download_call = ['curl', '-fs', '--retry', '5']
    # If an encryption key was provided, use it to create teh headers that need to be injected into
    # the curl script and append to the call
    if encryption_key:
        key = generate_unique_key(encryption_key, s3_url)
        encoded_key = base64.b64encode(key)
        encoded_key_md5 = base64.b64encode( hashlib.md5(key).digest() )
        h1 = 'x-amz-server-side-encryption-customer-algorithm:AES256'
        h2 = 'x-amz-server-side-encryption-customer-key:{}'.format(encoded_key)
        h3 = 'x-amz-server-side-encryption-customer-key-md5:{}'.format(encoded_key_md5)
        download_call.extend(['-H', h1, '-H', h2, '-H', h3])
    # This is also common to both types of downloads
    download_call.extend([s3_url, '-o', filename])
    try:
        subprocess.check_call(download_call)
    except subprocess.CalledProcessError:
        raise RuntimeError('Curl returned a non-zero exit status processing %s. Do you' % s3_url +
                           'have premssions to access the file?')
    except OSError:
        raise RuntimeError('Failed to find "curl". Install via "apt-get install curl"')
    assert os.path.exists(filename)
    if write_to_jobstore:
        filename = job.fileStore.writeGlobalFile(filename)
    return filename

def filter_geometry(queryset, **filters):
    fieldname = geo_field(queryset).name
    query = {'%s__%s' % (fieldname, k): v for k, v in filters.items()}
    return queryset.filter(**query)

def geo_field(queryset):
    try:
        srid = list(six.viewvalues(queryset.query.annotations))[0].srid
    except (AttributeError, IndexError):
        srid = None
    return srid or geo_field(queryset).srid

def agg_dims(arr, stat):
    axis = None
    if arr.ndim > 2:
        axis = 1
        arr = arr.reshape(arr.shape[0], -1)
    module = np.ma if hasattr(arr, 'mask') else np
    return getattr(module, stat)(arr, axis)

def extent(self, srid=None):
        expr = self.geo_field.name
        if srid:
            expr = geofn.Transform(expr, srid)
        expr = models.Extent(expr)
        clone = self.all()
        name, val = clone.aggregate(expr).popitem()
        return val

def pbf(self, bbox, geo_col=None, scale=4096):
        col = geo_col or self.geo_field.name
        w, s, e, n = bbox.extent
        trans = self._trans_scale(col, -w, -s,
                                  scale / (e - w),
                                  scale / (n - s))
        g = AsText(trans)
        return self.annotate(pbf=g)

def tile(self, bbox, z=0, format=None, clip=True):
        # Tile grid uses 3857, but GeoJSON coordinates should be in 4326.
        tile_srid = 3857
        bbox = getattr(bbox, 'geos', bbox)
        clone = filter_geometry(self, intersects=bbox)
        field = clone.geo_field
        srid = field.srid
        sql = field.name
        try:
            tilew = self.tilewidths[z]
        except IndexError:
            tilew = self.tilewidths[-1]
        if bbox.srid != srid:
            bbox = bbox.transform(srid, clone=True)
        # Estimate tile width in degrees instead of meters.
        if bbox.srs.geographic:
            p = geos.Point(tilew, tilew, srid=tile_srid)
            p.transform(srid)
            tilew = p.x
        if clip:
            bufbox = bbox.buffer(tilew)
            sql = geofn.Intersection(sql, bufbox.envelope)
        sql = SimplifyPreserveTopology(sql, tilew)
        if format == 'pbf':
            return clone.pbf(bbox, geo_col=sql)
        sql = geofn.Transform(sql, 4326)
        return clone.annotate(**{format: sql})

def arrays(self, field_name=None):
        fieldname = field_name or self.raster_field.name
        arrays = []
        for obj in self:
            arr = getattr(obj, fieldname)
            if isinstance(arr, np.ndarray):
                arrays.append(arr)
            else:
                arrays.append(obj.array())
        return arrays

def aggregate_periods(self, periods):
        try:
            fieldname = self.raster_field.name
        except TypeError:
            raise exceptions.FieldDoesNotExist('Raster field not found')
        arrays = self.arrays(fieldname)
        arr = arrays[0]
        if len(arrays) > 1:
            if getattr(arr, 'ndim', 0) > 2:
                arrays = np.vstack(arrays)
            fill = getattr(arr, 'fill_value', None)
            arr = np.ma.masked_values(arrays, fill, copy=False)
        # Try to reshape using equal sizes first and fall back to unequal
        # splits.
        try:
            means = arr.reshape((periods, -1)).mean(axis=1)
        except ValueError:
            means = np.array([a.mean() for a in np.array_split(arr, periods)])
        obj = self[0]
        setattr(obj, fieldname, means)
        return [obj]

def raster_field(self):
        if path:
            fp = open(path, 'w+b')
        else:
            prefix = '%s-' % arcdirname
            fp = tempfile.NamedTemporaryFile(prefix=prefix, suffix='.zip')
        with zipfile.ZipFile(fp, mode='w') as zf:
            for obj in self:
                img = obj.image
                arcname = os.path.join(arcdirname, os.path.basename(img.name))
                try:
                    zf.write(img.path, arcname=arcname)
                except OSError:
                    img.seek(0)
                    zf.writestr(arcname, img.read())
                    img.close()
        fp.seek(0)
        zobj = self.model(image=fp)
        return [zobj]

def init(self, app_id=None):
        self.set_app_id(app_id)

        err_msg = (
            'Unable to initialize. Check Steam client is running '

            'and Steam application ID is defined in steam_appid.txt or passed to Api.'
        )

        if self._lib.steam_init():

            try:
                _set_client(self._lib.Client())

                self.utils = Utils()
                self.current_user = CurrentUser()
                self.friends = Friends()
                self.groups = Groups()
                self.apps = Applications()
                self.overlay = Overlay()
                self.screenshots = Screenshots()

            except Exception as e:
                raise SteamApiStartupError('%s:\n%s' % (err_msg, e))

        else:
            raise SteamApiStartupError(err_msg)

def get_files_from_filestore(job, files, work_dir, docker=False):
    for name in files.keys():
        outfile = job.fileStore.readGlobalFile(files[name], '/'.join([work_dir, name]))
        # If the files will be sent to docker, we will mount work_dir to the container as /data and
        # we want the /data prefixed path to the file
        if docker:
            files[name] = docker_path(outfile)
        else:
            files[name] = outfile
    return files

def gunzip(input_gzip_file, block_size=1024):
    assert os.path.splitext(input_gzip_file)[1] == '.gz'
    assert is_gzipfile(input_gzip_file)
    with gzip.open(input_gzip_file) as infile:
        with open(os.path.splitext(input_gzip_file)[0], 'w') as outfile:
            while True:
                block = infile.read(block_size)
                if block == '':
                    break
                else:
                    outfile.write(block)
    return outfile.name

def is_gzipfile(filename):
    assert os.path.exists(filename), 'Input {} does not '.format(filename) + \
        'point to a file.'
    with open(filename, 'rb') as in_f:
        start_of_file = in_f.read(3)
        if start_of_file == '\x1f\x8b\x08':
            return True
        else:
            return False

def get_file_from_gdc(job, gdc_url, gdc_download_token, write_to_jobstore=True):
    work_dir = job.fileStore.getLocalTempDir()

    parsed_url = urlparse(gdc_url)
    assert parsed_url.scheme == 'gdc', 'Unexpected url scheme: %s' % gdc_url

    file_dir = '/'.join([work_dir, parsed_url.netloc])

    # This is common to encrypted and unencrypted downloads
    currwd = os.getcwd()
    os.chdir(work_dir)
    try:
        download_call = ['gdc-client', 'download', '-t', gdc_download_token, parsed_url.netloc]
        subprocess.check_call(download_call)
    finally:
        os.chdir(currwd)

    assert os.path.exists(file_dir)
    output_files = [os.path.join(file_dir, x) for x in os.listdir(file_dir)
                    if not x.endswith('logs')]
    # NOTE: We only handle vcf and bam+bai
    if len(output_files) == 1:
        assert output_files[0].endswith('vcf')
    else:
        if not {os.path.splitext(x)[1] for x in output_files} >= {'.bam', '.bai'}:
            raise ParameterError('Can currently only handle pre-indexed GDC bams.')
        # Always [bam, bai]
        output_files = [x for x in output_files if x.endswith(('bam', 'bai'))]
        output_files = sorted(output_files, key=lambda x: os.path.splitext(x)[1], reverse=True)
    if write_to_jobstore:
        output_files = [job.fileStore.writeGlobalFile(f) for f in output_files]
    return output_files

def get_file_from_url(job, any_url, encryption_key=None, per_file_encryption=True,
                      write_to_jobstore=True):
    work_dir = job.fileStore.getLocalTempDir()

    filename = '/'.join([work_dir, str(uuid.uuid4())])
    url = any_url
    parsed_url = urlparse(any_url)
    try:
        response = urllib2.urlopen(url)
    except urllib2.HTTPError:
        if parsed_url.netloc.startswith(('s3', 'S3')):
            job.fileStore.logToMaster("Detected https link is for an encrypted s3 file.")
            return get_file_from_s3(job, any_url, encryption_key=encryption_key,
                                    per_file_encryption=per_file_encryption,
                                    write_to_jobstore=write_to_jobstore)
        else:
            raise
    else:
        with open(filename, 'w') as f:
            f.write(response.read())

    if write_to_jobstore:
        filename = job.fileStore.writeGlobalFile(filename)
    return filename

def bam2fastq(bamfile, univ_options, picard_options):
    work_dir = os.path.split(bamfile)[0]
    base_name = os.path.split(os.path.splitext(bamfile)[0])[1]
    parameters = ['SamToFastq',
                  ''.join(['I=', docker_path(bamfile)]),
                  ''.join(['F=/data/', base_name, '_1.fastq']),
                  ''.join(['F2=/data/', base_name, '_2.fastq']),
                  ''.join(['FU=/data/', base_name, '_UP.fastq'])]
    docker_call(tool='picard', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], java_xmx=univ_options['java_Xmx'],
                tool_version=picard_options['version'])
    first_fastq = ''.join([work_dir, '/', base_name, '_1.fastq'])
    assert os.path.exists(first_fastq)
    return first_fastq

def export_results(job, fsid, file_name, univ_options, subfolder=None):
    job.fileStore.logToMaster('Exporting %s to output location' % fsid)
    file_name = os.path.basename(file_name)
    try:
        assert univ_options['output_folder'], 'Need a path to a folder to write out files'
        assert univ_options['storage_location'], 'Need to know where the files need to go. ' + \
                                                 'Local or AWS/Azure, etc.'
    except AssertionError as err:
        # This isn't a game killer.  Continue the pipeline without erroring out but do inform the
        # user about it.
        print('ERROR:', err.message, file=sys.stderr)
        return
    if univ_options['output_folder'] == 'NA':
        output_folder = ''
    else:
        output_folder = univ_options['output_folder']
    output_folder = os.path.join(output_folder, univ_options['patient'])
    output_folder = os.path.join(output_folder, subfolder) if subfolder else output_folder
    if univ_options['storage_location'] == 'local':
        # Handle Local
        try:
            # Create the directory if required
            os.makedirs(output_folder, 0755)
        except OSError as err:
            if err.errno != errno.EEXIST:
                raise
        output_url = 'file://' + os.path.join(output_folder, file_name)
    elif univ_options['storage_location'].startswith('aws'):
        # Handle AWS
        bucket_name = univ_options['storage_location'].split(':')[-1]
        output_url = os.path.join('S3://', bucket_name, output_folder.strip('/'), file_name)
    # Can't do Azure or google yet.
    else:
        # TODO: Azure support
        print("Currently doesn't support anything but Local and aws.")
        return
    job.fileStore.exportFile(fsid, output_url)

def parse_chromosome_string(job, chromosome_string):
    if chromosome_string is None:
        return []
    else:
        assert isinstance(chromosome_string, str)
        chroms = [c.strip() for c in chromosome_string.split(',')]
        if 'canonical' in chroms:
            assert 'canonical_chr' not in chroms, 'Cannot have canonical and canonical_chr'
            chr_prefix = False
            chroms.remove('canonical')
            out_chroms = [str(c) for c in range(1, 23)] + ['X', 'Y']
        elif 'canonical_chr' in chroms:
            assert 'canonical' not in chroms, 'Cannot have canonical and canonical_chr'
            chr_prefix = True
            chroms.remove('canonical_chr')
            out_chroms = ['chr' + str(c) for c in range(1, 23)] + ['chrX', 'chrY']
        else:
            chr_prefix = None
            out_chroms = []
        for chrom in chroms:
            if chr_prefix is not None and chrom.startswith('chr') is not chr_prefix:
                job.fileStore.logToMaster('chromosome %s does not match the rest that %s begin '
                                          'with "chr".' % (chrom,
                                                           'all' if chr_prefix else 'don\'t'),
                                          level=logging.WARNING)
            out_chroms.append(chrom)
        return chrom_sorted(out_chroms)

def email_report(job, univ_options):
    fromadd = "results@protect.cgl.genomics.ucsc.edu"
    msg = MIMEMultipart()
    msg['From'] = fromadd
    if  univ_options['mail_to'] is None:
        return
    else:
        msg['To'] = univ_options['mail_to']
    msg['Subject'] = "Protect run for sample %s completed successfully." % univ_options['patient']
    body = "Protect run for sample %s completed successfully." % univ_options['patient']
    msg.attach(MIMEText(body, 'plain'))
    text = msg.as_string()

    try:
        server = smtplib.SMTP('localhost')
    except socket.error as e:
        if e.errno == 111:
            print('No mail utils on this maachine')
        else:
            print('Unexpected error while attempting to send an email.')
        print('Could not send email report')
    except:
        print('Could not send email report')
    else:
        server.sendmail(fromadd, msg['To'], text)
        server.quit()

def make_key_hippie(obj, typed=True):
    ftype = type if typed else lambda o: None
    if is_hashable(obj):
        ## DO NOT RETURN hash(obj), as hash collision would generate bad
        ## cache collisions.
        return obj, ftype(obj)
    ## should we try to convert to frozen{set,dict} to get the C
    ## hashing function speed ? But the convertion has a cost also.
    if isinstance(obj, set):
        obj = sorted(obj)
    if isinstance(obj, (list, tuple)):
        return tuple(make_key_hippie(e, typed) for e in obj)
    if isinstance(obj, dict):
        return tuple(sorted(((make_key_hippie(k, typed),
                              make_key_hippie(v, typed))
                             for k, v in obj.items())))
    raise ValueError(
        "%r can not be hashed. Try providing a custom key function."
        % obj)

def undecorate(func):
    ctx.obj['appid'] = appid
    ctx.obj['title'] = title

def get_price(ctx, currency):

    appid = ctx.obj['appid']
    app = Application(appid)

    click.secho('Cards for `%s` [appid: %s]' % (app.title, appid), fg='green')

    if not app.has_cards:
        click.secho('This app has no cards.', fg='red', err=True)
        return

    cards, booster = app.get_cards()


    def get_line(card):
        return '%s [market hash: `%s`]' % (card.title, card.market_hash)

    for card in cards.values():
        click.echo(get_line(card))

    if booster:
        click.secho('* Booster pack: `%s`' % get_line(booster), fg='yellow')

    click.secho('* Total cards: %d' % len(cards), fg='green')

def get_card_prices(ctx, currency):
    appid = ctx.obj['appid']

    detailed = True

    appids = [appid]
    if ',' in appid:
        appids = [appid.strip() for appid in appid.split(',')]
        detailed = False

    for appid in appids:
        print_card_prices(appid, currency, detailed=detailed)
        click.echo('')

def get_gems(ctx):

    username = ctx.obj['username']
    games = User(username).get_games_owned()

    for game in sorted(games.values(), key=itemgetter('title')):
        click.echo('%s [appid: %s]' % (game['title'], game['appid']))

    click.secho('Total gems owned by `%s`: %d' % (username, len(games)), fg='green')

def get_booster_stats(ctx, currency):

    username = ctx.obj['username']

    cards_by_app = defaultdict(list)

    inventory = User(username).traverse_inventory(item_filter=TAG_ITEM_CLASS_CARD)
    for item in inventory:
        appid_ = item.app.appid
        if not appid or appid_ in appid:
            cards_by_app[appid_].append(item)

    if not cards_by_app:
        click.secho('User `%s` has no cards' % username, fg='red', err=True)
        return

    for appid_, cards in cards_by_app.items():
        app = cards[0].app
        print_card_prices(
            app.appid, currency,
            owned_cards=[card.title for card in cards],
            skip_owned=skip_owned,
            foil=foil,
        )

def run_radia_with_merge(job, rna_bam, tumor_bam, normal_bam, univ_options, radia_options):
    spawn = job.wrapJobFn(run_radia, rna_bam['rna_genome'], tumor_bam,
                          normal_bam, univ_options, radia_options, disk='100M',
                          memory='100M').encapsulate()
    merge = job.wrapJobFn(merge_perchrom_vcfs, spawn.rv(), univ_options, disk='100M', memory='100M')
    job.addChild(spawn)
    spawn.addChild(merge)
    return merge.rv()

def run_radia(job, rna_bam, tumor_bam, normal_bam, univ_options, radia_options):
    if 'rna_genome' in rna_bam.keys():
        rna_bam = rna_bam['rna_genome']
    elif set(rna_bam.keys()) == {'rna_genome_sorted.bam', 'rna_genome_sorted.bam.bai'}:
        pass
    else:
        raise RuntimeError('An improperly formatted dict was passed to rna_bam.')
        
    bams = {'tumor_rna': rna_bam['rna_genome_sorted.bam'],
            'tumor_rnai': rna_bam['rna_genome_sorted.bam.bai'],
            'tumor_dna': tumor_bam['tumor_dna_fix_pg_sorted.bam'],
            'tumor_dnai': tumor_bam['tumor_dna_fix_pg_sorted.bam.bai'],
            'normal_dna': normal_bam['normal_dna_fix_pg_sorted.bam'],
            'normal_dnai': normal_bam['normal_dna_fix_pg_sorted.bam.bai']}
    # Get a list of chromosomes to process
    if radia_options['chromosomes']:
        chromosomes = radia_options['chromosomes']
    else:
        chromosomes = sample_chromosomes(job, radia_options['genome_fai'])

    perchrom_radia = defaultdict()
    for chrom in chromosomes:
        radia = job.addChildJobFn(run_radia_perchrom, bams, univ_options, radia_options, chrom,
                                  memory='6G',
                                  disk=PromisedRequirement(
                                      radia_disk, tumor_bam['tumor_dna_fix_pg_sorted.bam'],
                                      normal_bam['normal_dna_fix_pg_sorted.bam'],
                                      rna_bam['rna_genome_sorted.bam'],
                                      radia_options['genome_fasta']))
        filter_radia = radia.addChildJobFn(run_filter_radia, bams, radia.rv(), univ_options,
                                           radia_options, chrom, memory='6G',
                                           disk=PromisedRequirement(
                                               radia_disk, tumor_bam['tumor_dna_fix_pg_sorted.bam'],
                                               normal_bam['normal_dna_fix_pg_sorted.bam'],
                                               rna_bam['rna_genome_sorted.bam'],
                                               radia_options['genome_fasta']))
        perchrom_radia[chrom] = filter_radia.rv()
    job.fileStore.logToMaster('Ran spawn_radia on %s successfully' % univ_options['patient'])
    return perchrom_radia

def run_radia_perchrom(job, bams, univ_options, radia_options, chrom):
    work_dir = os.getcwd()
    input_files = {
        'rna.bam': bams['tumor_rna'],
        'rna.bam.bai': bams['tumor_rnai'],
        'tumor.bam': bams['tumor_dna'],
        'tumor.bam.bai': bams['tumor_dnai'],
        'normal.bam': bams['normal_dna'],
        'normal.bam.bai': bams['normal_dnai'],
        'genome.fa.tar.gz': radia_options['genome_fasta'],
        'genome.fa.fai.tar.gz': radia_options['genome_fai']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)

    for key in ('genome.fa', 'genome.fa.fai'):
        input_files[key] = untargz(input_files[key + '.tar.gz'], work_dir)
    input_files = {key: docker_path(path) for key, path in input_files.items()}

    radia_output = ''.join([work_dir, '/radia_', chrom, '.vcf'])
    radia_log = ''.join([work_dir, '/radia_', chrom, '_radia.log'])
    parameters = [univ_options['patient'],  # shortID
                  chrom,
                  '-n', input_files['normal.bam'],
                  '-t', input_files['tumor.bam'],
                  '-r', input_files['rna.bam'],
                  ''.join(['--rnaTumorFasta=', input_files['genome.fa']]),
                  '-f', input_files['genome.fa'],
                  '-o', docker_path(radia_output),
                  '-i', univ_options['ref'],
                  '-m', input_files['genome.fa'],
                  '-d', 'aarjunrao@soe.ucsc.edu',
                  '-q', 'Illumina',
                  '--disease', 'CANCER',
                  '-l', 'INFO',
                  '-g', docker_path(radia_log)]
    docker_call(tool='radia', tool_parameters=parameters,
                work_dir=work_dir, dockerhub=univ_options['dockerhub'],
                tool_version=radia_options['version'])
    output_file = job.fileStore.writeGlobalFile(radia_output)
    job.fileStore.logToMaster('Ran radia on %s:%s successfully' % (univ_options['patient'], chrom))
    return output_file

def run_filter_radia(job, bams, radia_file, univ_options, radia_options, chrom):
    work_dir = os.getcwd()
    input_files = {
        'rna.bam': bams['tumor_rna'],
        'rna.bam.bai': bams['tumor_rnai'],
        'tumor.bam': bams['tumor_dna'],
        'tumor.bam.bai': bams['tumor_dnai'],
        'normal.bam': bams['normal_dna'],
        'normal.bam.bai': bams['normal_dnai'],
        'radia.vcf': radia_file,
        'genome.fa.tar.gz': radia_options['genome_fasta'],
        'genome.fa.fai.tar.gz': radia_options['genome_fai'],
        'cosmic_beds': radia_options['cosmic_beds'],
        'dbsnp_beds': radia_options['dbsnp_beds'],
        'retrogene_beds': radia_options['retrogene_beds'],
        'pseudogene_beds': radia_options['pseudogene_beds'],
        'gencode_beds': radia_options['gencode_beds']
    }
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)

    for key in ('genome.fa', 'genome.fa.fai'):
        input_files[key] = untargz(input_files[key + '.tar.gz'], work_dir)
    for key in ('cosmic_beds', 'dbsnp_beds', 'retrogene_beds', 'pseudogene_beds', 'gencode_beds'):
        input_files[key] = untargz(input_files[key], work_dir)

    input_files = {key: docker_path(path) for key, path in input_files.items()}

    filterradia_log = ''.join([work_dir, '/radia_filtered_', chrom, '_radia.log'])
    parameters = [univ_options['patient'],  # shortID
                  chrom.lstrip('chr'),
                  input_files['radia.vcf'],
                  '/data',
                  '/home/radia/scripts',
                  '-d', input_files['dbsnp_beds'],
                  '-r', input_files['retrogene_beds'],
                  '-p', input_files['pseudogene_beds'],
                  '-c', input_files['cosmic_beds'],
                  '-t', input_files['gencode_beds'],
                  '--noSnpEff',
                  '--noBlacklist',
                  '--noTargets',
                  '--noRnaBlacklist',
                  '-f', input_files['genome.fa'],
                  '--log=INFO',
                  '-g', docker_path(filterradia_log)]
    docker_call(tool='filterradia',
                tool_parameters=parameters, work_dir=work_dir, dockerhub=univ_options['dockerhub'],
                tool_version=radia_options['version'])
    output_file = ''.join([work_dir, '/', chrom, '.vcf'])
    os.rename(''.join([work_dir, '/', univ_options['patient'], '_', chrom, '.vcf']), output_file)
    output_fsid = job.fileStore.writeGlobalFile(output_file)
    export_results(job, output_fsid, output_file, univ_options, subfolder='mutations/radia')
    job.fileStore.logToMaster('Ran filter-radia on %s:%s successfully'
                              % (univ_options['patient'], chrom))
    return output_fsid

def index_bamfile(job, bamfile, sample_type, univ_options, samtools_options, sample_info=None,
                  export=True):
    work_dir = os.getcwd()
    in_bamfile = sample_type
    if sample_info is not None:
        assert isinstance(sample_info, str)
        in_bamfile = '_'.join([in_bamfile, sample_info])
    in_bamfile += '.bam'
    input_files = {
        in_bamfile: bamfile}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=True)
    parameters = ['index',
                  input_files[in_bamfile]]
    docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], tool_version=samtools_options['version'])
    out_bai = '/'.join([work_dir, in_bamfile + '.bai'])
    output_files = {in_bamfile: bamfile,
                    in_bamfile + '.bai': job.fileStore.writeGlobalFile(out_bai)}
    if export:
        export_results(job, bamfile, os.path.splitext(out_bai)[0], univ_options,
                       subfolder='alignments')
        export_results(job, output_files[in_bamfile + '.bai'], out_bai, univ_options,
                       subfolder='alignments')
    job.fileStore.logToMaster('Ran samtools-index on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return output_files

def sort_bamfile(job, bamfile, sample_type, univ_options, samtools_options):
    work_dir = os.getcwd()
    in_bamfile = ''.join([sample_type, '.bam'])
    out_bamfile = '_'.join([sample_type, 'sorted.bam'])
    input_files = {
        in_bamfile: bamfile}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=True)
    parameters = ['sort',
                  '-o', docker_path(out_bamfile),
                  '-O', 'bam',
                  '-T', 'temp_sorted',
                  '-@', str(samtools_options['n']),
                  input_files[in_bamfile]]
    docker_call(tool='samtools', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], tool_version=samtools_options['version'])
    job.fileStore.deleteGlobalFile(bamfile)
    job.fileStore.logToMaster('Ran samtools-sort on %s:%s successfully'
                              % (univ_options['patient'], sample_type))
    return job.fileStore.writeGlobalFile(out_bamfile)

def get_identity(user):
    identity = Identity(user.id)

    if hasattr(user, 'id'):
        identity.provides.add(UserNeed(user.id))

    for role in getattr(user, 'roles', []):
        identity.provides.add(RoleNeed(role.name))

    identity.user = user
    return identity

def object_to_items(data_structure):
    items = []
    # Get all items from dict
    try:
        items = list(data_structure.__dict__.items())
    except:
        pass
    # Get all slots
    hierarchy = [data_structure]
    try:
        hierarchy += inspect.getmro(data_structure)
    except:
        pass
    slots = []
    try:
        for b in hierarchy:
            try:
                slots += b.__slots__
            except:  # pragma: no cover
                pass
    except:  # pragma: no cover
        pass
    # Get attrs from slots
    for x in slots:
        items.append((x, getattr(data_structure, x)))
    return items

def recursive_sort(data_structure):
    # We don't sory primitve types
    if not isinstance(data_structure, _primitive_types):
        is_meta = isinstance(data_structure, Meta)
        was_dict = isinstance(data_structure, WasDict)
        if not (is_meta or was_dict):
            was_dict = isinstance(data_structure, dict)
            if not was_dict:
                # Dictize if possible (support objects)
                try:
                    data_structure = data_structure.__dict__
                    was_dict = True
                except:
                    pass
            # Itemize if possible
            try:
                data_structure = data_structure.items()
            except:
                pass
        tlen = -1
        # If item has a length we sort it
        try:
            tlen = len(data_structure)
        except:  # pragma: no cover
            pass
        if tlen != -1:
            # Well there are classes out in the wild that answer to len
            # but have no indexer.
            try:
                if was_dict:
                    return tuple(sorted(
                        [
                            (
                                recursive_sort(x[0]),
                                recursive_sort(x[1]),
                            )
                            for x in data_structure
                        ],
                        key=TraversalBasedReprCompare
                    ))
                elif is_meta:
                    return data_structure[0:-1] + [
                        recursive_sort(
                            data_structure[-1]
                        )
                    ]
                else:
                    return tuple(sorted(
                        [recursive_sort(
                            x,
                        ) for x in data_structure],
                        key=TraversalBasedReprCompare,
                    ))
            except:  # pragma: no cover
                pass
    return data_structure

def traverse_frozen_data(data_structure):
    parent_stack = [data_structure]
    while parent_stack:
        node = parent_stack.pop(0)
        # We don't iterate strings
        tlen = -1
        if not isinstance(node, _string_types):
            # If item has a length we freeze it
            try:
                tlen = len(node)
            except:
                pass
        if tlen == -1:
            yield node
        else:
            parent_stack = list(node) + parent_stack

def tree_diff(a, b, n=5, sort=False):
    a = dump(a)
    b = dump(b)
    if not sort:
        a = vformat(a).split("\n")
        b = vformat(b).split("\n")
    else:
        a = vformat(recursive_sort(a)).split("\n")
        b = vformat(recursive_sort(b)).split("\n")
    return "\n".join(difflib.unified_diff(a, b, n=n, lineterm=""))

def stats(self):
        stats_online = CRef.cint()
        stats_ingame = CRef.cint()
        stats_chatting = CRef.cint()

        self._iface.get_clan_stats(
            self.group_id,
            stats_online,
            stats_ingame,
            stats_chatting,
        )

        return {
            'online': int(stats_online),
            'ingame': int(stats_ingame),
            'chatting': int(stats_chatting),
        }

def startproject(name, directory, verbosity):
    handle_template('project', name, target=directory, verbosity=verbosity)
    click.echo(f"Success: '{name}' project was successfully created on '{directory}'")

def createbot(name, directory, verbosity):
    handle_template('bot', name, target=directory, verbosity=verbosity)
    click.echo(f"Success: '{name}' bot was successfully created on '{directory}'")

def get_state(self, as_str=False):

        uid = self.user_id

        if self._iface_user.get_id() == uid:
            result = self._iface.get_my_state()

        else:
            result = self._iface.get_state(uid)

        if as_str:
            return UserState.get_alias(result)

        return result

def load_permissions_on_identity_loaded(sender, identity):
    identity.provides.add(
        any_user
    )
    # if the user is not anonymous
    if current_user.is_authenticated:
        # Add the need provided to authenticated users
        identity.provides.add(
            authenticated_user
        )

def print_errors(self, file_name):
        for error in self.get_messages(file_name):
            print('\t', error.__unicode__())

def clean(self):
        data = super(RasterQueryForm, self).clean()
        geom = data.pop('upload', None) or data.pop('bbox', None)
        if geom:
            data['g'] = geom
        return data

def register(matcher, *aliases):
    docstr = matcher.__doc__ if matcher.__doc__ is not None else ''
    helpmatchers[matcher] = docstr.strip()

    for alias in aliases:
        matchers[alias] = matcher
        # Map a normalized version of the alias
        norm = normalize(alias)
        normalized[norm] = alias
        # Map a version without snake case
        norm = norm.replace('_', '')
        normalized[norm] = alias

def normalize(alias):
    # Convert from CamelCase to snake_case
    alias = re.sub(r'([a-z])([A-Z])', r'\1_\2', alias)
    # Ignore words
    words = alias.lower().split('_')
    words = filter(lambda w: w not in IGNORED_WORDS, words)
    return '_'.join(words)

def lookup(alias):

    if alias in matchers:
        return matchers[alias]
    else:
        norm = normalize(alias)
        if norm in normalized:
            alias = normalized[norm]
            return matchers[alias]

    # Check without snake case
    if -1 != alias.find('_'):
        norm = normalize(alias).replace('_', '')
        return lookup(norm)

    return None

def suggest(alias, max=3, cutoff=0.5):

    aliases = matchers.keys()
    similar = get_close_matches(alias, aliases, n=max, cutoff=cutoff)

    return similar

def sample_chromosomes(job, genome_fai_file):
    work_dir = os.getcwd()
    genome_fai = untargz(job.fileStore.readGlobalFile(genome_fai_file), work_dir)
    return chromosomes_from_fai(genome_fai)

def run_mutation_aggregator(job, mutation_results, univ_options):
    # Setup an input data structure for the merge function
    out = {}
    for chrom in mutation_results['mutect'].keys():
        out[chrom] = job.addChildJobFn(merge_perchrom_mutations, chrom, mutation_results,
                                       univ_options).rv()
    merged_snvs = job.addFollowOnJobFn(merge_perchrom_vcfs, out, 'merged', univ_options)
    job.fileStore.logToMaster('Aggregated mutations for %s successfully' % univ_options['patient'])
    return merged_snvs.rv()

def merge_perchrom_mutations(job, chrom, mutations, univ_options):
    work_dir = os.getcwd()
    from protect.mutation_calling.muse import process_muse_vcf
    from protect.mutation_calling.mutect import process_mutect_vcf
    from protect.mutation_calling.radia import process_radia_vcf
    from protect.mutation_calling.somaticsniper import process_somaticsniper_vcf
    from protect.mutation_calling.strelka import process_strelka_vcf
    mutations.pop('indels')
    mutations['strelka_indels'] = mutations['strelka']['indels']
    mutations['strelka_snvs'] = mutations['strelka']['snvs']
    vcf_processor = {'snvs': {'mutect': process_mutect_vcf,
                              'muse': process_muse_vcf,
                              'radia': process_radia_vcf,
                              'somaticsniper': process_somaticsniper_vcf,
                              'strelka_snvs': process_strelka_vcf
                              },
                     'indels': {'strelka_indels': process_strelka_vcf
                                }
                     }
    #                 'fusions': lambda x: None,
    #                 'indels': lambda x: None}
    # For now, let's just say 2 out of n need to call it.
    # num_preds = len(mutations)
    # majority = int((num_preds + 0.5) / 2)
    majority = {'snvs': 2,
                'indels': 1}


    accepted_hits = defaultdict(dict)

    for mut_type in vcf_processor.keys():
        # Get input files
        perchrom_mutations = {caller: vcf_processor[mut_type][caller](job, mutations[caller][chrom],
                                                                      work_dir, univ_options)
                              for caller in vcf_processor[mut_type]}
        # Process the strelka key
        perchrom_mutations['strelka'] = perchrom_mutations['strelka_' + mut_type]
        perchrom_mutations.pop('strelka_' + mut_type)
        # Read in each file to a dict
        vcf_lists = {caller: read_vcf(vcf_file) for caller, vcf_file in perchrom_mutations.items()}
        all_positions = list(set(itertools.chain(*vcf_lists.values())))
        for position in sorted(all_positions):
            hits = {caller: position in vcf_lists[caller] for caller in perchrom_mutations.keys()}
            if sum(hits.values()) >= majority[mut_type]:
                callers = ','.join([caller for caller, hit in hits.items() if hit])
                assert position[1] not in accepted_hits[position[0]]
                accepted_hits[position[0]][position[1]] = (position[2], position[3], callers)

    with open(''.join([work_dir, '/', chrom, '.vcf']), 'w') as outfile:
        print('##fileformat=VCFv4.0', file=outfile)
        print('##INFO=<ID=callers,Number=.,Type=String,Description=List of supporting callers.',
              file=outfile)
        print('#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO', file=outfile)
        for chrom in chrom_sorted(accepted_hits.keys()):
            for position in sorted(accepted_hits[chrom]):
                    print(chrom, position, '.', accepted_hits[chrom][position][0],
                          accepted_hits[chrom][position][1], '.', 'PASS',
                          'callers=' + accepted_hits[chrom][position][2], sep='\t', file=outfile)
    fsid = job.fileStore.writeGlobalFile(outfile.name)
    export_results(job, fsid, outfile.name, univ_options, subfolder='mutations/merged')
    return fsid

def read_vcf(vcf_file):
    vcf_dict = []
    with open(vcf_file, 'r') as invcf:
        for line in invcf:
            if line.startswith('#'):
                continue
            line = line.strip().split()
            vcf_dict.append((line[0], line[1], line[3], line[4]))
    return vcf_dict

def merge_perchrom_vcfs(job, perchrom_vcfs, tool_name, univ_options):
    work_dir = os.getcwd()
    input_files = {''.join([chrom, '.vcf']): jsid for chrom, jsid in perchrom_vcfs.items()}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)
    first = True
    with open(''.join([work_dir, '/', 'all_merged.vcf']), 'w') as outvcf:
        for chromvcfname in chrom_sorted([x.rstrip('.vcf') for x in input_files.keys()]):
            with open(input_files[chromvcfname + '.vcf'], 'r') as infile:
                for line in infile:
                    line = line.strip()
                    if line.startswith('#'):
                        if first:
                            print(line, file=outvcf)
                        continue
                    first = False
                    print(line, file=outvcf)
    output_file = job.fileStore.writeGlobalFile(outvcf.name)
    export_results(job, output_file, outvcf.name, univ_options, subfolder='mutations/' + tool_name)
    job.fileStore.logToMaster('Ran merge_perchrom_vcfs for %s successfully' % tool_name)
    return output_file

def unmerge(job, input_vcf, tool_name, chromosomes, tool_options, univ_options):
    work_dir = os.getcwd()
    input_files = {
        'input.vcf': input_vcf,
        'genome.fa.fai.tar.gz': tool_options['genome_fai']}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)

    input_files['genome.fa.fai'] = untargz(input_files['genome.fa.fai.tar.gz'], work_dir)


    read_chromosomes = defaultdict()
    with open(input_files['input.vcf'], 'r') as in_vcf:
        header = []
        for line in in_vcf:
            if line.startswith('#'):
                header.append(line)
                continue
            line = line.strip()
            chrom = line.split()[0]
            if chrom in read_chromosomes:
                print(line, file=read_chromosomes[chrom])
            else:
                read_chromosomes[chrom] = open(os.path.join(os.getcwd(), chrom + '.vcf'), 'w')
                print(''.join(header), file=read_chromosomes[chrom], end='')
                print(line, file=read_chromosomes[chrom])
    # Process chromosomes that had no mutations
    for chrom in set(chromosomes).difference(set(read_chromosomes.keys())):
        read_chromosomes[chrom] = open(os.path.join(os.getcwd(), chrom + '.vcf'), 'w')
        print(''.join(header), file=read_chromosomes[chrom], end='')
    outdict = {}
    chroms = set(chromosomes).intersection(set(read_chromosomes.keys()))
    for chrom, chromvcf in read_chromosomes.items():
        chromvcf.close()
        if chrom not in chroms:
            continue
        outdict[chrom] = job.fileStore.writeGlobalFile(chromvcf.name)
        export_results(job, outdict[chrom], chromvcf.name, univ_options,
                       subfolder='mutations/' + tool_name)
    return outdict

def as_feature(data):
    if not isinstance(data, (Feature, FeatureCollection)):
        if is_featurelike(data):
            data = Feature(**data)
        elif has_features(data):
            data = FeatureCollection(**data)
        elif isinstance(data, collections.Sequence):
            data = FeatureCollection(features=data)
        elif has_layer(data):
            data = LayerCollection(data)
        elif has_coordinates(data):
            data = Feature(geometry=data)
        elif isinstance(data, collections.Mapping) and not data:
            data = Feature()
    return data

def has_layer(fcollection):
    A wrapper for run_rsem using the results from run_star as input.

    :param dict star_bams: dict of results from star
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict rsem_options: Options specific to rsem
    :return: Dict of gene- and isoform-level expression calls
             output_files:
                 |- 'rsem.genes.results': fsID
                 +- 'rsem.isoforms.results': fsID
    :rtype: dict
    Run rsem on the input RNA bam.

    ARGUMENTS
    :param toil.fileStore.FileID rna_bam: fsID of a transcriptome bam generated by STAR
    :param dict univ_options: Dict of universal options used by almost all tools
    :param dict rsem_options: Options specific to rsem
    :return: Dict of gene- and isoform-level expression calls
             output_files:
                 |- 'rsem.genes.results': fsID
                 +- 'rsem.isoforms.results': fsID
    :rtype: dict

        :param str page: Overlay page alias (see OVERLAY_PAGE_*)
            or a custom URL.


    if len(args):
        value = (value,) + args

    return ExpectationAny(value)

def all_of(value, *args):

    if len(args):
        value = (value,) + args

    return ExpectationNone(value)

def run_cutadapt(job, fastqs, univ_options, cutadapt_options):
    work_dir = os.getcwd()
    input_files = {
        'rna_1.fastq': fastqs[0],
        'rna_2.fastq': fastqs[1]}
    input_files = get_files_from_filestore(job, input_files, work_dir, docker=False)
    # Handle gzipped file
    gz = '.gz' if is_gzipfile(input_files['rna_1.fastq']) else ''
    if gz:
        for read_file in 'rna_1.fastq', 'rna_2.fastq':
            os.symlink(read_file, read_file + gz)
            input_files[read_file + gz] = input_files[read_file] + gz
    input_files = {key: docker_path(path) for key, path in input_files.items()}
    parameters = ['-a', cutadapt_options['a'],  # Fwd read 3' adapter
                  '-A', cutadapt_options['A'],  # Rev read 3' adapter
                  '-m', '35',  # Minimum size of read
                  '-o', docker_path('rna_cutadapt_1.fastq.gz'),  # Output for R1
                  '-p', docker_path('rna_cutadapt_2.fastq.gz'),  # Output for R2
                  input_files['rna_1.fastq' + gz],
                  input_files['rna_2.fastq' + gz]]
    docker_call(tool='cutadapt', tool_parameters=parameters, work_dir=work_dir,
                dockerhub=univ_options['dockerhub'], tool_version=cutadapt_options['version'])
    output_files = []
    for fastq_file in ['rna_cutadapt_1.fastq.gz', 'rna_cutadapt_2.fastq.gz']:
        output_files.append(job.fileStore.writeGlobalFile('/'.join([work_dir, fastq_file])))
    job.fileStore.logToMaster('Ran cutadapt on %s successfully' % univ_options['patient'])
    return output_files

def index():
    identity = g.identity
    actions = {}
    for action in access.actions.values():
        actions[action.value] = DynamicPermission(action).allows(identity)

    message = 'You are opening a page requiring the "admin-access" permission'
    return render_template("invenio_access/limited.html",
                           message=message,
                           actions=actions,
                           identity=identity)

def read_fastas(input_files):
    tumor_file = [y for x, y in input_files.items() if x.startswith('T')][0]
    normal_file = [y for x, y in input_files.items() if x.startswith('N')][0]

    output_files = defaultdict(list)
    output_files = _read_fasta(tumor_file, output_files)
    num_entries = len(output_files)
    output_files = _read_fasta(normal_file, output_files)
    assert len(output_files) == num_entries
    return output_files

def _read_fasta(fasta_file, output_dict):
    read_name = None
    with open(fasta_file, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if line.startswith('>'):
                read_name = line.lstrip('>')
            else:
                assert read_name is not None, line
                output_dict[read_name].append(line.strip())
    return output_dict

def _process_consensus_mhcii(mhc_file, normal=False):
    core_col = None  # Variable to hold the column number with the core
    results = pandas.DataFrame(columns=['allele', 'pept', 'tumor_pred', 'core'])
    with open(mhc_file, 'r') as mf:
        peptides = set()
        for line in mf:
            # Skip header lines
            if not line.startswith('HLA'):
                continue
            line = line.strip().split('\t')
            allele = line[0]
            pept = line[4]
            pred = line[6]
            if core_col:
                core = line[core_col]
            else:
                methods = line[5].lstrip('Consensus(').rstrip(')')
                methods = methods.split(',')
                if 'NN' in methods:
                    core_col = 13
                elif 'netMHCIIpan' in methods:
                    core_col = 17
                elif 'Sturniolo' in methods:
                    core_col = 19
                elif 'SMM' in methods:
                    core_col = 10
                core = line[core_col] if core_col else 'NOCORE'
            if float(pred) > 5.00 and not normal:
                continue
            results.loc[len(results)] = [allele, pept, pred, core]
    results.drop_duplicates(inplace=True)
    return results

def _process_net_mhcii(mhc_file, normal=False):
    results = pandas.DataFrame(columns=['allele', 'pept', 'tumor_pred', 'core', 'peptide_name'])
    with open(mhc_file, 'r') as mf:
        peptides = set()
        # Get the allele from the first line and skip the second line
        allele = re.sub('-DQB', '/DQB', mf.readline().strip())
        _ = mf.readline()
        for line in mf:
            line = line.strip().split('\t')
            pept = line[1]
            pred = line[5]
            core = 'NOCORE'
            peptide_name = line[2]
            if float(pred) > 5.00 and not normal:
                continue
            results.loc[len(results)] = [allele, pept, pred, core, peptide_name]
    results.drop_duplicates(inplace=True)
    return results

def _process_mhci(mhc_file, normal=False):
    results = pandas.DataFrame(columns=['allele', 'pept', 'tumor_pred', 'core'])
    with open(mhc_file, 'r') as mf:
        peptides = set()
        for line in mf:
            # Skip header lines
            if not line.startswith('HLA'):
                continue
            line = line.strip().split('\t')
            allele = line[0]
            pept = line[5]
            pred = line[7]
            if float(pred) > 5.00 and not normal:
                continue
            results.loc[len(results)] = [allele, pept, pred, pept]
    results.drop_duplicates(inplace=True)
    return results

def pept_diff(p1, p2):
    if len(p1) != len(p2):
        return -1
    else:
        return sum([p1[i] != p2[i] for i in range(len(p1))])

def print_mhc_peptide(neoepitope_info, peptides, pepmap, outfile, netmhc=False):
    if netmhc:
        peptide_names = [neoepitope_info.peptide_name]
    else:
        peptide_names = [x for x, y in peptides.items() if neoepitope_info.pept in y]
    # Convert named tuple to dict so it can be modified
    neoepitope_info = neoepitope_info._asdict()
    # Handle fusion peptides (They are characterized by having all N's as the normal partner)
    if neoepitope_info['normal_pept'] == 'N' * len(neoepitope_info['pept']):
        neoepitope_info['normal_pept'] = neoepitope_info['normal_pred'] = 'NA'
    # For each peptide, append the ensembl gene
    for peptide_name in peptide_names:
        print('{ni[allele]}\t'
              '{ni[pept]}\t'
              '{ni[normal_pept]}\t'
              '{pname}\t'
              '{ni[core]}\t'
              '0\t'
              '{ni[tumor_pred]}\t'
              '{ni[normal_pred]}\t'
              '{pmap}'.format(ni=neoepitope_info, pname=peptide_name,
                                                  pmap=pepmap[peptide_name]), file=outfile)
    return None

def check(domain, prefix, code, strategies='*'):
    if strategies == '*' or 'dns_txt' in strategies:
        if check_dns_txt(domain, prefix, code):
            return True
    if strategies == '*' or 'dns_cname' in strategies:
        if check_dns_cname(domain, prefix, code):
            return True
    if strategies == '*' or 'meta_tag' in strategies:
        if check_meta_tag(domain, prefix, code):
            return True
    if strategies == '*' or 'html_file' in strategies:
        if check_html_file(domain, prefix, code):
            return True
    return False

def register_cache_buster(self, app, config=None):
        if not (config is None or isinstance(config, dict)):
            raise ValueError("`config` must be an instance of dict or None")

        bust_map = {}  # map from an unbusted filename to a busted one
        unbust_map = {}  # map from a busted filename to an unbusted one
        # http://flask.pocoo.org/docs/0.12/api/#flask.Flask.static_folder

        app.logger.debug('Starting computing hashes for static assets')
        # compute (un)bust tables.
        for dirpath, dirnames, filenames in os.walk(app.static_folder):
            for filename in filenames:
                # compute version component
                rooted_filename = os.path.join(dirpath, filename)
                if not self.__is_file_to_be_busted(rooted_filename):
                    continue
                app.logger.debug(f'Computing hashes for {rooted_filename}')
                with open(rooted_filename, 'rb') as f:
                    version = hashlib.md5(
                        f.read()
                    ).hexdigest()[:self.hash_size]

                # add version
                unbusted = os.path.relpath(rooted_filename, app.static_folder)
                # busted = os.path.join(version, unbusted)
                busted = f"{unbusted}?q={version}"

                # save computation to map
                bust_map[unbusted] = busted
                unbust_map[busted] = unbusted
        app.logger.debug('Finished Starting computing hashes for static assets')


        def bust_filename(file):
            return bust_map.get(file, file)


        def unbust_filename(file):
            return unbust_map.get(file, file)


        @app.url_defaults

        def reverse_to_cache_busted_url(endpoint, values):
            if endpoint == 'static':
                values['filename'] = bust_filename(values['filename'])


        def debusting_static_view(*args, **kwargs):
            kwargs['filename'] = unbust_filename(kwargs.get('filename'))
            return original_static_view(*args, **kwargs)


        # Replace the default static file view with our debusting view.
        original_static_view = app.view_functions['static']
        app.view_functions['static'] = debusting_static_view

def env_or_default(var, default=None):
    if var in os.environ:
        return os.environ[var]

    return default

def kms_encrypt(value, key, aws_config=None):
    aws_config = aws_config or {}
    aws = boto3.session.Session(**aws_config)
    client = aws.client('kms')
    enc_res = client.encrypt(KeyId=key,
                             Plaintext=value)
    return n(b64encode(enc_res['CiphertextBlob']))

def get_value(*args, **kwargs):
    global _config
    if _config is None:
        raise ValueError('configuration not set; must run figgypy.set_config first')
    return _config.get_value(*args, **kwargs)

def set_value(*args, **kwargs):

        :param host_value: a value of the host of a type that can be
        listed by the service
        :returns: an instance of AddressListItem representing
        a matched value
        :raises InvalidHostError: if the argument is not a valid
        host string

        :param urls: an iterable containing URLs
        :returns: True if any host has a listed match
        :raises InvalidURLError: if there are any invalid URLs in
        the sequence

        :param urls: an iterable containing URLs
        :returns: instances of AddressListItem representing listed
        hosts matching the ones used by the given URLs
        :raises InvalidURLError: if there are any invalid URLs in
        the sequence

        :param urls: an iterable containing URLs to filter
        :returns: a generator yielding matching URLs
        :raises InvalidURLError: if there are any invalid URLs in
        the sequence

    Decodes a bytestring containing modified UTF-8 as defined in section
    4.4.7 of the JVM specification.

    :param s: bytestring to be converted.
    :returns: A unicode representation of the original string.

    Encodes a unicode string as modified UTF-8 as defined in section 4.4.7
    of the JVM specification.

    :param u: unicode string to be converted.
    :returns: A decoded bytearray.
        Remove a previously registered callback

        :param str name: Hook name
        :param callable func: A function reference\
        that was registered previously
        Remove all callbacks

        :param str name: Hook name

    def decorator(func):
            if hasattr(self, name):
                # Return the old property
                return getattr(self, name)
            else:
                return func(self)
        return func_wrapper
    return decorator

def get(code):
    instance = _cache.get(code)
    if instance is None:
        url = '{prefix}{code}.gml?download'.format(prefix=EPSG_IO_URL,
                                                   code=code)
        xml = requests.get(url).content
        root = ET.fromstring(xml)
        class_for_tag = {
            GML_NS + 'CartesianCS': CartesianCS,
            GML_NS + 'GeodeticCRS': GeodeticCRS,
            GML_NS + 'ProjectedCRS': ProjectedCRS,
            GML_NS + 'CompoundCRS': CompoundCRS,
            GML_NS + 'BaseUnit': UOM,
        }
        if root.tag in class_for_tag:
            instance = class_for_tag[root.tag](root)
        else:
            raise ValueError('Unsupported code type: {}'.format(root.tag))
        _cache[code] = instance
    return instance

def id(self):
        Return the OGC WKT which corresponds to the CRS as HTML.

        For example::

            >>> print(get(27700).as_html())  # doctest: +ELLIPSIS
            <div class="syntax"><pre><span class="gh">PROJCS</span><span...

        Return the PROJ.4 string which corresponds to the CRS.

        For example::

            >>> print(get(21781).as_proj4())
            +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 \
+k_0=1 +x_0=600000 +y_0=200000 +ellps=bessel \

+towgs84=674.4,15.1,405.3,0,0,0,0 +units=m +no_defs

        Removes a `method` from the table by identity.
        Creates a new method from `name` and `descriptor`. If `code` is not
        ``None``, add a `Code` attribute to this method.
        Read the MethodTable from the file-like object `source`.

        .. note::

            Advanced usage only. You will typically never need to call this
            method as it will be called for you when loading a ClassFile.

        :param source: Any file-like object providing `read()`
        Write the MethodTable to the file-like object `out`.

        .. note::

            Advanced usage only. You will typically never need to call this
            method as it will be called for you when saving a ClassFile.

        :param out: Any file-like object providing `write()`
        Read the CodeAttribute from the byte string `info`.

        .. note::

            Advanced usage only. You will typically never need to call this
            method as it will be called for you when loading a ClassFile.

        :param info: A byte string containing an unparsed CodeAttribute.
        The `CodeAttribute` in packed byte string form.
    if ramp.is_playing:
        play_symbol = u'\u25B6'
    else:
        play_symbol = u'\u2759\u2759'

    print u' %s %s by %s from %s via %s, %s of %s' % (
        play_symbol,
        ramp.title,
        ramp.artist,
        ramp.album,
        cast.app.app_id,
        _to_minutes(ramp.current_time),
        _to_minutes(ramp.duration)
    )

def main():
    opts = docopt(__doc__, version="cast 0.1")

    cast = pychromecast.PyChromecast(CHROMECAST_HOST)
    ramp = cast.get_protocol(pychromecast.PROTOCOL_RAMP)

    # Wait for ramp connection to be initted.
    time.sleep(SLEEP_TIME)

    if ramp is None:
        print 'Chromecast is not up or current app does not handle RAMP.'
        return 1

    if opts['next']:
        ramp.next()
    elif opts['pause']:
        ramp.pause()
    elif opts['play']:
        ramp.play()
    elif opts['toggle']:
        ramp.playpause()
    elif opts['seek']:
        ramp.seek(opts['<second>'])
    elif opts['rewind']:
        ramp.rewind()
    elif opts['status']:
        _status_command(cast, ramp)
    elif opts['volume']:
        _volume_command(ramp, opts['<value>'])

    # Wait for command to be sent.
    time.sleep(SLEEP_TIME)

def untokenize(iterable):
    ut = Untokenizer()
    out = ut.untokenize(iterable)
    if ut.encoding is not None:
        out = out.encode(ut.encoding)
    return out

def get_powers_of_2(_sum):
    return [2**y for y, x in enumerate(bin(_sum)[:1:-1]) if int(x)]

def _query(self, host_object):
        host_to_query = host_object.relative_domain
        query_name = host_to_query.derelativize(self._query_suffix)
        try:
            return query(query_name)
        except NXDOMAIN:
            return None

def _query(self, host_object, classification=False):
        template = 'http://verify.hosts-file.net/?v={}&s={}'
        url = template.format(self.app_id, host_object.to_unicode())
        url = url + '&class=true' if classification else url
        return get(url).text

def _request_address(self):

        :param urls: a sequence of URLs to put in request body
        :returns: a response object
        :raises UnathorizedAPIKeyError: when the API key for this
        instance is not valid
        :raises HTTPError: if the HTTPError was raised for a HTTP code
        other than 401, the exception is reraised

        :param urls: a sequence of URLs  to be tested
        :returns: a tuple containing chunk of URLs and a response
        pertaining to them if the code of response was 200, which
        means at least one of the queried URLs is matched in either
        the phishing, malware, or unwanted software lists.

        :param urls: a sequence of URLs to test
        :return: a tuple containing matching URL and classification
        string pertaining to it

        :param urls: a sequence of URLs to be tested
        :returns: objects representing listed URLs
        :raises InvalidURLError: if there are any invalid URLs in
        the sequence
        A view decorator to extend another view class or function to itself
        It will inherit all its methods and propeties and use them on itself

        -- EXAMPLES --

        class Index(Pylot):
            pass

        index = Index()

        ::-> As decorator on classes ::
        @index.extends_
        class A(object):

            def hello(self):
                pass

        @index.extends_
        class C()

            def world(self):
                pass

        ::-> Decorator With function call ::
        @index.extends_

        def hello(self):
            pass

        Send simple message
        Send Template message
    Displays pylint data to the console.

    :param pylint_data:
    :return:
    Post the data to gerrit. This right now is a stub, as
    I'll need to write the code to post up to gerrit.

    :param commit: Commit ID of the review.
    :param message: Message to accompany the review score.
    :param user: SSH User for posting to gerrit.
    :param gerrit: Hostname of the gerrit server.
    :param score: Score to post to gerrit (+1/-1, etc)
    :return:
    Sorts a list of files into types.

    :param file_list: List of file paths.
    :return: {extension: [<list of file paths with that extension>]}
    Check out target into the current directory.
    Target can be a branch, review Id, or commit.

    :param repository: Current git repository.
    :param target: Review ID, commit, branch.
    :return: Return the most recent commit ID (top of the git log).
    Get a list of files changed compared to the given review.
    Compares against current directory.

    :param repository: Git repository. Used to get remote.

      - By default uses first remote in list.
    :param review_id: Gerrit review ID.
    :return: List of file paths relative to current directory.
        for name, child in self._compound_children.items():
            self.logger.debug('start %s (%s)', name, child.__class__.__name__)
            child.start()

def join(self, end_comps=False):
        for name, child in self._compound_children.items():
            if end_comps and not child.is_pipe_end():
                continue
            self.logger.debug('join %s (%s)', name, child.__class__.__name__)
            child.join()

def main(review_id, repository, branch="development", user='admin', gerrit=None):
    checkout(repository, branch)
    raw_file_list = get_files_changed(repository=repository, review_id=review_id)
    checkout(repository=repository, target=branch)

    files = sort_by_type(raw_file_list)
    old_data = run_linters(files)

    commit_id = checkout(repository=repository, target=review_id)

    new_data = run_linters(files)
    dump_to_console(new_data['py'])

    validations = run_validators(new_data, old_data)

    # Get the lowest score from all validators.
    final_score = min(list(validations.values()), key=lambda x: x[0])[0]
    comment = ""
    for name, validation in list(validations.items()):
        score, message = validation
        # Each validator should return it's own specialized comment
        # Ex: 'Passed <name> Validation!\n', or 'Failed <name> Validation!\n<reasons/data>\n'
        if message[-1:] != "\n":
            message += "\n"
        comment += message

    exit_code = 1 if final_score < 0 else 0

    post_to_gerrit(commit_id, score=final_score, message=comment, user=user, gerrit=gerrit)
    exit(exit_code)

def set_sqlite_pragma(dbapi_connection, connection_record):
            appropriately
        subj, pred, obj = triple

        if self._should_ignore_predicate(pred):
            log.info("Ignoring triple with predicate '{}'"
                     .format(self._field_name_from_uri(pred)))
            return

        classes = []
        log.warning("Possible member %s found" % pred)

        pred = self._expand_qname(pred)

        if self._namespace_from_uri(pred) not in self.allowed_namespaces:
            log.info("Member %s does not use an allowed namespace", pred)
            return

        instanceof = self._is_instance((subj, pred, obj))
        if type(instanceof) == rt.URIRef:
            instanceof = self._expand_qname(instanceof)


        if hasattr(self.schema_def, "attributes_by_class") and \

           not self.schema_def.attributes_by_class:
            log.info("Parsed ontology not found. Parsing...")

            self.schema_def.parse_ontology()

        class_invalid = self._validate_class(instanceof)
        if class_invalid:
            log.warning("Invalid class %s" % instanceof)
            return class_invalid
        # TODO - the above sometimes fails when a single object has more than
        # one rdfa type (eg <span property="schema:creator rnews:creator"
        # typeof="schema:Person rnews:Person">
        # Graph chooses the type in an arbitrary order, so it's unreliable
        # eg: http://semanticweb.com/the-impact-of-rdfa_b35003

        classes = self._superclasses_for_subject(self.graph, instanceof)
        classes.append(instanceof)

        member_invalid = self._validate_member(pred, classes, instanceof)
        if member_invalid:
            log.warning("Invalid member of class")
            return member_invalid

        dupe_invalid = self._validate_duplication((subj, pred), instanceof)
        if dupe_invalid:
            log.warning("Duplication found")
            return dupe_invalid

        # collect a list of checked attributes
        self.checked_attributes.append((subj, pred))
        log.warning("successfully validated triple, no errors")
        return

def _validate_class(self, cl):
        `classes`
        subj, pred = subj_and_pred

        log.info("Validating duplication of member %s" % pred)
        if (subj, pred) in self.checked_attributes:
            err = self.err("{0} - duplicated member of {1}",
                           self._field_name_from_uri(pred),
                           self._field_name_from_uri(cl))
            return ValidationWarning(ValidationResult.WARNING, err['err'],
                                     err['line'], err['num'])

def _superclasses_for_subject(self, graph, typeof):
        subj, pred, obj = triple
        input_pred_ns = self._namespace_from_uri(self._expand_qname(pred))
        triples = self.graph.triples(

            (subj, rt.URIRef(self.schema_def.lexicon['type']), None)
        )
        if triples:
            for tr in triples:
                triple_obj_ns = self._namespace_from_uri(
                    self._expand_qname(tr[2]))
                if input_pred_ns == triple_obj_ns:  # match namespaces
                    return tr[2]

def _namespace_from_uri(self, uri):
    Downloads ports data from IANA & Wikipedia and converts
    it to a python module. This function is used to generate _ranges.py.
    Returns used port ranges according to Wikipedia page.
    This page contains unofficial well-known ports.
    Returns unassigned port ranges according to IANA.
    Run through file list, and try to find a linter
    that matches the given file type.

    If it finds a linter, it will run it, and store the
    resulting data in a dictionary (keyed to file_type).

    :param files:
    :return: {file_extension: lint_data}
    Run through all matching validators.

    :param new_data: New lint data.
    :param old_data: Old lint data (before review)
    :return:
        if isinstance(s, basestring):
            return unicode(HTMLParser.unescape(self, s))
        elif isinstance(s, list):
            return [unicode(HTMLParser.unescape(self, item)) for item in s]
        else:
            return s

def get_standard(self):
        parser = ParselyPageParser()
        ret = None
        try:
            parser.feed(body)
        except HTMLParseError:
            pass  # ignore and hope we got ppage
        if parser.ppage is None:
            return

        ret = parser.ppage
        if ret:
            ret = {parser.original_unescape(k): parser.original_unescape(v)
                   for k, v in iteritems(ret)}
        return ret

def _read_schema(self):
        cache_filename = os.path.join(
            CACHE_ROOT, "%s.smt" % self._representation)
        log.info("Attempting to read local schema at %s" % cache_filename)
        try:
            if time.time() - os.stat(cache_filename).st_mtime > CACHE_EXPIRY:
                log.warning("Cache expired, re-pulling")

                self._pull_schema_definition(cache_filename)
        except OSError:
            log.warning("Local schema not found. Pulling from web.")

            self._pull_schema_definition(cache_filename)
        else:
            log.info("Success")

        return cache_filename

def _pull_schema_definition(self, fname):
        name, ext = os.path.splitext(self._ontology_file)
        if ext in ['.ttl']:
            self._ontology_parser_function = \
                lambda s: rdflib.Graph().parse(s, format='n3')
        else:
            self._ontology_parser_function = \
                lambda s: pyRdfa().graph_from_source(s)
        if not self._ontology_parser_function:
            raise ValueError(
                "No function found to parse ontology. %s" %
                self.errorstring_base)
        if not self._ontology_file:
            raise ValueError(
                "No ontology file specified. %s" % self.errorstring_base)
        if not self.lexicon:
            raise ValueError(
                "No lexicon object assigned. %s" % self.errorstring_base)

        latest_file = self._read_schema()

        try:
            self.graph = self._ontology_parser_function(latest_file)
        except:
            raise IOError("Error parsing ontology at %s" % latest_file)

        for subj, pred, obj in self.graph:
            self.ontology[subj].append((pred, obj))
            yield (subj, pred, obj)

def baseDomain(domain, includeScheme=True):
    result = ''
    url    = urlparse(domain)
    if includeScheme:
        result = '%s://' % url.scheme
    if len(url.netloc) == 0:
        result += url.path
    else:
        result += url.netloc
    return result

def search(session, query):
    flat_query = "".join(query.split())

    artists = session.query(Artist).filter(
            or_(Artist.name.ilike(f"%%{query}%%"),
                Artist.name.ilike(f"%%{flat_query}%%"))
               ).all()
    albums = session.query(Album).filter(
            Album.title.ilike(f"%%{query}%%")).all()
    tracks = session.query(Track).filter(
            Track.title.ilike(f"%%{query}%%")).all()

    return dict(artists=artists,
                albums=albums,
                tracks=tracks)

def prompt(message, default=None, strip=True, suffix=' '):
        Register a Validator class for file verification.

        :param validator:
        :return:
        self.update_config()
        path = self.config['path']
        encoder = self.config['encoder']
        fps = self.config['fps']
        bit16 = self.config['16bit']
        numpy_image = in_frame.as_numpy()
        ylen, xlen, bpc = numpy_image.shape
        if bpc == 3:
            if in_frame.type != 'RGB':
                self.logger.warning('Expected RGB input, got %s', in_frame.type)
            pix_fmt = ('rgb24', 'rgb48le')[bit16]
        elif bpc == 1:
            if in_frame.type != 'Y':
                self.logger.warning('Expected Y input, got %s', in_frame.type)
            pix_fmt = ('gray', 'gray16le')[bit16]
        else:
            self.logger.critical(
                'Cannot write %s frame with %d components', in_frame.type, bpc)
            return
        md = Metadata().copy(in_frame.metadata)
        audit = md.get('audit')
        audit += '%s = data\n' % path
        audit += '    encoder: "%s"\n' % (encoder)
        audit += '    16bit: %s\n' % (self.config['16bit'])
        md.set('audit', audit)
        md.to_file(path)
        with self.subprocess(
                ['ffmpeg', '-v', 'warning', '-y', '-an',
                 '-s', '%dx%d' % (xlen, ylen),
                 '-f', 'rawvideo', '-c:v', 'rawvideo',
                 '-r', '%d' % fps, '-pix_fmt', pix_fmt, '-i', '-',
                 '-r', '%d' % fps] + encoder.split() + [path],
                stdin=subprocess.PIPE) as sp:
            while True:
                in_frame = yield True
                if not in_frame:
                    break
                if bit16:
                    numpy_image = in_frame.as_numpy(dtype=pt_float)
                    numpy_image = numpy_image * pt_float(256.0)
                    numpy_image = numpy_image.clip(
                        pt_float(0), pt_float(2**16 - 1)).astype(numpy.uint16)
                else:
                    numpy_image = in_frame.as_numpy(dtype=numpy.uint8)
                sp.stdin.write(numpy_image.tostring())
                del in_frame

def HHIPreFilter(config={}):

    fil = numpy.array(
        [-4, 8, 25, -123, 230, 728, 230, -123, 25, 8, -4],
        dtype=numpy.float32).reshape((-1, 1, 1)) / numpy.float32(1000)
    resize = Resize(config=config)
    out_frame = Frame()
    out_frame.data = fil
    out_frame.type = 'fil'
    audit = out_frame.metadata.get('audit')
    audit += 'data = HHI pre-interlace filter\n'
    out_frame.metadata.set('audit', audit)
    resize.filter(out_frame)
    return resize

def port_ranges():
    try:
        return _linux_ranges()
    except (OSError, IOError): # not linux, try BSD
        try:
            ranges = _bsd_ranges()
            if ranges:
                return ranges
        except (OSError, IOError):
            pass

    # fallback
    return [DEFAULT_EPHEMERAL_PORT_RANGE]

def run(self):
        try:
            self.owner.start_event()
            while True:
                while not self.incoming:
                    time.sleep(0.01)
                while self.incoming:
                    command = self.incoming.popleft()
                    if command is None:
                        raise StopIteration()
                    command()
        except StopIteration:
            pass
        self.owner.stop_event()

def send(self, output_name, frame):
        for input_method in self._component_connections[output_name]:
            input_method(frame)

def start_event(self):
        # create object pool for each output
        if self.with_outframe_pool:
            self.update_config()
            for name in self.outputs:
                self.outframe_pool[name] = ObjectPool(
                    Frame, self.new_frame, self.config['outframe_pool_len'])
        try:
            self.on_start()
        except Exception as ex:
            self.logger.exception(ex)
            raise StopIteration()

def stop_event(self):
        self.logger.debug('stopping')
        try:
            self.on_stop()
        except Exception as ex:
            self.logger.exception(ex)
        for name in self.outputs:
            self.send(name, None)

def is_pipe_end(self):
        for name in self.outputs:
            if self._component_connections[name]:
                return False
        return True

def new_config_event(self):
        try:
            self.on_set_config()
        except Exception as ex:
            self.logger.exception(ex)
            raise StopIteration()

def new_frame_event(self):
        # check output frames are available
        for out_pool in self.outframe_pool.values():
            if not out_pool.available():
                return
        # check input frames are available, and get current frame numbers
        frame_nos = {}
        for in_buff in self.input_buffer.values():
            if not in_buff.available():
                return
            in_frame = in_buff.peek()
            if in_frame is None:
                raise StopIteration()
            if in_frame.frame_no >= 0:
                frame_nos[in_buff] = in_frame.frame_no
            else:
                # discard any superseded 'static' input
                while in_buff.available() > 1 and in_buff.peek(1) is not None:
                    in_buff.get()
        if len(frame_nos) > 1:
            frame_no = max(frame_nos.values())
            # discard old frames that can never be used
            for in_buff in frame_nos:
                while frame_nos[in_buff] < frame_no and in_buff.available() > 1:
                    in_buff.get()
                    in_frame = in_buff.peek()
                    if in_frame is None:
                        raise StopIteration()
                    frame_nos[in_buff] = in_frame.frame_no
            # check for complete set of matching frame numbers
            if min(frame_nos.values()) != max(frame_nos.values()):
                return
        # now have a full set of correlated inputs to process
        try:
            self.process_frame()
        except StopIteration:
            raise
        except Exception as ex:
            self.logger.exception(ex)
            raise StopIteration()

def get_tree_members(self):


    If item has dots (eg: 'foo.bar.baz'), recursively call getattribute():
    e = getattr(model, 'foo')
    e = getattr(e, 'bar')
    e = getattr(e, 'baz')
    At each step, check if e is a callable, and if so, use e()

    if hasattr(item, 'all'):  # RelatedManager: display a list
        return ', '.join(map(text_type, item.all()))
    return item

def mostCommonItem(lst):
    # This elegant solution from: http://stackoverflow.com/a/1518632/1760218
    lst = [l for l in lst if l]
    if lst:
        return max(set(lst), key=lst.count)
    else:
        return None

def safeDbUrl(db_url):
        if obj is None:
            return []
        return super(ExportAdmin, self).get_readonly_fields(request, obj)

def response_add(self, request, obj, post_url_continue=POST_URL_CONTINUE):
        if '_addanother' not in request.POST and '_popup' not in request.POST:
            request.POST['_continue'] = 1
        return super(ExportAdmin, self).response_add(request,
                                                     obj,
                                                     post_url_continue)

def above_score_threshold(new_data, old_data, strict=False, threshold=PYLINT_SCORE_THRESHOLD):
    success = True
    score = 0
    message = ''
    if strict:
        for fscore, fname in new_data['scores']:
            if fscore < threshold:
                success = False
                score = -1
                message += "File {} score ({}) below threshold {}\n".format(fname, fscore, threshold)

        return success, score, message

    else:
        if new_data['average'] < threshold:
            success = False
            message = "Failed! Average pylint score ({})" \
                      " below threshold (9)!".format(new_data['average'])
            score = -1

    return success, score, message

def run(self, new_pylint_data, old_pylint_data):
        for validator in self.checkers:
            success, score, message = validator(new_pylint_data, old_pylint_data)
            if not success:
                return score, message


        message = self.default_message.format(new_pylint_data['average'])

        return self.default_score, message

def _get_document(self, source):
    Returns random unused port number.
    Returns a list of 'good' port ranges.
    Such ranges are large and don't contain ephemeral or well-known ports.
    Ranges borders are also excluded.
    Returns if port is used. Port is considered used if the current process
    can't bind to it or the port doesn't refuse connections.
    Most recent day, if it's during the Advent of Code. Happy Holidays!
    Day 1 is assumed, otherwise.
        Register a Linter class for file verification.

        :param linter:
        :return:

    return query.replace("\\", r"\5C").replace("*", r"\2A").replace("(", r"\28").replace(")", r"\29")

def _validate_xor_args(self, p):
        if len(p[1]) != 2:
            raise ValueError('Invalid syntax: XOR only accepts 2 arguments, got {0}: {1}'.format(len(p[1]), p))

def _match_value_filter(self, p, value):
        return self._VALUE_FILTER_MAP[p[0]](value[p[1]], p[2])

def get_field_keys(self, pattern=None):
        # Use own pattern or passed in argument for recursion
        pattern = pattern or self.pattern

        # Validate the pattern so we can make assumptions about the data
        self._validate(pattern)

        keys = set()

        # Valid pattern length can only be 2 or 3
        # With key filters, field key is second item just like 3 item patterns
        if len(pattern) == 2 and pattern[0] not in self._KEY_FILTER_MAP:
            if pattern[0] in ('&', '|', '^'):
                # Pass each nested pattern to get_field_keys
                for filter_item in pattern[1]:
                    keys = keys.union(self.get_field_keys(filter_item))
            else:
                # pattern[0] == '!'
                keys = keys.union(self.get_field_keys(pattern[1]))
        else:
            # Pattern length is 3
            keys.add(pattern[1])
        return keys

def to_file(self, path):
        xmp_path = path + '.xmp'
        # remove any existing XMP file
        if os.path.exists(xmp_path):
            os.unlink(xmp_path)
        # attempt to open image/video file for metadata
        md_path = path
        md = GExiv2.Metadata()
        try:
            md.open_path(md_path)
        except GLib.GError:
            # file type does not support metadata so use XMP sidecar
            md_path = xmp_path
            # create empty XMP file
            with open(md_path, 'w') as of:
<?xpacket end="w"?>''')
            md = GExiv2.Metadata()
            md.open_path(md_path)
        # add our namespace
        md.register_xmp_namespace(
            'https://github.com/jim-easterbrook/pyctools', 'pyctools')
        # copy metadata
        for tag, value in self.data.items():
            if md.get_tag_type(tag) in ('XmpBag', 'XmpSeq'):
                md.set_tag_multiple(tag, value)
            else:
                md.set_tag_string(tag, value)
        if self.comment is not None:
            md.set_comment(self.comment)
        # save file
        md.save_file(md_path)

def image_size(self):
        xlen = None
        ylen = None
        for tag in ('Xmp.pyctools.xlen', 'Exif.Photo.PixelXDimension',
                    'Exif.Image.ImageWidth', 'Xmp.tiff.ImageWidth'):
            if tag in self.data:
                xlen = int(self.data[tag])
                break
        for tag in ('Xmp.pyctools.ylen', 'Exif.Photo.PixelYDimension',
                    'Exif.Image.ImageLength', 'Xmp.tiff.ImageLength'):
            if tag in self.data:
                ylen = int(self.data[tag])
                break
        if xlen and ylen:
            return xlen, ylen
        raise RuntimeError('Metadata does not have image dimensions')

def get(self, tag, default=None):
        full_tag = 'Xmp.pyctools.' + tag
        if full_tag in self.data:
            return self.data[full_tag]

        return default

def set(self, tag, value):
        full_tag = 'Xmp.pyctools.' + tag
        self.data[full_tag] = value

def get_connection(self, is_read_only=False) -> redis.StrictRedis:
        if self.connection is not None:
            return self.connection

        if self.is_sentinel:
            kwargs = dict()
            if self.password:
                kwargs["password"] = self.password
            sentinel = Sentinel([(self.host, self.port)], **kwargs)
            if is_read_only:
                connection = sentinel.slave_for(self.sentinel_service, decode_responses=True)
            else:
                connection = sentinel.master_for(self.sentinel_service, decode_responses=True)
        else:
            connection = redis.StrictRedis(host=self.host, port=self.port, decode_responses=True,
                                           password=self.password)
        self.connection = connection
        return connection

def calculate_hash_for_file(name):
    '''
    longlongformat = 'q'  # long long
    bytesize = struct.calcsize(longlongformat)

    f = open(name, "rb")

    filesize = os.path.getsize(name)
    hash = filesize

    minimum_size = 65536 * 2
    assert filesize >= minimum_size, \
        'Movie {name} must have at least {min} bytes'.format(min=minimum_size,
                                                             name=name)

    for x in range(65536//bytesize):
        buffer = f.read(bytesize)
        (l_value,)= struct.unpack(longlongformat, buffer)
        hash += l_value
        hash = hash & 0xFFFFFFFFFFFFFFFF #to remain as 64bit number


    f.seek(max(0,filesize-65536),0)
    for x in range(65536//bytesize):
        buffer = f.read(bytesize)
        (l_value,)= struct.unpack(longlongformat, buffer)
        hash += l_value
        hash = hash & 0xFFFFFFFFFFFFFFFF

    f.close()
    returnedhash = "%016x" % hash
    return returnedhash

def set_config(self, config):
        # put copy of config on queue for running component
        self._configmixin_queue.append(copy.deepcopy(config))
        # notify component, using thread safe method
        self.new_config()

def GaussianFilterCore(x_sigma=0.0, y_sigma=0.0):

    def filter_1D(sigma):
        alpha = 1.0 / (2.0 * (max(sigma, 0.0001) ** 2.0))
        coefs = []
        coef = 1.0
        while coef > 0.0001:
            coefs.append(coef)
            coef = math.exp(-(alpha * (float(len(coefs) ** 2))))
        fil_dim = len(coefs) - 1
        result = numpy.zeros(1 + (fil_dim * 2), dtype=numpy.float32)
        for n, coef in enumerate(coefs):
            result[fil_dim - n] = coef
            result[fil_dim + n] = coef
        # normalise result
        result /= result.sum()
        return result

    x_sigma = max(x_sigma, 0.0)
    y_sigma = max(y_sigma, 0.0)
    x_fil = filter_1D(x_sigma)
    y_fil = filter_1D(y_sigma)
    result = numpy.empty(
        [y_fil.shape[0], x_fil.shape[0], 1], dtype=numpy.float32)
    for y in range(y_fil.shape[0]):
        for x in range(x_fil.shape[0]):
            result[y, x, 0] = x_fil[x] * y_fil[y]
    out_frame = Frame()
    out_frame.data = result
    out_frame.type = 'fil'
    audit = out_frame.metadata.get('audit')
    audit += 'data = GaussianFilter()\n'
    if x_sigma != 0.0:
        audit += '    x_sigma: %g\n' % (x_sigma)
    if y_sigma != 0.0:
        audit += '    y_sigma: %g\n' % (y_sigma)
    out_frame.metadata.set('audit', audit)
    return out_frame

def queue_command(self, command):
        if self._running:
            # queue event normally
            QtCore.QCoreApplication.postEvent(
                self, ActionEvent(command), QtCore.Qt.LowEventPriority)
        else:
            # save event until we are started
            self._incoming.append(command)

def join(self, timeout=3600):
        start = time.time()
        while self._running:
            now = time.time()
            maxtime = timeout + start - now
            if maxtime <= 0:
                return
            QCoreApplication.processEvents(
                QEventLoop.AllEvents, int(maxtime * 1000))

def IntraField(config={}):

    return Compound(
        config = config,
        deint = SimpleDeinterlace(),
        interp = Resize(),
        filgen = FilterGenerator(yaperture=8, ycut=50),
        gain = Arithmetic(func='data * pt_float(2)'),
        linkages = {
            ('self',   'input')  : [('deint',  'input')],
            ('deint',  'output') : [('interp', 'input')],
            ('interp', 'output') : [('self',   'output')],
            ('filgen', 'output') : [('gain',   'input')],
            ('gain',   'output') : [('interp', 'filter')],
            }
        )

def create(self, period: int, limit: int):
        self.period = period
        self.limit = limit

def is_rate_limited(self, namespace: str) -> bool:
        return not self.__can_attempt(namespace=namespace, add_attempt=False)

def main():
    credentials = get_credentials()
    http = credentials.authorize(httplib2.Http())
    service = discovery.build('calendar', 'v3', http=http)

    now = datetime.datetime.utcnow().isoformat() + 'Z' # 'Z' indicates UTC time
    print('Getting the upcoming 10 events')
    eventsResult = service.events().list(
        calendarId='primary', timeMin=now, maxResults=10, singleEvents=True,
        orderBy='startTime').execute()
    events = eventsResult.get('items', [])

    if not events:
        print('No upcoming events found.')
    for event in events:
        start = event['start'].get('dateTime', event['start'].get('date'))
        print(start, event['summary'])

def fix_list_arguments(self):
        # Currently the pattern will not be equivalent, but more "narrow",
        # although good enough to reason about list arguments.
        if not hasattr(self, 'children'):
            return Either(Required(self))
        else:
            ret = []
            groups = [[self]]
            while groups:
                children = groups.pop(0)
                types = [type(c) for c in children]
                if Either in types:
                    either = [c for c in children if type(c) is Either][0]
                    children.pop(children.index(either))
                    for c in either.children:
                        groups.append([c] + children)
                elif Required in types:
                    required = [c for c in children if type(c) is Required][0]
                    children.pop(children.index(required))
                    groups.append(list(required.children) + children)
                elif Optional in types:
                    optional = [c for c in children if type(c) is Optional][0]
                    children.pop(children.index(optional))
                    groups.append(list(optional.children) + children)
                elif OneOrMore in types:
                    oneormore = [c for c in children if type(c) is OneOrMore][0]
                    children.pop(children.index(oneormore))
                    groups.append(list(oneormore.children) * 2 + children)
                else:
                    ret.append(children)
            return Either(*[Required(*e) for e in ret])

def syncImage(img, current, session):

        This is done at the formset level as there's no other way i could find
        to get the parent object (stored in self.instance), and the form at the
        same time.

    package name.

    From a list of args, extract the one param if supplied,
    returning the value and unused args.

    >>> extract_param('port', ['foo', '--port=999', 'bar'], type=int)
    (999, ['foo', 'bar'])
    >>> extract_param('port', ['foo', '--port', '999', 'bar'], type=int)
    (999, ['foo', 'bar'])
    >>> extract_param('port', ['foo', 'bar'])
    (None, ['foo', 'bar'])

    Args:
        languages (list): A list of supported languages.
    for language in set(SUPPORTED_LANGUAGES) - {"en"}:
        language_stemmer = partial(nltk_stemmer, get_language_stemmer(language))
        Pipeline.register_function(language_stemmer, "stemmer-{}".format(language))

def ordinal(value):
    '''

    try:
        value = int(value)
    except (TypeError, ValueError):
        raise ValueError

    if value % 100 in (11, 12, 13):
        return '%d%s' % (value, ORDINAL_SUFFIX[0])
    else:
        return '%d%s' % (value, ORDINAL_SUFFIX[value % 10])

def percentage(value, digits=2):
    '''

    value = float(value) * 100.0
    return u'' + '%s %%' % (_format(value, digits),)

def word(value, digits=2):
    '''

    convention = locale.localeconv()
    decimal_point = convention['decimal_point']
    decimal_zero = re.compile(r'%s0+' % re.escape(decimal_point))
    prefix = value < 0 and '-' or ''
    value = abs(int(value))
    if value < 1000:
        return u''.join([
            prefix,
            decimal_zero.sub('', _format(value, digits)),
        ])

    for base, suffix in enumerate(LARGE_NUMBER_SUFFIX):
        exp = (base + 2) * 3
        power = 10 ** exp
        if value < power:
            value = value / float(10 ** (exp - 3))
            return ''.join([
                prefix,
                decimal_zero.sub('', _format(value, digits)),
                ' ',
                suffix,
            ])

    raise OverflowError

def _full_rename(args):
    return (
        args.ns and
        all(map(args.rename.affects, args.ns))
    )

def apply(db, op):
    dbname = op['ns'].split('.')[0] or "admin"
    opts = bson.CodecOptions(uuid_representation=bson.binary.STANDARD)
    db[dbname].command("applyOps", [op], codec_options=opts)

def since(self, ts):
        spec = {'ts': {'$gt': ts}}
        cursor = self.query(spec)
        while True:
            # todo: trap InvalidDocument errors:
            # except bson.errors.InvalidDocument as e:
            #  logging.info(repr(e))
            for doc in cursor:
                yield doc
            if not cursor.alive:
                break
            time.sleep(1)

def has_ops_before(self, ts):
        spec = {'ts': {'$lt': ts}}
        return bool(self.coll.find_one(spec))

def since(self, ts):
        while True:
            items = super(TailingOplog, self).since(ts)
            for doc in items:
                yield doc
                ts = doc['ts']

def dump(self, stream):
        items = (
            ('time', self.time),
            ('inc', self.inc),
        )
        # use ordered dict to retain order
        ts = collections.OrderedDict(items)
        json.dump(dict(ts=ts), stream)

def load(cls, stream):
        data = json.load(stream)['ts']
        return cls(data['time'], data['inc'])

def for_window(cls, window):
        utcnow = datetime.datetime.utcnow()
        return cls(utcnow - window, 0)

def save(self, ts):
        with open(self, 'w') as f:
            Timestamp.wrap(ts).dump(f)

def Tokenizer(obj, metadata=None, separator=SEPARATOR):
    if obj is None:
        return []

    metadata = metadata or {}

    if isinstance(obj, (list, tuple)):
        return [
            Token(as_string(element).lower(), deepcopy(metadata)) for element in obj
        ]

    string = str(obj).strip().lower()
    length = len(string)
    tokens = []
    slice_start = 0
    for slice_end in range(length):
        char = string[slice_end]
        slice_length = slice_end - slice_start
        if separator.match(char) or slice_end == length - 1:
            if slice_length > 0:
                sl = slice(slice_start, slice_end if slice_end < length - 1 else None)

                token_metadata = {}
                token_metadata["position"] = [
                    slice_start,
                    slice_length if slice_end < length - 1 else slice_length + 1,
                ]
                token_metadata["index"] = len(tokens)
                token_metadata.update(metadata)

                tokens.append(Token(string[sl], token_metadata))

            slice_start = slice_end + 1

    return tokens

def all_collections(db):
	include_pattern = r'(?!system\.)'
	return (
		db[name]
		for name in db.list_collection_names()
		if re.match(include_pattern, name)
	)

def safe_purge_collection(coll):
	op = (
		drop_collection
		if coll.options().get('capped', False)
		else purge_collection
	)
	return op(coll)

def generate_stop_word_filter(stop_words, language=None):


    def stop_word_filter(token, i=None, tokens=None):
        if token and str(token) not in stop_words:
            return token

    # camelCased for for compatibility with lunr.js
    label = (
        "stopWordFilter-{}".format(language)
        if language is not None
        else "stopWordFilter"
    )
    Pipeline.register_function(stop_word_filter, label)
    return stop_word_filter

def pesn(number, separator=u''):
    '''

    number = re.sub(r'[\s-]', '', meid(number))
    serial = hashlib.sha1(unhexlify(number[:14]))
    return separator.join(['80', serial.hexdigest()[-6:].upper()])

def filesize(value, format='decimal', digits=2):
    '''

    if format not in FILESIZE_SUFFIX:
        raise TypeError

    base = FILESIZE_BASE[format]
    size = int(value)
    sign = size < 0 and u'-' or ''
    size = abs(size)

    for i, suffix in enumerate(FILESIZE_SUFFIX[format]):
        unit = base ** (i + 1)
        if size < unit:
            result = u''.join([
                sign,
                _format(base * size / float(unit), digits),
                u' ',
                suffix,
            ])
            if format == 'gnu':
                result = result.replace(' ', '')
            return result

    raise OverflowError

def create_dn_in_filter(filter_class, filter_value, helper):

        :param managed_object: MO classid
        :in_filter: input filter value
        :returns: Managed Object
        :raises: UcsException in case of failure.

        :param uuid: MO config
        :param p_dn: parent MO DN
        :param p_class_id: parent MO class ID
        :param class_id: MO class ID
        :param MO configuration: MO config
        :param mo_dn: MO DN value
        :param handle: optional UCS Manager handle object
        :returns: Managed Object
        :raises: UcsOperationError in case of failure.
    Build a projection for MongoDB.

    Due to https://jira.mongodb.org/browse/SERVER-3156, until MongoDB 2.6,
    the values must be integers and not boolean.

    >>> project(a=True) == {'a': 1}
    True

    Once MongoDB 2.6 is released, replace use of this function with a simple
    dict.
    Fetch exactly one matching document or upsert
    the document if not found, returning the matching
    or upserted document.

    See https://jira.mongodb.org/browse/SERVER-28434
    describing the condition where MongoDB is uninterested in
    providing an upsert and fetch behavior.

    >>> instance = getfixture('mongodb_instance').get_connection()
    >>> coll = instance.test_upsert_and_fetch.items
    >>> doc = {'foo': 'bar'}
    >>> inserted = upsert_and_fetch(coll, doc)
    >>> inserted
    {...'foo': 'bar'...}
    >>> upsert_and_fetch(coll, doc) == inserted
    True
        try:
            _LOGGER.debug("Updating device state.")
            key = ON_KEY if not self._flip_on_off else OFF_KEY
            self.state = self._device.readCharacteristic(HANDLE) == key
        except (bluepy.btle.BTLEException, AttributeError):
            if retry < 1 or not self._connect():
                self.available = False
                _LOGGER.error("Failed to update device state.", exc_info=True)
                return None
            return self.update(retry-1)
        self.available = True
        return None

def combine(self, other):
        for term in other.metadata.keys():
            if term not in self.metadata:
                self.metadata[term] = {}

            fields = other.metadata[term].keys()
            for field in fields:
                if field not in self.metadata[term]:
                    self.metadata[term][field] = {}

                keys = other.metadata[term][field].keys()
                for key in keys:
                    if key not in self.metadata[term][field]:
                        self.metadata[term][field][key] = other.metadata[term][field][
                            key
                        ]
                    else:
                        self.metadata[term][field][key].extend(
                            other.metadata[term][field][key]
                        )

def get_power_state(self):
        rn_array = [self.helper.service_profile,
             ManagedObject(NamingId.LS_POWER).MakeRn()]
        try:
            ls_power = ucs_helper.get_managed_object(
                           self.helper.handle,
                           LsPower.ClassId(),
                           {LsPower.DN: UcsUtils.MakeDn(rn_array)})
            if not ls_power:
                raise exception.UcsOperationError("get_power_state",
                          "Failed to get LsPower MO, configure valid "
                          "service-profile")
            return ls_power[0].getattr(LsPower.STATE)
        except UcsException as ex:
            raise exception.UcsOperationError(message=ex)

def set_power_state(self, desired_state):
        rn_array = [self.helper.service_profile,
             ManagedObject(NamingId.LS_POWER).MakeRn()]
        try:
            ls_power = ucs_helper.get_managed_object(self.helper.handle,
                                  LsPower.ClassId(),
                                  {LsPower.DN: UcsUtils.MakeDn(rn_array)})
            if not ls_power:
                raise exception.UcsOperationError("set_power_state",
                          "Failed to get power MO,"
                          " configure valid service-profile.")
            else:
                ls_power_set = self.helper.handle.SetManagedObject(
                                    ls_power,
                                    LsPower.ClassId(),
                                    {LsPower.STATE: desired_state},
                                    dumpXml=YesOrNo.TRUE
                                    )
                if ls_power_set:
                    power = ls_power_set.pop()
                    return power.getattr(LsPower.STATE)
                else:
                    return states.ERROR
        except Exception as ex:
            raise exception.UcsOperationError("set_power_state",
                  "Failed to get power MO,"
                  "configure valid servie-profile.")

def reboot(self):
        if self.get_power_state() == LsPower.CONST_STATE_DOWN:
            self.set_power_state(LsPower.CONST_STATE_UP)
        else:
            self.set_power_state(LsPower.CONST_STATE_HARD_RESET_IMMEDIATE)

def connect(uri, factory=pymongo.MongoClient):
    warnings.warn(
        "do not use. Just call MongoClient directly.", DeprecationWarning)
    return factory(uri)

def connect_gridfs(uri, db=None):
    return gridfs.GridFS(
        db or connect_db(uri),
        collection=get_collection(uri) or 'fs',
    )

def Compare(fromMo, toMo, diff):
	from UcsBase import UcsUtils, WriteUcsWarning
	from Mos import OrgOrg

	xMO = mObj.Clone()
	xMO.SetHandle(mObj.GetHandle())
	if (xlateOrg != None):
		matchObj = re.match(r'^(org-[\-\.:_a-zA-Z0-9]{1,16}/)*org-[\-\.:_a-zA-Z0-9]{1,16}', xMO.Dn)
		if matchObj:
			if UcsUtils.WordL(xMO.classId) == OrgOrg.ClassId():
				orgMoMeta = UcsUtils.GetUcsPropertyMeta(UcsUtils.WordU(OrgOrg.ClassId()), "Meta")
				if orgMoMeta == None:
					# TODO: Add Warning/Error messages in Logger.
					WriteUcsWarning('[Warning]: Could not translate [%s]' % (xMO.Dn))
					return xMO

				# Check for naming property
				matchObj1 = re.findall(r'(\[[^\]]+\])', orgMoMeta.rn)
				if matchObj1:
					UpdateMoDnAlongWithNamingProperties(xMO, orgMoMeta, xlateOrg)
				else:
					newDn = re.sub("%s" % (matchObj.group(0)), "%s" % (xlateOrg), xMO.Dn)
					# print "Translating", xMO.Dn, " => ", newDn
					xMO.Dn = newDn
			else:
				newDn = re.sub("^%s/" % (matchObj.group(0)), "%s/" % (xlateOrg), xMO.Dn)
				# print "Translating", xMO.Dn, " => ", newDn
				xMO.Dn = newDn

	if (xlateMap != None):
		originalDn = xMO.Dn
		if originalDn in xlateMap:
			xMoMeta = UcsUtils.GetUcsPropertyMeta(UcsUtils.WordU(xMO.classId), "Meta")
			if xMoMeta == None:
				# TODO: Add Warning/Error messages in Logger.
				WriteUcsWarning('[Warning]: Could not translate [%s]' % (originalDn))
				return xMO

			# Check for naming property
			matchObj = re.findall(r'(\[[^\]]+\])', xMoMeta.rn)
			if matchObj:
				UpdateMoDnAlongWithNamingProperties(xMO, xMoMeta, xlateMap[originalDn])
			else:
				# print "Translating", xMO.Dn, " => ", xlateMap[originalDn]
				xMO.Dn = xlateMap[originalDn]
		else:
			originalDn = re.sub(r'[/]*[^/]+$', '', originalDn)
			while (originalDn != None or originalDn == ""):
				if (not (originalDn in xlateMap)):
					originalDn = re.sub(r'[/]*[^/]+$', '', originalDn)
					continue

				newDn = re.sub("^%s/" % (originalDn), "%s/" % (xlateMap[originalDn]), xMO.Dn)
				# print "Translating", xMO.Dn, " => ", newDn
				xMO.Dn = newDn
				break

	return xMO

def ImportUcsSession(filePath, key):
	from UcsBase import UcsUtils, WriteUcsWarning, UcsValidationException
	# from p3 import p3_encrypt, p3_decrypt

	if filePath is None:
		raise UcsValidationException("filePath parameter is not provided.")
	# raise Exception('[Error]: Please provide filePath')

	if key is None:
		raise UcsValidationException("key parameter is not provided.")
	# raise Exception('[Error]: Please provide key')

	if not os.path.isfile(filePath) or not os.path.exists(filePath):
		raise UcsValidationException('[Error]: File <%s> does not exist ' % (filePath))
	# raise Exception('[Error]: File <%s> does not exist ' %(filePath))

	doc = xml.dom.minidom.parse(filePath)
	topNode = doc.documentElement
	# print topNode.localName

	if topNode is None or topNode.localName != UcsLoginXml.UCS_HANDLES:
		return None

	if (topNode.hasChildNodes()):
		# childList = topNode._get_childNodes()
		# childCount = childList._get_length()
		childList = topNode.childNodes
		childCount = len(childList)
		for i in range(childCount):
			childNode = childList.item(i)
			if (childNode.nodeType != Node.ELEMENT_NODE):
				continue

			if childNode.localName != UcsLoginXml.UCS:
				continue

			lName = None
			lUsername = None
			lPassword = None
			lNoSsl = False
			lPort = None

			if childNode.hasAttribute(UcsLoginXml.NAME):
				lName = childNode.getAttribute(UcsLoginXml.NAME)

			if childNode.hasAttribute(UcsLoginXml.USER_NAME):
				lUsername = childNode.getAttribute(UcsLoginXml.USER_NAME)

			if childNode.hasAttribute(UcsLoginXml.PASSWORD):
				# lPassword = p3_decrypt(childNode.getAttribute(UcsLoginXml.PASSWORD), key)
				lPassword = UcsUtils.DecryptPassword(childNode.getAttribute(UcsLoginXml.PASSWORD), key)

			if childNode.hasAttribute(UcsLoginXml.NO_SSL):
				lNoSsl = childNode.getAttribute(UcsLoginXml.NO_SSL)

			if childNode.hasAttribute(UcsLoginXml.PORT):
				lPort = childNode.getAttribute(UcsLoginXml.PORT)

			# Process Login
			if ((lName is None) or (lUsername == None) or (lPassword == None)):
				# WriteUcsWarning("[Warning] Insufficient information for login ...")
				continue
			try:

				handle = UcsHandle()
				handle.Login(name=lName, username=lUsername, password=lPassword, noSsl=lNoSsl, port=lPort)


			except Exception, err:
				# TODO: Add Warning/Error messages in Logger.
				WriteUcsWarning("[Connection Error<%s>] %s" % (lName, str(err)))

def Uri(self):
		from Ucs import ConfigMap

		self._transactionInProgress = False
		self._configMap = ConfigMap()

def CompleteTransaction(self, dumpXml=None):
		from Ucs import ConfigMap, Pair
		from UcsBase import ManagedObject, WriteUcsWarning, WriteObject, UcsException

		self._transactionInProgress = False
		ccm = self.ConfigConfMos(self._configMap, YesOrNo.FALSE, dumpXml)
		self._configMap = ConfigMap()
		if ccm.errorCode == 0:
			moList = []
			for child in ccm.OutConfigs.GetChild():
				if (isinstance(child, Pair) == True):
					for mo in child.GetChild():
						moList.append(mo)
				elif (isinstance(child, ManagedObject) == True):
					moList.append(child)
			# WriteObject(moList)
			return moList
		else:
			# raise Exception('[Error]: CompleteTransaction [Code]:' + ccm.errorCode + ' [Description]:' + ccm.errorDescr)
			raise UcsException(ccm.errorCode, ccm.errorDescr)

def XmlRawQuery(self, xml, dumpXml=None):
		from UcsBase import UcsException

		if (self._cookie == None):
			return True

		if self._refreshTimer:
			self._refreshTimer.cancel()

		response = self.AaaLogout(dumpXml)
		self._cookie = None
		self._lastUpdateTime = str(time.asctime())
		self._domains = None
		self._priv = None
		self._sessionId = None
		self._version = None


		if self._ucs in defaultUcs:

			del defaultUcs[self._ucs]

		if (response.errorCode != 0):
			raise UcsException(response.errorCode, response.errorDescr)
		# raise Exception('[Error]: Logout [Code]:' + response.errorCode + '[Description]:' + response.errorDescr)

		return True

def _Start_refresh_timer(self):
		self._enqueueThreadSignal.acquire()
		self._enqueueThread = Thread(target=self._enqueue_function)
		self._enqueueThread.daemon = True
		self._enqueueThread.start()
		self._enqueueThreadSignal.wait()
		self._enqueueThreadSignal.release()

def _add_watch_block(self, params, filterCb, capacity=500, cb=None):
		if (self._wbslock == None):
			self._wbslock = Lock()

		self._wbslock.acquire()
		self._wbs.remove(wb)
		if len(self._wbs) == 0:
			self._stop_enqueue_thread()
			self._stop_dequeue_thread()

		self._wbslock.release()

def RemoveEventHandler(self, wb):
		self._dequeueThread = Thread(target=self._dequeue_function)
		self._dequeueThread.daemon = True
		self._dequeueThread.start()

def StartGuiSession(self):
		Imports backUp.
		
		This operation will upload the UCSM backup taken earlier via GUI or BackupUcs operation for all configuration, system configuration, and 
		logical configuration files. User can perform an import while the system is up and running.
		- path specifies path of the backup file.
		- merge specifies whether to merge the backup configuration with the existing UCSM configuration. 
		Uploads a specific CCO Image on UCS.
		
		- path specifies the path of the image to be uploaded.
		Gets Child Managed Object from UCS.

		- in_mo, if provided, it acts as a parent for the present operation. (required if in_dn is None).
		- in_dn, parent mo dn (required if in_mo is None)
		- class_id of the managed object/s to get.(optional)
		- in_hierarchical, Explores hierarchy if true, else returns managed objects at a single level.(optional)

        Unless the clause contains the fields to be matched all fields will be

        matched. In addition a default boost of 1 is applied to the clause.

        If the first argument is a `lunr.Clause` it will be mutated and added,
        otherwise args and kwargs will be used in the constructor.

        Returns:
            lunr.Query: The Query itself.
        the list of clauses making up this Query.

        The term is not tokenized and used "as is". Any conversion to token
        or token-like strings should be performed before calling this method.

        For example:
            query.term(lunr.Tokenizer("foo bar"))

        Args:
            term (Token or iterable): Token or iterable of tokens to add.
            kwargs (dict): Additional properties to add to the Clause.
        prohibited. These queries require some special processing to return
        the expected results.
        recipients can be either a list of emails or a list of users,
        if it is users the system will change to the language that the
        user has set as theyr mother toungue
	# from UcsBase import UcsUtils
	classNotFound = False
	if (UcsUtils.FindClassIdInMoMetaIgnoreCase(mo.classId) == None):
		classNotFound = True

	tabsize = 8
	outstr = "\n"
	if classNotFound:
		outstr += "Managed Object\t\t\t:\t" + str(UcsUtils.WordU(mo.classId)) + "\n"
	else:
		outstr += "Managed Object\t\t\t:\t" + str(mo.propMoMeta.name) + "\n"
	outstr += "-" * len("Managed Object") + "\n"
	if (not classNotFound):
		for prop in UcsUtils.GetUcsPropertyMetaAttributeList(mo.propMoMeta.name):
			propMeta = UcsUtils.GetUcsPropertyMeta(mo.propMoMeta.name, prop)
			if (propMeta.access == UcsPropertyMeta.Internal):
				continue
			val = mo.getattr(prop)
			# if val != None and val != "":
			outstr += str(prop).ljust(tabsize * 4) + ':' + str(val) + "\n"
	else:
		for prop in mo.__dict__:
			if (prop in ['classId', 'XtraProperty', 'handle', 'propMoMeta', 'dirtyMask', 'child']):
				continue
			val = mo.__dict__[prop]
			outstr += str(UcsUtils.WordU(prop)).ljust(tabsize * 4) + ':' + str(val) + "\n"
	if mo.__dict__.has_key('XtraProperty'):
		for xtraProp in mo.__dict__['XtraProperty']:
			outstr += '[X]' + str(UcsUtils.WordU(xtraProp)).ljust(tabsize * 4) + ':' + str(
				mo.__dict__['XtraProperty'][xtraProp]) + "\n"

	outstr += str("Ucs").ljust(tabsize * 4) + ':' + str(mo.handle._ucs) + "\n"

	outstr += "\n"
	return outstr

def WriteObject(moList):
		ch = []
		for c in self.child:
			ch.append(c.WriteXml(w, option))
		return ch

def setattr(self, key, value):
			self.__dict__['XtraProperty'][UcsUtils.WordU(key)] = value

def getattr(self, key):
					return self.__dict__[key]
			else:
				if self.__dict__.has_key('XtraProperty'):
					if self.__dict__['XtraProperty'].has_key(key):
						return self.__dict__['XtraProperty'][UcsUtils.WordU(key)]
					else:
						raise AttributeError(key)
				else:
					# TODO: Add Warning/Error messages in Logger.
					print "No XtraProperty in mo:", self.classId, " key:", key
		else:
		if ((UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId) == None) and (not self.IsDirty())):
			self.dirtyMask = ManagedObject.DUMMYDIRTY
		else:
			self.dirtyMask = self.propMoMeta.mask

def MakeRn(self):
		self.SetHandle(handle)
		if node.hasAttributes():
			# attributes = node._get_attributes()
			# attCount = attributes._get_length()
			attributes = node.attributes
			attCount = len(attributes)
			for i in range(attCount):
				attNode = attributes.item(i)
				# attr = UcsUtils.WordU(attNode._get_name())
				attr = UcsUtils.WordU(attNode.localName)
				if (UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId) != None):
					if (attr in UcsUtils.GetUcsPropertyMetaAttributeList(self.classId)):
						# self.setattr(attr, str(attNode.nodeValue))
						self.setattr(attr, str(attNode.value))
					else:
						# self.setattr(UcsUtils.WordU(attr), str(attNode.nodeValue))
						self.setattr(UcsUtils.WordU(attr), str(attNode.value))
				else:
					# self.setattr(UcsUtils.WordU(attr), str(attNode.nodeValue))
					self.setattr(UcsUtils.WordU(attr), str(attNode.value))

			if self.getattr("Rn") == None and self.getattr("Dn") != None:
				self.setattr("Rn", str(re.sub(r'^.*/', '', self.getattr("Dn"))))

		if (node.hasChildNodes()):
			# childList = node._get_childNodes()
			# childCount = childList._get_length()
			childList = node.childNodes
			childCount = len(childList)
			for i in range(childCount):
				childNode = childList.item(i)
				if (childNode.nodeType != Node.ELEMENT_NODE):
					continue

				if childNode.localName in self.propMoMeta.fieldNames:
					# .LoadFromXml(childNode, handle)
					pass
				# TODO: Need code analysis.
				# if childNode.localName in self.propMoMeta.childFieldNames:
				c = ManagedObject(UcsUtils.WordU(childNode.localName))
				self.child.append(c)
				c.LoadFromXml(childNode, handle)

def setattr(self, key, value):
			# print "No such property ClassId: %s Property:%s" %(self.classId, key)
			return None

def getattr(self, key):
			return self.__dict__[key]
		else:
		self.errorCode = errorCode
		self.errorDescr = errorDescr
		self.response = "yes"
		return self

def GetUcsPropertyMeta(classId, key):
		if classId in _MethodFactoryMeta:
			if key in _MethodFactoryMeta[classId]:
				return _MethodFactoryMeta[classId][key]
		return None

def GetUcsPropertyMetaAttributeList(classId):
		if classId in _ManagedObjectMeta:
			for prop in _ManagedObjectMeta[classId]:
				if (prop.lower() == key.lower()):
					return _ManagedObjectMeta[classId][prop]
		if classId in _MethodFactoryMeta:
			for prop in _MethodFactoryMeta[classId]:
				if (prop.lower() == key.lower()):
					return _MethodFactoryMeta[classId][prop]
		return None

def CheckRegistryKey(javaKey):
		import os, platform

		# Get JavaPath for Ubuntu
		# if os.name == "posix":
		if platform.system() == "Linux":
			path = os.environ.get('JAVA_HOME')
			if not path:
				raise UcsValidationException(
					"Please make sure JAVA is installed and variable JAVA_HOME is set properly.")
			# raise Exception("Please make sure JAVA is installed and variable JAVA_HOME is set properly.")
			else:
				path = os.path.join(path, 'bin')
				path = os.path.join(path, 'javaws')
				if not os.path.exists(path):
					raise UcsValidationException("javaws is not installed on System.")
				# raise Exception("javaws is not installed on System.")
				else:
					return path

		# Get JavaPath for Windows
		# elif os.name == "nt":
		elif platform.system() == "Windows" or platform.system() == "Microsoft":

			path = os.environ.get('JAVA_HOME')

			if path == None:
				path = UcsUtils.CheckRegistryKey(r"SOFTWARE\\JavaSoft\\Java Runtime Environment\\")

			if path == None:  # Check for 32 bit Java on 64 bit machine.
				path = UcsUtils.CheckRegistryKey(r"SOFTWARE\\Wow6432Node\\JavaSoft\\Java Runtime Environment")

			if not path:
				raise UcsValidationException("Please make sure JAVA is installed.")
			# raise Exception("Please make sure JAVA is installed.")
			else:
				path = os.path.join(path, 'bin')
				path = os.path.join(path, 'javaws.exe')
				if not os.path.exists(path):
					raise UcsValidationException("javaws.exe is not installed on System.")
				# raise Exception("javaws.exe is not installed on System.")
				else:
					return path

def DownloadFile(hUcs, source, destination):
		import urllib2
		from sys import stdout
		from time import sleep

		httpAddress = "%s/%s" % (hUcs.Uri(), source)
		file_name = httpAddress.split('/')[-1]

		req = urllib2.Request(httpAddress)  # send the new url with the cookie.
		req.add_header('Cookie', 'ucsm-cookie=%s' % (hUcs._cookie))
		res = urllib2.urlopen(req)

		meta = res.info()
		file_size = int(meta.getheaders("Content-Length")[0])
		print "Downloading: %s Bytes: %s" % (file_name, file_size)

		f = open(destination, 'wb')
		file_size_dl = 0
		block_sz = 8192
		while True:
			rBuffer = res.read(block_sz)
			if not rBuffer:
				break

			file_size_dl += len(rBuffer)
			f.write(rBuffer)
			status = r"%10d  [%3.2f%%]" % (file_size_dl, file_size_dl * 100. / file_size)
			status = status + chr(8) * (len(status) + 1)
			stdout.write("\r%s" % status)
			stdout.flush()
		# print status

		f.close()

def GetSyncMoConfigFilePath():
		moConfigMap = {}
		configList = ConfigDoc.getElementsByTagName("mo")

		for moConfigNode in configList:
			classId = None
			noun = None
			version = None
			actionVersion = None
			action = None
			ignoreReason = None
			status = None
			excludeList = None

			if moConfigNode.hasAttribute("classid"):
				classId = moConfigNode.getAttribute("classid")

			if moConfigNode.hasAttribute("noun"):
				noun = moConfigNode.getAttribute("noun")

			if moConfigNode.hasAttribute("version"):
				version = moConfigNode.getAttribute("version")

			if moConfigNode.hasAttribute("actionVersion"):
				actionVersion = moConfigNode.getAttribute("actionVersion")

			if moConfigNode.hasAttribute("action"):
				action = moConfigNode.getAttribute("action")

			if moConfigNode.hasAttribute("ignoreReason"):
				ignoreReason = moConfigNode.getAttribute("ignoreReason")

			if moConfigNode.hasAttribute("status"):
				status = moConfigNode.getAttribute("status")

			if moConfigNode.hasAttribute("excludeList"):
				excludeList = moConfigNode.getAttribute("excludeList")

			# SyncMoConfig Object
			moConfig = None

			if classId:
				moConfig = SyncMoConfig(classId, noun, version, actionVersion, action, ignoreReason, status,
										excludeList)

			if moConfig:
				if classId in moConfigMap:
					moConfigMap[classId] = moConfig
				else:
					moConfigList = []
					moConfigList.append(moConfig)
					moConfigMap[classId] = moConfigList

		return moConfigMap

def Expandkey(key, clen):
		from time import time
		from array import array
		import hmac
		import sha
		import os
		import base64

		H = UcsUtils.GetShaHash

		uhash = H(','.join(str(x) for x in [`time()`, `os.getpid()`, `len(password)`, password, key]))[:16]

		k_enc, k_auth = H('enc' + key + uhash), H('auth' + key + uhash)
		n = len(password)
		passwordStream = array('L', password + '0000'[n & 3:])
		xkey = UcsUtils.Expandkey(k_enc, n + 4)

		for i in xrange(len(passwordStream)):
			passwordStream[i] = passwordStream[i] ^ xkey[i]

		ct = uhash + passwordStream.tostring()[:n]
		auth = hmac.new(ct, k_auth, sha).digest()

		encryptStr = ct + auth[:8]
		encodedStr = base64.encodestring(encryptStr)
		encryptedPassword = encodedStr.rstrip('\n')
		return encryptedPassword

def DecryptPassword(cipher, key):
		import os

		self.classId = node.localName
		metaClassId = UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId)

		if metaClassId:
			self.classId = metaClassId

		if node.hasAttribute(NamingPropertyId.DN):
			self.dn = node.getAttribute(NamingPropertyId.DN)

		if self.dn:
			self.rn = os.path.basename(self.dn)

		# Write the attribute and value to dictionary properties, as it is .
		self.WriteToAttributes(node)

		# Run the LoadFromXml for each childNode recursively and populate child list too.
		if (node.hasChildNodes()):
			# childList = node._get_childNodes()
			# childCount = childList._get_length()
			childList = node.childNodes
			childCount = len(childList)
			for i in range(childCount):
				childNode = childList.item(i)
				if (childNode.nodeType != Node.ELEMENT_NODE):
					continue
				c = _GenericMO()
				self.child.append(c)
				c.LoadFromXml(childNode)

def WriteXml(self, w, option, elementName=None):
		Method creates and returns an object of ManagedObject class using the classId and information from the
		Generic managed object.
		Method creates and returns an object of _GenericMO class using the classId and other information from the
		managed object.
		Method extracts and returns the child object list same as the given classId

        Results will be returned sorted by their score, the most relevant
        results will be returned first.

        For more programmatic querying use `lunr.Index.query`.

        Args:
            query_string (str): A string to parse into a Query.

        Returns:
            dict: Results of executing the query.

        Args:
            fields (iterable, optional): The fields to include in the Query,

                defaults to the Index's `all_fields`.

        Returns:
            Query: With the specified fields or all the fields in the Index.
        from lunr import __TARGET_JS_VERSION__

        if isinstance(serialized_index, basestring):
            serialized_index = json.loads(serialized_index)

        if serialized_index["version"] != __TARGET_JS_VERSION__:
            logger.warning(
                "Version mismatch when loading serialized index. "
                "Current version of lunr {} does not match that of serialized "
                "index {}".format(__TARGET_JS_VERSION__, serialized_index["version"])
            )

        field_vectors = {
            ref: Vector(elements) for ref, elements in serialized_index["fieldVectors"]
        }

        tokenset_builder = TokenSetBuilder()
        inverted_index = {}
        for term, posting in serialized_index["invertedIndex"]:
            tokenset_builder.insert(term)
            inverted_index[term] = posting

        tokenset_builder.finish()

        return Index(
            fields=serialized_index["fields"],
            field_vectors=field_vectors,
            inverted_index=inverted_index,
            token_set=tokenset_builder.root,
            pipeline=Pipeline.load(serialized_index["pipeline"]),
        )

def configure(logstash_host=None, logstash_port=None, logdir=None):
    '''Return new logger which will log both to logstash and to file in JSON
    format.

    Log files are stored in <logdir>/name.json

        pagination_keys = ['count', 'next', 'previous', 'results']
        for key in pagination_keys:
            if not (data and key in data):
                raise WrapperNotApplicable('Not paginated results')

        view = renderer_context.get("view", None)
        model = self.model_from_obj(view)
        resource_type = self.model_to_resource_type(model)

        try:
            from rest_framework.utils.serializer_helpers import ReturnList

            results = ReturnList(
                data["results"],
                serializer=data.serializer.fields["results"],
            )
        except ImportError:
            results = data["results"]


        # Use default wrapper for results

        wrapper = self.wrap_default(results, renderer_context)

        # Add pagination metadata
        pagination = self.dict_class()

        pagination['previous'] = data['previous']
        pagination['next'] = data['next']
        pagination['count'] = data['count']


        wrapper.setdefault('meta', self.dict_class())


        wrapper['meta'].setdefault('pagination', self.dict_class())

        wrapper['meta']['pagination'].setdefault(
            resource_type, self.dict_class()).update(pagination)

        return wrapper

def wrap_default(self, data, renderer_context):

        wrapper = self.dict_class()
        view = renderer_context.get("view", None)
        request = renderer_context.get("request", None)

        model = self.model_from_obj(view)
        resource_type = self.model_to_resource_type(model)

        if isinstance(data, list):
            many = True
            resources = data
        else:
            many = False
            resources = [data]

        items = []
        links = self.dict_class()
        linked = self.dict_class()
        meta = self.dict_class()

        for resource in resources:
            converted = self.convert_resource(resource, data, request)
            item = converted.get('data', {})
            linked_ids = converted.get('linked_ids', {})
            if linked_ids:
                item["links"] = linked_ids
            items.append(item)

            links.update(converted.get('links', {}))
            linked = self.update_nested(linked,
                                        converted.get('linked', {}))
            meta.update(converted.get('meta', {}))

        if many:
            wrapper[resource_type] = items
        else:
            wrapper[resource_type] = items[0]

        if links:
            links = self.prepend_links_with_name(links, resource_type)
            wrapper["links"] = links

        if linked:
            wrapper["linked"] = linked

        if meta:
            wrapper["meta"] = meta

        return wrapper

def acquire_lock(self):
		# first ensure that a record exists for this session id
		try:
			self.collection.insert_one(dict(_id=self.id))
		except pymongo.errors.DuplicateKeyError:
			pass
		unlocked_spec = dict(_id=self.id, locked=None)
		lock_timer = (
			timers.Timer.after(self.lock_timeout)
			if self.lock_timeout
			else timers.NeverExpires()
		)
		while not lock_timer.expired():
			locked_spec = {'$set': dict(locked=datetime.datetime.utcnow())}
			res = self.collection.update_one(unlocked_spec, locked_spec)
			if res.raw_result['updatedExisting']:
				# we have the lock
				break
			time.sleep(0.1)
		else:
			raise LockTimeout(f"Timeout acquiring lock for {self.id}")
		self.locked = True

def set_boot_device(self, device, persistent=False):

        operation = "set_boot_device"
        try:
            self.sp_manager.create_boot_policy()
            self.sp_manager.set_boot_device(device)

        except UcsException as ex:
            raise exception.UcsOperationError(operation=operation, error=ex)

def get_boot_device(self):
        operation = 'get_boot_device'
        try:
            boot_device = self.sp_manager.get_boot_device()
            return boot_device
        except UcsException as ex:
            print(_("Cisco client exception: %(msg)s."), {'msg': ex})
            raise exception.UcsOperationError(operation=operation, error=ex)

def lunr(ref, fields, documents, languages=None):
    if languages is not None and lang.LANGUAGE_SUPPORT:
        if isinstance(languages, basestring):
            languages = [languages]

        unsupported_languages = set(languages) - set(lang.SUPPORTED_LANGUAGES)
        if unsupported_languages:
            raise RuntimeError(
                "The specified languages {} are not supported, "
                "please choose one of {}".format(
                    ", ".join(unsupported_languages),
                    ", ".join(lang.SUPPORTED_LANGUAGES.keys()),
                )
            )
        builder = lang.get_nltk_builder(languages)
    else:
        builder = Builder()
        builder.pipeline.add(trimmer, stop_word_filter, stemmer)
        builder.search_pipeline.add(stemmer)

    builder.ref(ref)
    for field in fields:
        if isinstance(field, dict):
            builder.field(**field)
        else:
            builder.field(field)

    for document in documents:
        if isinstance(document, (tuple, list)):
            builder.add(document[0], attributes=document[1])
        else:
            builder.add(document)

    return builder.build()

def from_config(_config, **options):
        expected_args = ('path',)
        rconfig.check_config_options("SQLiteEventStore", expected_args,
                                     tuple(), options)
        return SQLiteEventStore(options['path'])

def key_exists(self, key):
        assert isinstance(key, str)
        cursor = self.conn.cursor()
        with contextlib.closing(cursor):
            cursor.execute('SELECT COUNT(*) FROM events WHERE uuid=?', (key,))
            res = cursor.fetchone()
            count = res[0]
        if count == 0:
            return False
        else:
            assert count in (0, 1), \
                "Duplicate event ids detected: {0}".format(count)
            return True

def count(self):

        Important to close to not have any file descriptor leakages.


        Parameters:
        _config    -- the configuration file options read from file(s).
        **options -- various options given to the specific event store. Shall
                     not be used with this event store. Warning will be logged
                     for every extra non-recognized option. The only required
                     key to this function is 'path'.

        returns   -- a newly instantiated `LogEventStore`.


        This function makes a linear search through the log file and is very
        slow.

        Returns True if the event has previously been added, False otherwise.

        fname = os.path.basename(self._path)
        checksum_persister = _get_checksum_persister(self._path)
        with contextlib.closing(checksum_persister):
            checksum_persister[fname] = self._hasher.hexdigest()

        self._close()

def from_config(config, **options):
        expected_args = ('prefix', 'realclass')
        for arg in expected_args:
            if arg not in options:
                msg = "Required option missing: {0}"
                raise rconfig.ConfigurationError(msg.format(arg))
        # Not logging unrecognized options here, because they might be used
        # by the real event store instantiated below.

        classpath = options['realclass']
        classpath_pieces = classpath.split('.')
        classname = classpath_pieces[-1]
        modulepath = '.'.join(classpath_pieces[0:-1])
        module = importlib.import_module(modulepath)
        estore_class = getattr(module, classname)

        return RotatedEventStore(lambda fname: estore_class(fname),
                                 options['path'], options['prefix'])

def _construct_filename(self, batchno):
        return os.path.join(self.dirpath,
                            "{0}.{1}".format(self.prefix, batchno))

def rotate(self):
        self._logger.info('Rotating data files. New batch number will be: %s',
                          self.batchno + 1)
        self.estore.close()
        self.estore = None
        self.batchno += 1
        self.estore = self._open_event_store()

def _find_batch_containing_event(self, uuid):
        if self.estore.key_exists(uuid):
            # Reusing already opened DB if possible
            return self.batchno
        else:
            for batchno in range(self.batchno - 1, -1, -1):
                # Iterating backwards here because we are more likely to find
                # the event in an later archive, than earlier.
                db = self._open_event_store(batchno)
                with contextlib.closing(db):
                    if db.key_exists(uuid):
                        return batchno
        return None

def from_config(config, **options):
        required_args = ('storage-backends',)
        optional_args = {'events_per_batch': 25000}
        rconfig.check_config_options("SyncedRotationEventStores",
                                     required_args,
                                     tuple(optional_args.keys()), options)

        if "events_per_batch" in options:
            events_per_batch = int(options["events_per_batch"])
        else:
            events_per_batch = optional_args["events_per_batch"]

        estore = SyncedRotationEventStores(events_per_batch)

        for section in options['storage-backends'].split(' '):
            try:
                substore = rconfig.construct_eventstore(config, section)
                estore.add_rotated_store(substore)
            except Exception as e:
                _logger.exception('Could not instantiate substore from'
                                  ' section %s', section)
                estore.close()
                raise

        return estore

def hexdump(stream):
    '''

    if isinstance(stream, six.string_types):
        stream = BytesIO(stream)

    row = 0
    while True:
        data = stream.read(16)
        if not data:
            break

        hextets = data.encode('hex').ljust(32)
        canonical = printable(data)

        print('%08x %s  %s  |%s|' % (
            row * 16,
            ' '.join(hextets[x:x + 2] for x in range(0x00, 0x10, 2)),
            ' '.join(hextets[x:x + 2] for x in range(0x10, 0x20, 2)),
            canonical,
        ))
        row += 1

def printable(sequence):
    '''
    return ''.join(list(
        map(lambda c: c if c in PRINTABLE else '.', sequence)
    ))

def sparkline(data):
    '''

    min_value = float(min(data))
    max_value = float(max(data))
    steps = (max_value - min_value) / float(len(SPARKCHAR) - 1)
    return ''.join([
        SPARKCHAR[int((float(value) - min_value) / steps)]
        for value in data
    ])

def get_language_stemmer(language):
    from lunr.languages import SUPPORTED_LANGUAGES
    from nltk.stem.snowball import SnowballStemmer

    return SnowballStemmer(SUPPORTED_LANGUAGES[language])

def nltk_stemmer(stemmer, token, i=None, tokens=None):


    def wrapped_stem(token, metadata=None):
        return stemmer.stem(token)

    return token.update(wrapped_stem)

def is_seq(obj):
		Decorate a migration function with this method
		to make it available for migrating cases.
		Add .source and .target attributes to the registered function.
		Migrate the doc from its current version to the target version
		and return it.
		Return exactly one function to convert from source to target

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: Plone UID
    :rtype: string

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :param html_tag: A value of 'True' returns the HTML tag, else the image url
    :type html_tag: bool
    :returns: HTML '<img>' tag if 'html_tag' is True else the image url
    :rtype: string

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: Workflow history
    :rtype: [{}, ...]

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: Value of the review_status variable
    :rtype: String

    :param brain_or_object: A single catalog brain or content object
    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain
    :returns: Value of the review_status variable
    :rtype: String
        if label in cls.registered_functions:
            log.warning("Overwriting existing registered function %s", label)

        fn.label = label
        cls.registered_functions[fn.label] = fn

def load(cls, serialised):

        Functions must accept three arguments:
        - Token: A lunr.Token object which will be updated
        - i: The index of the token in the set
        - tokens: A list of tokens representing the set
        getting strings out. This method takes care of wrapping the passed
	Use the same MongoDB client as pmxbot if available.
    In a sharded cluster, create a database in a particular shard.
    Returns a method from a given class or instance. When the method doest not exist, it returns `None`. Also works with
    properties and cached properties.

    import astropy.io.fits as fits
    import numina.types.dataframe as df

    refs = []
    for frame in list_of_frame:
        if isinstance(frame, str):
            ref = fits.open(frame)
            refs.append(ref)
        elif isinstance(frame, fits.HDUList):
            refs.append(frame)
        elif isinstance(frame, df.DataFrame):
            ref = frame.open()
            refs.append(ref)
        else:
            refs.append(frame)
    try:
        yield refs
    finally:
        # release
        for obj in refs:
            obj.close()

def logging_from_debugplot(debugplot):

    if isinstance(debugplot, int):
        if abs(debugplot) >= 10:
            logging.basicConfig(level=logging.DEBUG)
        else:
            logging.basicConfig(level=logging.INFO)
    else:
        raise ValueError("Unexpected debugplot=" + str(debugplot))

def ximplot(ycut, title=None, show=True, plot_bbox=(0, 0),
            geometry=(0, 0, 640, 480), tight_layout=True,
            debugplot=None):

    # protections
    if type(ycut) is not np.ndarray:
        raise ValueError("ycut=" + str(ycut) +
                         " must be a numpy.ndarray")
    elif ycut.ndim is not 1:
        raise ValueError("ycut.ndim=" + str(ycut.dim) +
                         " must be 1")

    # read bounding box limits
    nc1, nc2 = plot_bbox
    plot_coord = (nc1 == 0 and nc2 == 0)

    naxis1_ = ycut.size
    if not plot_coord:
        # check that ycut size corresponds to bounding box size
        if naxis1_ != nc2 - nc1 + 1:
            raise ValueError("ycut.size=" + str(ycut.size) +
                             " does not correspond to bounding box size")

    # display image
    from numina.array.display.matplotlib_qt import plt
    if not show:
        plt.ioff()

    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.autoscale(False)
    ymin = ycut.min()
    ymax = ycut.max()
    if plot_coord:
        xmin = -0.5
        xmax = (naxis1_ - 1) + 0.5
        xcut = np.arange(naxis1_, dtype=np.float)
        ax.set_xlabel('image array index in the X direction')
        ax.set_ylabel('pixel value')
    else:
        xmin = float(nc1) - 0.5
        xmax = float(nc2) + 0.5
        xcut = np.linspace(start=nc1, stop=nc2, num=nc2 - nc1 + 1)
        ax.set_xlabel('image pixel in the X direction')
        ax.set_ylabel('pixel value')
    ax.set_xlim(xmin, xmax)
    ax.set_ylim(ymin, ymax)
    ax.plot(xcut, ycut, '-')
    if title is not None:
        ax.set_title(title)

    # set the geometry
    set_window_geometry(geometry)

    if show:
        pause_debugplot(debugplot, pltshow=show, tight_layout=tight_layout)
    else:
        if tight_layout:
            plt.tight_layout()
        # return axes
        return ax

def oversample1d(sp, crval1, cdelt1, oversampling=1, debugplot=0):

    if sp.ndim != 1:
        raise ValueError('Unexpected array dimensions')

    naxis1 = sp.size
    naxis1_over = naxis1 * oversampling
    cdelt1_over = cdelt1 / oversampling
    xmin = crval1 - cdelt1/2   # left border of first pixel
    crval1_over = xmin + cdelt1_over / 2

    sp_over = np.zeros(naxis1_over)
    for i in range(naxis1):
        i1 = i * oversampling
        i2 = i1 + oversampling
        sp_over[i1:i2] = sp[i]

    if abs(debugplot) in (21, 22):
        crvaln = crval1 + (naxis1 - 1) * cdelt1
        crvaln_over = crval1_over + (naxis1_over - 1) * cdelt1_over
        xover = np.linspace(crval1_over, crvaln_over, naxis1_over)
        ax = ximplotxy(np.linspace(crval1, crvaln, naxis1), sp, 'bo',
                       label='original', show=False)
        ax.plot(xover, sp_over, 'r+', label='resampled')
        pause_debugplot(debugplot, pltshow=True)

    return sp_over, crval1_over, cdelt1_over

def map_borders(wls):
    midpt_wl = 0.5 * (wls[1:] + wls[:-1])
    all_borders = np.zeros((wls.shape[0] + 1,))
    all_borders[1:-1] = midpt_wl
    all_borders[0] = 2 * wls[0] - midpt_wl[0]
    all_borders[-1] = 2 * wls[-1] - midpt_wl[-1]
    return all_borders

def import_object(path):

    Other projects can call this to get an ArgumentParser with losser's
    command line interface to use as a parent parser for their own parser.
    For example::

        parent_parser = losser.cli.make_parser(
            add_help=False, exclude_args=["-i"])
        parser = argparse.ArgumentParser(
            description="Export datasets from a CKAN site to JSON or CSV.",
            parents=[parent_parser])
        parser.add_argument(...

    :param add_help: Whether or not to add losser's help text to the parser.
        Pass add_help=False if you want to use your own help text in a child
        parser.
    :type add_help: bool

    :param exclude_args: List of losser command-line arguments to exclude, use

        this to exclude any default losser arguments that you don't want in
        your own command. For example: exclude_args=["-i", "--max-length"].
    :type exclude_args: list of strings


    Other projects can call this function and pass in their own ArgumentParser
    object (which should have a losser ArgumentParser from make_parser() above
    as parent) to do the argument parsing and get the result (this does some
    custom post-processing, beyond what argparse's parse_args() does). For
    example::

        parent_parser = losser.cli.make_parser(...)
        parser = argparse.ArgumentParser(parents=[parent_parser])
        parser.add_argument(...)
        try:
            parsed_args = losser.cli.parse(parser=parser)
        except losser.cli.CommandLineError as err:
            ...

    :raises CommandLineError: If something went wrong during command-line
        parsing. If the exception has a non-empty .message attribute it
        contains an error message that hasn't been printed to stdout yet,
        otherwise any error message has already been printed.

    :raises CommandLineExit: If the result of command-line parsing means that
        the command should exit without continuing, but this is not because of
        an error (for example if the user passed --help). Any help text will
        already have been written to stdout, the exit code that the process
        should exit with is in the exception's .code attribute.
        CommandLineExit is a subclass of CommandLineError above.


    Read the command line arguments and the input data from stdin, pass them to
    the table() function to do the filter and transform, and return the string
    of CSV- or JSON-formatted text that should be written to stdout.

    Note that although the output data is returned rather than written to
    stdout, this function may write error messages or help text to stdout
    (for example if there's an error with the command-line parsing).

    :raises CommandLineError: see parse() above for details

    FWHM_G = 2 * math.sqrt(2 * math.log(2))
    sigma = seeing_fwhm / FWHM_G
    amplitude = 1.0 / (2 * math.pi * sigma * sigma)
    seeing_model = Gaussian2D(amplitude=amplitude,
                              x_mean=0.0,
                              y_mean=0.0,
                              x_stddev=sigma,
                              y_stddev=sigma)
    return seeing_model

def generate_moffat_profile(seeing_fwhm, alpha):
    Converts a model field to a dictionary
    The same implementation as django model_to_dict but editable fields are allowed
        Changes a given `changed_fields` on each object in the queryset, saves objects
        and returns the changed objects in the queryset.
        return (
            self.intervals[1].pix1 - 0.5,
            self.intervals[1].pix2 - 0.5,
            self.intervals[0].pix1 - 0.5,
            self.intervals[0].pix2 - 0.5,
        )

def readout(self):

    in the form a=1, b='c'
    as a dictionary
    Return a given `number` formatter a price for humans.

        objl = self.convert(obj)

        result = super(DataFrameType, self).extract_db_info(objl, keys)
        ext = self.datamodel.extractor_map['fits']
        if objl:
            with objl.open() as hdulist:
                for field in keys:
                    result[field] = ext.extract(field, hdulist)

                tags = result['tags']
                for field in self.tags_keys:
                    tags[field] = ext.extract(field, hdulist)

                return result
        else:
            return result

def readc(prompt, default=None, valid=None, question_mark=True):

    cresult = None  # Avoid PyCharm warning

    # question mark
    if question_mark:
        cquestion_mark = ' ? '
    else:
        cquestion_mark = ''

    # main loop
    loop = True
    while loop:

        # display prompt

        if default is None:
            print(prompt + cquestion_mark, end='')
            sys.stdout.flush()
        else:

            print(prompt + ' [' + str(default) + ']' + cquestion_mark, end='')
            sys.stdout.flush()

        # read user's input
        cresult = sys.stdin.readline().strip()

        if cresult == '' and default is not None:

            cresult = str(default)

        if len(cresult) == 1:
            # check that all the characters are valid
            loop = False
            if valid is not None:
                for c in cresult:
                    if c not in str(valid):
                        print('*** Error: invalid characters found.')
                        print('*** Valid characters are:', valid)
                        print('*** Try again!')
                        loop = True
        else:
            print('*** Error: invalid string length. Try again!')

    return cresult

def read_value(ftype, prompt, default=None, minval=None, maxval=None,
               allowed_single_chars=None, question_mark=True):

    # avoid PyCharm warning 'might be referenced before assignment'
    result = None

    # question mark
    if question_mark:
        cquestion_mark = ' ? '
    else:
        cquestion_mark = ''

    # check minimum value
    if minval is not None:
        try:
            iminval = ftype(minval)
        except ValueError:
            raise ValueError("'" + str(minval) + "' cannot " +
                             "be used as an minval in readi()")
    else:
        iminval = None

    # check maximum value
    if maxval is not None:
        try:
            imaxval = ftype(maxval)
        except ValueError:
            raise ValueError("'" + str(maxval) + "' cannot " +
                             "be used as an maxval in readi()")
    else:
        imaxval = None

    # minimum and maximum values
    if minval is None and maxval is None:
        cminmax = ''
    elif minval is None:
        cminmax = ' (number <= ' + str(imaxval) + ')'
    elif maxval is None:
        cminmax = ' (number >= ' + str(iminval) + ')'
    else:
        cminmax = ' (' + str(minval) + ' <= number <= ' + str(maxval) + ')'

    # main loop
    loop = True
    while loop:

        # display prompt

        if default is None:
            print(prompt + cminmax + cquestion_mark, end='')
            sys.stdout.flush()
        else:

            print(prompt + cminmax + ' [' + str(default) + ']' +
                  cquestion_mark, end='')
            sys.stdout.flush()

        # read user's input
        cresult = sys.stdin.readline().strip()

        if cresult == '' and default is not None:

            cresult = str(default)

        # if valid allowed single character, return character
        if len(cresult) == 1:
            if allowed_single_chars is not None:
                if cresult in allowed_single_chars:
                    return cresult

        # convert to ftype value
        try:
            result = ftype(cresult)
        except ValueError:
            print("*** Error: invalid " + str(ftype) + " value. Try again!")
        else:
            # check number is within expected range
            if minval is None and maxval is None:
                loop = False
            elif minval is None:
                if result <= imaxval:
                    loop = False
                else:
                    print("*** Error: number out of range. Try again!")
            elif maxval is None:
                if result >= iminval:
                    loop = False
                else:
                    print("*** Error: number out of range. Try again!")
            else:
                if iminval <= result <= imaxval:
                    loop = False
                else:
                    print("*** Error: number out of range. Try again!")

    return result

def load_product_object(self, name):
        # load everything
        requires = {}
        provides = {}
        for mode, r in self.recipes.items():
            l = self.load_recipe_object(mode)

            for field, vv in l.requirements().items():
                if vv.type.isproduct():
                    name = vv.type.name()
                    pe = ProductEntry(name, mode, field)
                    requires[name] = pe

            for field, vv in l.products().items():
                if vv.type.isproduct():
                    name = vv.type.name()
                    pe = ProductEntry(name, mode, field)
                    provides[name] = pe

        return requires, provides

def search_mode_provides(self, product, pipeline='default'):

        logger = logging.getLogger(__name__)

        logger.debug('calling default configuration selector')

        # get first possible image
        ref = obresult.get_sample_frame()
        extr = self.datamodel.extractor_map['fits']
        if ref:
            # get INSCONF configuration
            result = extr.extract('insconf', ref)
            if result:
                # found the keyword, try to match
                logger.debug('found insconf config uuid=%s', result)
                # Use insconf as uuid key
                if result in self.configurations:
                    return self.configurations[result]
                else:
                    # Additional check for conf.name
                    for conf in self.configurations.values():
                        if conf.name == result:
                            return conf
                    else:
                        raise KeyError('insconf {} does not match any config'.format(result))

            # If not, try to match by DATE
            date_obs = extr.extract('observation_date', ref)
            for key, conf in self.configurations.items():

                if key == 'default':

                    # skip default
                    continue
                if conf.date_end is not None:
                    upper_t = date_obs < conf.date_end
                else:
                    upper_t = True
                if upper_t and (date_obs >= conf.date_start):
                    logger.debug('found date match, config uuid=%s', key)
                    return conf
        else:

            logger.debug('no match, using default configuration')

            return self.configurations['default']

def select_profile(self, obresult):
        active_mode = self.modes[mode_name]
        active_pipeline = self.pipelines[pipeline_name]
        recipe = active_pipeline.get_recipe_object(active_mode)
        return recipe

def pause_debugplot(debugplot, optional_prompt=None, pltshow=False,
                    tight_layout=True):

    if debugplot not in DEBUGPLOT_CODES:
        raise ValueError('Invalid debugplot value:', debugplot)

    if debugplot < 0:
        debugplot_ = -debugplot
        pltclose = True
    else:
        debugplot_ = debugplot
        pltclose = False

    if pltshow:
        if debugplot_ in [1, 2, 11, 12, 21, 22]:
            if tight_layout:
                plt.tight_layout()
            if debugplot_ in [1, 11, 21]:
                plt.show(block=False)
                plt.pause(0.2)
            elif debugplot_ in [2, 12, 22]:
                print('Press "q" to continue...', end='')
                sys.stdout.flush()
                plt.show()
                print('')
    else:
        if debugplot_ in [2, 12, 22]:
            if optional_prompt is None:
                print('Press <RETURN> to continue...', end='')
            else:
                print(optional_prompt, end='')
            sys.stdout.flush()
            cdummy = sys.stdin.readline().strip()

    if debugplot_ in [1, 2, 11, 12, 21, 22] and pltclose:
        plt.close()

def mode_half_sample(a, is_sorted=False):
    '''

    a = np.asanyarray(a)

    if not is_sorted:
        sdata = np.sort(a)
    else:
        sdata = a

    n = len(sdata)
    if n == 1:
        return sdata[0]
    elif n == 2:
        return 0.5 * (sdata[0] + sdata[1])
    elif n == 3:
        ind = -sdata[0] + 2 * sdata[1] - sdata[2]
        if ind < 0:
            return 0.5 * (sdata[0] + sdata[1])
        elif ind > 0:
            return 0.5 * (sdata[1] + sdata[2])
        else:
            return sdata[1]
    else:
        N = int(math.ceil(n / 2.0))
        w = sdata[(N-1):] - sdata[:(n-N+1)]
        ar = w.argmin()
        return mode_half_sample(sdata[ar:ar+N], is_sorted=True)

def overplot_ds9reg(filename, ax):

    # read ds9 region file
    with open(filename) as f:
        file_content = f.read().splitlines()

    # check first line
    first_line = file_content[0]
    if "# Region file format: DS9" not in first_line:
        raise ValueError("Unrecognized ds9 region file format")

    for line in file_content:
        if line[0:4] == "line":
            line_fields = line.split()
            x1 = float(line_fields[1])
            y1 = float(line_fields[2])
            x2 = float(line_fields[3])
            y2 = float(line_fields[4])
            if "color" in line:
                i = line.find("color=")
                color = line[i+6:i+13]
            else:
                color = "green"
            ax.plot([x1,x2], [y1,y2], '-', color=color)
        elif line[0:4] == "text":
            line_fields = line.split()
            x0 = float(line_fields[1])
            y0 = float(line_fields[2])
            text=line_fields[3][1:-1]
            if "color" in line:
                i = line.find("color=")
                color = line[i+6:i+13]
            else:
                color = "green"
            ax.text(x0, y0, text, fontsize=8,
                    bbox=dict(boxstyle="round,pad=0.1", fc="white", ec="grey", ),
                    color=color, fontweight='bold', backgroundcolor='white',
                    ha='center')
        else:
            # ignore
            pass

def find_peaks_indexes(arr, window_width=5, threshold=0.0, fpeak=0):

    _check_window_width(window_width)

    if (fpeak<0 or fpeak + 1 >= window_width):
        raise ValueError('fpeak must be in the range 0- window_width - 2')

    kernel_peak = kernel_peak_function(threshold, fpeak)
    out = generic_filter(arr, kernel_peak, window_width, mode="reflect")
    result, =  numpy.nonzero(out)

    return filter_array_margins(arr, result, window_width)

def refine_peaks(arr, ipeaks, window_width):
    _check_window_width(window_width)

    step = window_width // 2

    ipeaks = filter_array_margins(arr, ipeaks, window_width)

    winoff = numpy.arange(-step, step+1, dtype='int')
    peakwin = ipeaks[:, numpy.newaxis] + winoff
    ycols = arr[peakwin]

    ww = return_weights(window_width)

    coff2 = numpy.dot(ww, ycols.T)

    uc = -0.5 * coff2[1] / coff2[2]
    yc = coff2[0] + uc * (coff2[1] + coff2[2] * uc)
    xc = ipeaks + 0.5 * (window_width-1) * uc

    return xc, yc

def complete_config(config):
    Set Cache-Control headers and Expires-header.

    Expects a timedelta instance.
    Set Cache-Control headers.

    Expects keyword arguments and/or an item list.

    Each pair is used to set Flask Response.cache_control attributes,
    where the key is the attribute name and the value is its value.

    Use True as value for attributes without values.

    In case of an invalid attribute, CacheControlAttributeInvalidError
    will be thrown.
    Set Cache-Control headers for no caching

    Will generate proxy-revalidate, no-cache, no-store, must-revalidate,
    max-age=0.

    :param func: function that you want wrapping
    :type func: function

    Args:
        uid (string): a UID string
        schema (string): try to resolve to schema

    Returns:
        Tuple[string, string]: a tuple (uid, schema) where:
        - uid: the UID normalized to comply with the id.json schema
        - schema: a schema of the UID or *None* if not recognised

    Raise:

        UnknownUIDSchema: if UID is too little to definitively guess the schema
        SchemaUIDConflict: if specified schema is not matching the given UID

    This properly capitalizes the category and replaces the dash by a dot if
    needed. If the category is obsolete, it also gets converted it to its
    current equivalent.

    Example:
        >>> from inspire_schemas.utils import normalize_arxiv_category
        >>> normalize_arxiv_category('funct-an')  # doctest: +SKIP
        u'math.FA'


    Example:
        >>> from inspire_schemas.utils import valid_arxiv_categories
        >>> 'funct-an' in valid_arxiv_categories()
        True


    Args:
        value(str): an Inspire category to properly case, or an arXiv category
            to translate to the corresponding Inspire category.

    Returns:
        str: ``None`` if ``value`` is not a non-empty string,
            otherwise the corresponding Inspire category.

    pubnote = {}
    parts = pubnote_str.split(',')

    if len(parts) > 2:
        pubnote['journal_title'] = parts[0]
        pubnote['journal_volume'] = parts[1]
        pubnote['page_start'], pubnote['page_end'], pubnote['artid'] = split_page_artid(parts[2])

    return {key: val for (key, val) in six.iteritems(pubnote) if val is not None}

def get_schema_path(schema, resolved=False):

    def _strip_first_path_elem(path):
        stripped_path = path.split(os.path.sep, 1)[1:]
        return ''.join(stripped_path)


    def _schema_to_normalized_path(schema):
        path = os.path.normpath(os.path.sep + urlsplit(schema).path)
        if path.startswith(os.path.sep):
            path = path[1:]

        if not path.endswith('.json'):
            path += '.json'

        return path

    path = _schema_to_normalized_path(schema)
    while path:
        if resolved:
            schema_path = os.path.abspath(os.path.join(_resolved_schema_root_path, path))
        else:
            schema_path = os.path.abspath(os.path.join(_schema_root_path, path))
        if os.path.exists(schema_path):
            return os.path.abspath(schema_path)

        path = _strip_first_path_elem(path)

    raise SchemaNotFound(schema=schema)

def load_schema(schema_name, resolved=False):
    schema_data = ''
    with open(get_schema_path(schema_name, resolved)) as schema_fd:
        schema_data = json.loads(schema_fd.read())

    return schema_data

def _load_schema_for_record(data, schema=None):
    if schema is None:
        if '$schema' not in data:
            raise SchemaKeyNotFound(data=data)
        schema = data['$schema']

    if isinstance(schema, six.string_types):
        schema = load_schema(schema_name=schema)
    return schema

def validate(data, schema=None):
    schema = _load_schema_for_record(data, schema)

    return jsonschema_validate(
        instance=data,
        schema=schema,
        resolver=LocalRefResolver.from_schema(schema),
        format_checker=inspire_format_checker,
    )

def get_validation_errors(data, schema=None):
    schema = _load_schema_for_record(data, schema)

    errors = Draft4Validator(
        schema,
        resolver=LocalRefResolver.from_schema(schema),
        format_checker=inspire_format_checker
    )
    return errors.iter_errors(data)

def normalize_collaboration(collaboration):
    if not collaboration:
        return []

    collaboration = collaboration.strip()
    if collaboration.startswith('(') and collaboration.endswith(')'):
        collaboration = collaboration[1:-1]

    collaborations = _RE_AND.split(collaboration)
    collaborations = (_RE_COLLABORATION_LEADING.sub('', collab)
                      for collab in collaborations)
    collaborations = (_RE_COLLABORATION_TRAILING.sub('', collab)
                      for collab in collaborations)

    return [collab.strip() for collab in collaborations]

def get_license_from_url(url):
    if not url:
        return

    split_url = urlsplit(url, scheme='http')

    if split_url.netloc.lower() == 'creativecommons.org':
        if 'publicdomain' in split_url.path:
            match = _RE_PUBLIC_DOMAIN_URL.match(split_url.path)
            if match is None:
                license = ['public domain']
            else:
                license = ['CC0']
                license.extend(part for part in match.groups() if part)
        else:
            license = ['CC']
            match = _RE_LICENSE_URL.match(split_url.path)
            license.extend(part.upper() for part in match.groups() if part)
    elif split_url.netloc == 'arxiv.org':
        license = ['arXiv']
        match = _RE_LICENSE_URL.match(split_url.path)
        license.extend(part for part in match.groups() if part)
    else:
        raise ValueError('Unknown license URL')

    return u' '.join(license)

def convert_old_publication_info_to_new(publication_infos):
    result = []
    hidden_publication_infos = []

    for publication_info in publication_infos:
        _publication_info = copy.deepcopy(publication_info)
        journal_title = _publication_info.get('journal_title')

        try:
            journal_title = _JOURNALS_RENAMED_OLD_TO_NEW[journal_title]
            _publication_info['journal_title'] = journal_title
            result.append(_publication_info)
            continue
        except KeyError:
            pass

        journal_volume = _publication_info.get('journal_volume')

        if journal_title in _JOURNALS_WITH_YEAR_ADDED_TO_VOLUME and journal_volume and len(journal_volume) == 4:
            try:
                was_last_century = int(journal_volume[:2]) > 50
            except ValueError:
                pass
            else:
                _publication_info['year'] = int('19' + journal_volume[:2] if was_last_century else '20' + journal_volume[:2])
                _publication_info['journal_volume'] = journal_volume[2:]
            result.append(_publication_info)
            continue

        if journal_title and journal_volume and journal_title.lower() not in JOURNALS_IGNORED_IN_OLD_TO_NEW:
            volume_starts_with_a_letter = _RE_VOLUME_STARTS_WITH_A_LETTER.match(journal_volume)
            volume_ends_with_a_letter = _RE_VOLUME_ENDS_WITH_A_LETTER.match(journal_volume)
            match = volume_starts_with_a_letter or volume_ends_with_a_letter
            if match:
                _publication_info.pop('journal_record', None)
                if journal_title in _JOURNALS_RENAMED_OLD_TO_NEW.values():
                    _publication_info['journal_title'] = journal_title
                else:
                    _publication_info['journal_title'] = ''.join([
                        journal_title,
                        '' if journal_title.endswith('.') else ' ',
                        match.group('letter'),
                    ])
                _publication_info['journal_volume'] = match.group('volume')

        hidden = _publication_info.pop('hidden', None)
        if hidden:
            hidden_publication_infos.append(_publication_info)
        else:
            result.append(_publication_info)

    for publication_info in hidden_publication_infos:
        if publication_info not in result:
            publication_info['hidden'] = True
            result.append(publication_info)

    return result

def convert_new_publication_info_to_old(publication_infos):

    def _needs_a_hidden_pubnote(journal_title, journal_volume):
        return (
            journal_title in _JOURNALS_THAT_NEED_A_HIDDEN_PUBNOTE and
            journal_volume in _JOURNALS_THAT_NEED_A_HIDDEN_PUBNOTE[journal_title]
        )

    result = []

    for publication_info in publication_infos:
        _publication_info = copy.deepcopy(publication_info)
        journal_title = _publication_info.get('journal_title')

        try:
            journal_title = _JOURNALS_RENAMED_NEW_TO_OLD[journal_title]
            _publication_info['journal_title'] = journal_title
            result.append(_publication_info)
            continue
        except KeyError:
            pass

        journal_volume = _publication_info.get('journal_volume')
        year = _publication_info.get('year')

        if (journal_title in _JOURNALS_WITH_YEAR_ADDED_TO_VOLUME and year and
                journal_volume and len(journal_volume) == 2):
            two_digit_year = str(year)[2:]
            _publication_info['journal_volume'] = ''.join([two_digit_year, journal_volume])
            result.append(_publication_info)
            continue

        if journal_title and journal_volume:
            match = _RE_TITLE_ENDS_WITH_A_LETTER.match(journal_title)
            if match and _needs_a_hidden_pubnote(journal_title, journal_volume):
                _publication_info['journal_title'] = match.group('title')
                _publication_info['journal_volume'] = journal_volume + match.group('letter')
                result.append(_publication_info)
                _publication_info = copy.deepcopy(publication_info)
                _publication_info['hidden'] = True
                _publication_info['journal_title'] = match.group('title')
                _publication_info['journal_volume'] = match.group('letter') + journal_volume
            elif match and journal_title not in _JOURNALS_ALREADY_ENDING_WITH_A_LETTER:
                _publication_info['journal_title'] = match.group('title')
                _publication_info['journal_volume'] = match.group('letter') + journal_volume

        result.append(_publication_info)

    return result

def fix_reference_url(url):
    new_url = url

    new_url = fix_url_bars_instead_of_slashes(new_url)

    new_url = fix_url_add_http_if_missing(new_url)

    new_url = fix_url_replace_tilde(new_url)

    try:
        rfc3987.parse(new_url, rule="URI")
        return new_url
    except ValueError:
        return url

def is_arxiv(obj):
    arxiv_test = obj.split()
    if not arxiv_test:
        return False

    matched_arxiv = (RE_ARXIV_PRE_2007_CLASS.match(arxiv_test[0]) or
                     RE_ARXIV_POST_2007_CLASS.match(arxiv_test[0]))

    if not matched_arxiv:
        return False

    if not matched_arxiv.group('category'):
        return True

    valid_arxiv_categories_lower = [category.lower() for category in valid_arxiv_categories()]
    category = matched_arxiv.group('category').lower()
    return (category in valid_arxiv_categories_lower or
            category.replace('-', '.') in valid_arxiv_categories_lower)

def normalize_arxiv(obj):
        try:
            return super(LocalRefResolver, self).resolve_remote(uri)
        except ValueError:
            return super(LocalRefResolver, self).resolve_remote(
                'file://' + get_schema_path(uri.rsplit('.json', 1)[0])
            )

def set_path(self, path):
		file = File(path if path else self.path, cwd=options.get("cwd", self.cwd))
		file.base = options.get("base", self.base)

		if with_contents:
			file.contents = options.get("contents", self.contents)

		return file

def launch_cli():

        Args:
            value (string): affiliation value
            curated_relation (bool): is relation curated
            record (dict): affiliation JSON reference

        If a UID of a given schema already exists in a record it will
        be overwritten, otherwise it will be appended to the record.

        Args:
            uid (string): unique identifier.
            schema (Optional[string]): schema of the unique identifier. If
                ``None``, the schema will be guessed based on the shape of
                ``uid``.

        Raises:
            SchemaUIDConflict: it UID and schema are not matching
    Create singleton from class
    Activate language only for one method or function
        return await self.request(
            'get', 'protection', params={
                'from': str(low),
                'to': str(high)
            })

def runstring(self):
        Override instrument configuration if configuration is not None
    newn = [map_tree(visitor, node) for node in tree.nodes]
    return visitor(tree, newn)

def filter_tree(condition, tree):

        def change_p_node_tags(node, children):
            if isinstance(node, Placeholder):
                value = ConstExpr(tags[node.name])
                return value
            else:
                return node.clone(children)

        return map_tree(change_p_node_tags, self)

def molecules2symbols(molecules, add_hydrogen=True):
    symbols = sorted(
        list(set(
            ase.symbols.string2symbols(''.join(
                map(
                    lambda _x:
                    ''.join(ase.symbols.string2symbols(_x)), molecules)
            ))
        )),
        key=lambda _y: ase.data.atomic_numbers[_y])

    if add_hydrogen and 'H' not in symbols:
        symbols.insert(0, 'H')

    return symbols

def construct_reference_system(
    symbols,
    candidates=None,
    options=None,
):
    if hasattr(options, 'no_hydrogen') and options.no_hydrogen:
        add_hydrogen = False
    else:
        add_hydrogen = True

    references = {}
    sorted_candidates = [
        'H2',
        'H2O',
        'NH3',
        'N2',
        'CH4',
        'CO',
        'H2S',
        'HCl',
        'O2']
    if candidates is None:
        candidates = sorted_candidates
    else:
        odd_candidates = [c for c in candidates if c not in sorted_candidates]
        candidates = [c for c in sorted_candidates if c in candidates] \
            + odd_candidates

    added_symbols = []
    # go symbols in adsorbate
    # to add reference species in procedural manner
    for symbol in symbols:
        added_symbols.append(symbol)
        for candidate in candidates:
            _symbols = ase.symbols.string2symbols(candidate)
            # Add partial adsorbate species
            # is subset of reference species
            # and reference species
            # is subset of full adsorbate species set
            if set(added_symbols) <= set(list(references.keys()) + _symbols) \
                    and set(list(references.keys()) + _symbols) <= set(symbols) \
                    and candidate not in references.values():
                references[symbol] = candidate
                break
        else:
            raise UserWarning((
                "No candidate satisfied {symbol}. Add more candidates\n"
                "    Symbols {symbols}\n"
                "    _Symbols {_symbols}\n"
                "    References {references}\n"
                "    Candidates {candidates}\n"
            ).format(
                symbol=symbol,
                symbols=symbols,
                _symbols=_symbols,
                candidates=candidates,
                references=list(references.keys()),
            ))

    sorted_references = []
    references = list(references.items())

    # put references in order so that each reference
    # only adds one one additional species in each step
    # while references:
    #     for i, reference in enumerate(references):
    #         if len(set(ase.symbols.string2symbols(reference[1])) -
    #                 set(x[0] for x in sorted_references)) == 1:
    #             sorted_references.append(references.pop(i))
    #             break

    return references

def get_stoichiometry_factors(adsorbates, references):
    stoichiometry = get_atomic_stoichiometry(references)
    stoichiometry_factors = {}
    for adsorbate in adsorbates:
        for symbol in ase.symbols.string2symbols(adsorbate):
            symbol_index = list(
                map(lambda _x: _x[0], references)).index(symbol)

            for (factor, (ref_symbol, ref_molecule)) in zip(
                    stoichiometry[symbol_index], references):

                stoichiometry_factors.setdefault(
                    adsorbate,

                    {})[ref_molecule] = stoichiometry_factors.setdefault(
                    adsorbate,
                    {}).get(
                    ref_molecule,
                    0) + factor

        nonzero_factors = {}
        for key, value in stoichiometry_factors[adsorbate].items():
            if not np.isclose(value, 0.):
                nonzero_factors[key] = value
        stoichiometry_factors[adsorbate] = nonzero_factors

    return stoichiometry_factors

def get_fields_dict(self, row):
        return {k: getattr(self, 'clean_{}'.format(k), lambda x: x)(v.strip() if isinstance(v, str)
                                                                    else None)
                for k, v in zip_longest(self.get_fields(), row)}

def process_node(node):
    more = {}
    for key, value  in data.items():
        if key != 'elements':
            newnode = value
        else:
            newnode = {}
            for el in value:
                nkey, nvalue = process_node(el)
                newnode[nkey] = nvalue

        more[key] = newnode

    return more

def _finalize(self, all_msg_errors=None):
        if all_msg_errors is None:
            all_msg_errors = []

        for key in self.stored():
            try:
                getattr(self, key)
            except (ValueError, TypeError) as err:
                all_msg_errors.append(err.args[0])

        # Raises a list of all the missing entries
        if all_msg_errors:
            raise ValueError(all_msg_errors)

def validate(self):

    @wraps(method)

    def mod_run(self, rinput):
        self.validate_input(rinput)
        #
        result = method(self, rinput)
        #
        self.validate_result(result)
        return result

    return mod_run

def as_list(callable):

    Parameters
    ==========
    minval: numeric, optional:
        Values strictly lesser than `minval` are rejected
    maxval: numeric, optional:
        Values strictly greater than `maxval` are rejected

    Returns
    =======
    A function that returns values if are in the range and raises
    ValidationError is the values are outside the range


	# Test if the pylpfile exists
	readable_path = make_readable_path(path)
	if not os.path.isfile(path):
		logger.log(logger.red("Can't read pylpfile "), logger.magenta(readable_path))
		sys.exit(-1)
	else:
		logger.log("Using pylpfile ", logger.magenta(readable_path))


	# Run the pylpfile
	try:
		runpy.run_path(path, None, "pylpfile")
	except Exception as e:
		traceback.print_exc(file=sys.stdout)
		logger.log(logger.red("\nAn error has occurred during the execution of the pylpfile"))
		sys.exit(-1)


	# Start the tasks
	for name in tasks:
		pylp.start(name)

	# Wait until all task are executed
	loop = asyncio.get_event_loop()
	loop.run_until_complete(wait_and_quit(loop))

async def wait_and_quit(loop):

        We say that a record is published if it is citeable, which means that
        it has enough information in a ``publication_info``, or if we know its
        DOI and a ``journal_title``, which means it is in press.

        Returns:
            bool: whether the record is published.

        Examples:
            >>> record = {
            ...     'dois': [
            ...         {'value': '10.1016/0029-5582(61)90469-2'},
            ...     ],
            ...     'publication_info': [
            ...         {'journal_title': 'Nucl.Phys.'},
            ...     ],
            ... }
            >>> LiteratureReader(record).is_published
            True


        Args:
            publication_info(dict): a publication_info field entry of a record

            separator(basestring): optional page range symbol, defaults to a single dash

        Returns:
            string: the page range or the article id of the record.

        Examples:
            >>> publication_info = {'artid': '054021'}
            >>> get_page_artid(publication_info)
            '054021'


        Args:

            separator(basestring): optional page range symbol, defaults to a single dash

        Returns:
            string: the page range or the article id of the record.

        Examples:
            >>> record = {
            ...     'publication_info': [
            ...         {'artid': '054021'},
            ...     ],
            ... }
            >>> LiteratureReader(record).get_page_artid()
            '054021'


    >>> list(chunkreverse([0b10000000, 0b11000000, 0b00000001], 'B'))
    [1, 3, 128]

    >>> list(chunkreverse([0x8000, 0xC000, 0x0001], 'H'))
    [1, 3, 32768]

    >>> pack([0, 1, 0, 1, 0, 1], 1)
    42

    >>> pack([0, 1], 8)
    256

    >>> pack([0, 1], 0)
    Traceback (most recent call last):
        ...
    ValueError: pack needs r > 0

    >>> list(unpack(42, 1))
    [0, 1, 0, 1, 0, 1]

    >>> list(unpack(256, 8))
    [0, 1]

    >>> list(unpack(2, 0))
    Traceback (most recent call last):
        ...
    ValueError: unpack needs r > 0

    >>> list(packbools([False, True, False, True, False, True], 'B'))
    [42]

    >>> list(unpackbools([42], 'B'))
    [False, True, False, True, False, True, False, False]

    Parameters
    ----------
    list_of_wvfeatures : list (of WavecalFeature instances)
        A list of size equal to the number of identified lines, which
        elements are instances of the class WavecalFeature, containing
        all the relevant information concerning the line
        identification.

    Returns
    -------
    nfit : int
        Number of valid points for posterior fits.
    ifit : list of int
        List of indices corresponding to the arc lines which
        coordinates are going to be employed in the posterior fits.
    xfit : 1d numpy aray
        X coordinate of points for posterior fits.
    yfit : 1d numpy array
        Y coordinate of points for posterior fits.
    wfit : 1d numpy array
        Cost function of points for posterior fits. The inverse of
        these values can be employed for weighted fits.


    Determine all the possible triplets that can be generated from the
    array `wv_master`. In addition, the relative position of the
    central line of each triplet is also computed.

    Parameters
    ----------
    wv_master : 1d numpy array, float
        Array with wavelengths corresponding to the master table
        (Angstroms).
    geometry : tuple (4 integers) or None
        x, y, dx, dy values employed to set the window geometry.
    debugplot : int
        Determines whether intermediate computations and/or plots

        are displayed. The valid codes are defined in
        numina.array.display.pause_debugplot.

    Returns
    -------
    ntriplets_master : int
        Number of triplets built from master table.
    ratios_master_sorted : 1d numpy array, float
        Array with values of the relative position of the central line
        of each triplet, sorted in ascending order.
    triplets_master_sorted_list : list of tuples
        List with tuples of three numbers, corresponding to the three
        line indices in the master table. The list is sorted to be in
        correspondence with `ratios_master_sorted`.


    This function is a wrapper of two functions, which are responsible
    of computing all the relevant information concerning the triplets
    generated from the master table and the actual identification
    procedure of the arc lines, respectively.

    The separation of those computations in two different functions
    helps to avoid the repetition of calls to the first function when
    calibrating several arcs using the same master table.

    Parameters
    ----------
    wv_master : 1d numpy array, float
        Array with wavelengths corresponding to the master table
        (Angstroms).
    xpos_arc : 1d numpy array, float
        Location of arc lines (pixels).
    naxis1_arc : int
        NAXIS1 for arc spectrum.
    crpix1 : float
        CRPIX1 value to be employed in the wavelength calibration.
    wv_ini_search : float
        Minimum expected wavelength in spectrum.
    wv_end_search : float
        Maximum expected wavelength in spectrum.
    wvmin_useful : float
        If not None, this value is used to clip detected lines below it.
    wvmax_useful : float
        If not None, this value is used to clip detected lines above it.
    error_xpos_arc : float
        Error in arc line position (pixels).
    times_sigma_r : float
        Times sigma to search for valid line position ratios.
    frac_triplets_for_sum : float
        Fraction of distances to different triplets to sum when
        computing the cost function.
    times_sigma_theil_sen : float
        Number of times the (robust) standard deviation around the
        linear fit (using the Theil-Sen method) to reject points.
    poly_degree_wfit : int
        Degree for polynomial fit to wavelength calibration.
    times_sigma_polfilt : float
        Number of times the (robust) standard deviation around the
        polynomial fit to reject points.
    times_sigma_cook : float
        Number of times the standard deviation of Cook's distances
        to detect outliers. If zero, this method of outlier detection
        is ignored.
    times_sigma_inclusion : float
        Number of times the (robust) standard deviation around the
        polynomial fit to include a new line in the set of identified
        lines.
    geometry : tuple (4 integers) or None
        x, y, dx, dy values employed to set the window geometry.
    debugplot : int
        Determines whether intermediate computations and/or plots

        are displayed. The valid codes are defined in
        numina.array.display.pause_debugplot.

    Returns
    -------
    list_of_wvfeatures : list (of WavecalFeature instances)
        A list of size equal to the number of identified lines, which
        elements are instances of the class WavecalFeature, containing
        all the relevant information concerning the line
        identification.


    Assign individual wavelengths from wv_master to each expected
    wavelength when the latter is within the maximum allowed range.

    Parameters
    ----------
    wv_master : numpy array
        Array containing the master wavelengths.
    wv_expected_all_peaks : numpy array
        Array containing the expected wavelengths (computed, for
        example, from an approximate polynomial calibration applied to
        the location of the line peaks).
    delta_wv_max : float
        Maximum distance to accept that the master wavelength
        corresponds to the expected wavelength.

    Returns
    -------
    wv_verified_all_peaks : numpy array
        Verified wavelengths from master list.


    Parameters
    ==========
    geometry : tuple (4 integers) or None
        x, y, dx, dy values employed to set the Qt backend geometry.

    values = []
    line = []
    for width, parser in types:
        if not line:
            line = lines.pop(0).replace('\n', '')

        values.append(parser(line[:width]))
        line = line[width:]

    return values

def _parse_curves(block, **kwargs):
    wt_layer, length, _, name = parse_fixed_width(
        3 * [(5, int)] + [(55, to_str)], block)

    layers = []
    soil_types = []
    for i in range(length):
        index, soil_idx, thickness, shear_mod, damping, unit_wt, shear_vel = \
            parse_fixed_width(
                [(5, int), (5, int), (15, to_float)] + 4 * [(10, to_float)],
                block
            )

        st = site.SoilType(
            soil_idx,
            unit_wt,
            curves[(soil_idx, 'mod_reduc')],
            curves[(soil_idx, 'damping')], )
        try:
            # Try to find previously added soil type
            st = soil_types[soil_types.index(st)]
        except ValueError:
            soil_types.append(st)

        layers.append(site.Layer(st, thickness, shear_vel))

    if units == 'english':
        # Convert from English to metric
        for st in soil_types:
            st.unit_wt *= 0.00015708746

        for l in layers:
            l.thickness *= 0.3048
            l.shear_vel *= 0.3048

    p = site.Profile(layers)
    p.update_layers()
    p.wt_depth = p[wt_layer - 1].depth

    return p

def _parse_motion(block, **kwargs):
    layer, wave_field = parse_fixed_width(2 * [(5, int)], block)

    return profile.location(
        motion.WaveField[wave_field],
        index=(layer - 1), )

def _parse_run_control(block):

    blockgen1d computes the slices by recursively halving the initial
    interval (0, size) by 2 until its size is lesser or equal than block

    :param block: an integer maximum block size
    :param size: original size of the interval, it corresponds to a 0:size slice
    :return: a list of slices

    Example:

        >>> blockgen1d(512, 1024)
        [slice(0, 512, None), slice(512, 1024, None)]



    The tuples represent regions in an N-dimensional image.

    :param blocks: a tuple of block sizes
    :param shape: the shape of the n-dimensional array
    :return: an iterator to the list of tuples of slices

    Example:

        >>> blocks = (500, 512)
        >>> shape = (1040, 1024)
        >>> for i in blockgen(blocks, shape):
        ...     print i
        (slice(0, 260, None), slice(0, 512, None))
        (slice(0, 260, None), slice(512, 1024, None))
        (slice(260, 520, None), slice(0, 512, None))
        (slice(260, 520, None), slice(512, 1024, None))
        (slice(520, 780, None), slice(0, 512, None))
        (slice(520, 780, None), slice(512, 1024, None))
        (slice(780, 1040, None), slice(0, 512, None))
        (slice(780, 1040, None), slice(512, 1024, None))


    :param blk: size of the 1d block
    :param size: size of the 1d a image
    :return: a tuple of size covered and remaining size

    Example:

        >>> blk_coverage_1d(7, 100)
        (98, 2)


    :param blk: the N-dimensional shape of the block
    :param shape: the N-dimensional shape of the array
    :return: the shape of the covered region

    Example:

        >>> max_blk_coverage(blk=(7, 6), shape=(100, 43))
        (98, 42)



    Iterate trough the blocks that recover the part of the array
    given by max_blk_coverage.

    :param blk: the N-dimensional shape of the block
    :param shape: the N-dimensional shape of the array
    :return: a generator that yields the blocks

    Example:

        >>> result = list(blk_nd_short(blk=(5,3), shape=(11, 11)))
        >>> result[0]
        (slice(0, 5, None), slice(0, 3, None))
        >>> result[1]
        (slice(0, 5, None), slice(3, 6, None))
        >>> result[-1]
        (slice(5, 10, None), slice(6, 9, None))

        In this case, the output of max_blk_coverage
        is (10, 9), so only this part of the array is covered


    .. seealso::

        :py:func:`blk_nd`
          Yields blocks of blk size until the remaining part is
          smaller than `blk` and the yields smaller blocks.


    This function first iterates trough the blocks that recover
    the part of the array given by max_blk_coverage
    and then iterates with smaller blocks for the rest
    of the array.

    :param blk: the N-dimensional shape of the block
    :param shape: the N-dimensional shape of the array
    :return: a generator that yields the blocks

    Example:

        >>> result = list(blk_nd(blk=(5,3), shape=(11, 11)))
        >>> result[0]
        (slice(0, 5, None), slice(0, 3, None))
        >>> result[1]
        (slice(0, 5, None), slice(3, 6, None))
        >>> result[-1]
        (slice(10, 11, None), slice(9, 11, None))

    The generator yields blocks of size blk until
    it covers the part of the array given by
    :func:`max_blk_coverage` and then yields
    smaller blocks until it covers the full array.

    .. seealso::

        :py:func:`blk_nd_short`
          Yields blocks of fixed size


    No error checking made. Therefore meaningful (as implemented) only for
    blocks strictly compatible with the shape of A.


    :param publication_info: publication_info field
    already populated
    :type publication_info: list

        :param abstract: abstract for the current document.
        :type abstract: string

        :param source: source for the given abstract.
        :type source: string

        :param arxiv_id: arxiv id for the current document.
        :type arxiv_id: string

        :param arxiv_categories: arXiv categories for the current document.
        :type arxiv_categories: list

        :param doi: doi for the current document.
        :type doi: string

        :param source: source for the doi.
        :type source: string

        :param material: material for the doi.
        :type material: string

        Args:
            full_name(str): full name of the author. If not yet in standard
                Inspire form, it will be normalized.
            affiliations(List[str]): Inspire normalized affiliations of the
                author.
            roles(List[str]): Inspire roles of the author.
            raw_affiliations(List[str]): raw affiliation strings of the author.
            source(str): source for the affiliations when
                ``affiliations_normalized`` is ``False``.
            ids(List[Tuple[str,str]]): list of ids of the author, whose
                elements are of the form ``(schema, value)``.
            emails(List[str]): email addresses of the author.
            alternative_names(List[str]): alternative names of the author.

        Returns:
            dict: a schema-compliant subrecord.
        Make a dictionary that is representing a book.

        :param publisher: publisher name

        :type publisher: string

        :param place: place of publication
        :type place: string

        :param date: A (partial) date in any format.
            The date should contain at least a year
        :type date: string

        :rtype: dict

        :param subject_terms: user categories for the current document.
        :type subject_terms: list

        :param source: source for the given categories.
        :type source: string

        Args:
            keyword(str): keyword to add.
            schema(str): schema to which the keyword belongs.
            source(str): source for the keyword.

        :param private_notes: hidden notes for the current document
        :type private_notes: string

        :param source: source for the given private notes
        :type source: string

        :param year: year of publication
        :type year: integer

        :param cnum: inspire conference number
        :type cnum: string

        :param artid: article id
        :type artid: string

        :param page_end: final page for the article
        :type page_end: string

        :param page_start: initial page for the article
        :type page_start: string

        :param journal_issue: issue of the journal where
        the document has been published
        :type journal_issue: string

        :param journal_title: title of the journal where
        the document has been published
        :type journal_title: string

        :param journal_volume: volume of the journal where
        the document has been published
        :type journal_volume: string

        :param pubinfo_freetext: Unstructured text describing the publication
        information.
        :type pubinfo_freetext: string

        :param material: material of the article
        :type material: string

        :param parent_record: reference for the parent record
        :type parent_record: string

        :param parent_isbn: isbn for the parent record
        :type parent_isbn: string


        :param defense_date: defense date for the current thesis

        :type defense_date: string. A formatted date is required (yyyy-mm-dd)

        :param degree_type: degree type for the current thesis
        :type degree_type: string

        :param institution: author's affiliation for the current thesis
        :type institution: string

        :param date: publication date for the current thesis
        :type date: string. A formatted date is required (yyyy-mm-dd)

        :param url: url for the description of the license
        :type url: string

        :param license: license type
        :type license: string

        :param material: material type
        :type material: string

        :param imposing: imposing type
        :type imposing: string

        :param public_note: public note for the current article.
        :type public_note: string

        :param source: source for the given notes.
        :type source: string

        :param title: title for the current document
        :type title: string

        :param subtitle: subtitle for the current document
        :type subtitle: string

        :param source: source for the given title
        :type source: string

        :param title: translated title
        :type title: string

        :param language: language for the original title
        :type language: string (2 characters ISO639-1)

        :param source: source for the given title
        :type source: string

        :param report_number: report number for the current document
        :type report_number: string

        :param source: source for the given report number
        :type source: string

        :param collaboration: collaboration for the current document
        :type collaboration: string

        :type material: string

        :type holder: string

        :type statement: string

        :type url: string

        :type year: int

        Args:
            key (string): document key
            url (string): document url
        Keyword Args:
            caption (string): simple description
            label (string):
            material (string):
            original_url (string): original url
            filename (string): current url

        Returns: None


    Fit a rotation matrix and a traslation bewtween two matched sets
    consisting of M N-dimensional points

    Parameters
    ----------
    coords0 : (M, N) array_like
    coords1 : (M, N) array_lke

    Returns
    -------
    offset : (N, ) array_like
    rotation : (N, N) array_like

    Notes
    ------
    Fit offset and rotation using Kabsch's algorithm [1]_ [2]_

    .. [1] Kabsch algorithm: https://en.wikipedia.org/wiki/Kabsch_algorithm

    .. [2] Also here: http://nghiaho.com/?page_id=671


	def _color(text = ""):
		return (_color_sep + color + _color_sep2 + text +

			_color_sep + "default" + _color_sep2)
	return _color

def just_log(*texts, sep = ""):
	text = sep.join(texts)
	count = text.count("\n")
	just_log("\n" * count, *get_time(), text.replace("\n", ""), sep=sep)

def find_files(globs):

	# Create an array of globs if only one string is given
	if isinstance(globs, str):
		globs = [ globs ]

	# Find files
	files = find_files(globs)

	# Create a stream
	stream = Stream()

	# Options
	options["cwd"] = config.cwd

	if "base" in options:
		options["base"] = os.path.abspath(options["base"])

	# Create a File object for each file to include
	for infile in files:
		file = File(infile[2], **options)
		file.relpath = file.path
		file.order = infile[0]
		file.base = options.get("base", infile[1])
		stream.append_file(file)

	# No more files to add
	stream.end_of_stream()

	# Pipe a file reader and return the stream
	if options.get("read", True):
		return stream.pipe(FileReader())
	return stream

def log_to_history(logger, name):
        result = {}
        result['instrument'] = ''
        result['uuid'] = ''
        result['tags'] = {}
        result['type'] = ''
        result['mode'] = ''
        result['observation_date'] = ""
        result['origin'] = {}
        return result

def task(obj = None, deps = None):
        c = os.read(fd, 1)
        if not c:
            raise OSError
        return c

def arg_file_is_new(parser, arg, mode='w'):
    if os.path.exists(arg):
        parser.error("\nThe file \"%s\"\nalready exists and "
                     "cannot be overwritten!" % arg)
    else:
        # return an open file handle
        handler = open(arg, mode=mode)
        return handler

def intersection_spectrail_arcline(spectrail, arcline):

    # approximate location of the solution
    expected_x = (arcline.xlower_line + arcline.xupper_line) / 2.0

    # composition of polynomials to find intersection as
    # one of the roots of a new polynomial
    rootfunct = arcline.poly_funct(spectrail.poly_funct)
    rootfunct.coef[1] -= 1
    # compute roots to find solution
    tmp_xroots = rootfunct.roots()

    # take the nearest root to the expected location
    xroot = tmp_xroots[np.abs(tmp_xroots - expected_x).argmin()]
    if np.isreal(xroot):
        xroot = xroot.real
    else:
        raise ValueError("xroot=" + str(xroot) +
                         " is a complex number")
    yroot = spectrail.poly_funct(xroot)

    return xroot, yroot

def offset(self, offset_value):

        new_instance = deepcopy(self)
        new_instance.poly_funct.coef[0] += offset_value
        return new_instance

def compute_operation(file1, file2, operation, output, display,
                      args_z1z2, args_bbox, args_keystitle, args_geometry):

    # read first FITS file
    with fits.open(file1) as hdulist:
        image_header1 = hdulist[0].header
        image1 = hdulist[0].data.astype(np.float)
    naxis1 = image_header1['naxis1']
    naxis2 = image_header1['naxis2']

    # if required, display file1
    if display == 'all':
        ximshow_file(file1.name,
                     args_z1z2=args_z1z2, args_bbox=args_bbox,
                     args_keystitle=args_keystitle,
                     args_geometry=args_geometry,
                     debugplot=12)

    # read second FITS file
    with fits.open(file2) as hdulist:
        image_header2 = hdulist[0].header
        image2 = hdulist[0].data.astype(np.float)
    naxis1_ = image_header2['naxis1']
    naxis2_ = image_header2['naxis2']

    # if required, display file2
    if display == 'all':
        ximshow_file(file2.name,
                     args_z1z2=args_z1z2, args_bbox=args_bbox,
                     args_keystitle=args_keystitle,
                     args_geometry=args_geometry,
                     debugplot=12)

    # check dimensions
    if naxis1 != naxis1_:
        raise ValueError("NAXIS1 values are different.")
    if naxis2 != naxis2_:
        raise ValueError("NAXIS2 values are different.")

    # compute operation
    if operation == "+":
        solution = image1 + image2
    elif operation == "-":
        solution = image1 - image2
    elif operation == "*":
        solution = image1 * image2
    elif operation == "/":
        solution = image1 / image2
    else:
        raise ValueError("Unexpected operation=" + str(operation))

    # save output file
    hdu = fits.PrimaryHDU(solution.astype(np.float), image_header1)
    hdu.writeto(output, overwrite=True)

    # if required, display result
    if display in ['all', 'result']:
        ximshow_file(output.name,
                     args_z1z2=args_z1z2, args_bbox=args_bbox,
                     args_keystitle=args_keystitle,
                     args_geometry=args_geometry,
                     debugplot=12)

def robust_std(x, debug=False):

    x = numpy.asarray(x)

    # compute percentiles and robust estimator
    q25 = numpy.percentile(x, 25)
    q75 = numpy.percentile(x, 75)
    sigmag = 0.7413 * (q75 - q25)

    if debug:
        print('debug|sigmag -> q25......................:', q25)
        print('debug|sigmag -> q75......................:', q75)
        print('debug|sigmag -> Robust standard deviation:', sigmag)

    return sigmag

def summary(x, rm_nan=False, debug=False):

    # protections
    if type(x) is np.ndarray:
        xx = np.copy(x)
    else:
        if type(x) is list:
            xx = np.array(x)
        else:
            raise ValueError('x=' + str(x) + ' must be a numpy.ndarray')

    if xx.ndim is not 1:
        raise ValueError('xx.dim=' + str(xx.ndim) + ' must be 1')

    # filter out NaN's
    if rm_nan:
        xx = xx[np.logical_not(np.isnan(xx))]

    # compute basic statistics
    npoints = len(xx)
    ok = npoints > 0
    result = {
        'npoints' : npoints,
        'minimum' : np.min(xx) if ok else 0,
        'percentile25' : np.percentile(xx, 25) if ok else 0,
        'median' : np.percentile(xx, 50) if ok else 0,
        'mean' : np.mean(xx) if ok else 0,
        'percentile75': np.percentile(xx, 75) if ok else 0,
        'maximum' : np.max(xx) if ok else 0,
        'std': np.std(xx) if ok else 0,
        'robust_std' : robust_std(xx) if ok else 0,
        'percentile15': np.percentile(xx, 15.86553) if ok else 0,
        'percentile84': np.percentile(xx, 84.13447) if ok else 0
    }

    if debug:
        print('>>> ========================================')
        print('>>> STATISTICAL SUMMARY:')
        print('>>> ----------------------------------------')
        print('>>> Number of points.........:', result['npoints'])
        print('>>> Minimum..................:', result['minimum'])
        print('>>> 1st Quartile.............:', result['percentile25'])
        print('>>> Median...................:', result['median'])
        print('>>> Mean.....................:', result['mean'])
        print('>>> 3rd Quartile.............:', result['percentile75'])
        print('>>> Maximum..................:', result['maximum'])
        print('>>> ----------------------------------------')
        print('>>> Standard deviation.......:', result['std'])
        print('>>> Robust standard deviation:', result['robust_std'])
        print('>>> 0.1586553 percentile.....:', result['percentile15'])
        print('>>> 0.8413447 percentile.....:', result['percentile84'])
        print('>>> ========================================')

    return result

def fit_trace_polynomial(trace, deg, axis=0):
    '''

    dispaxis = axis_to_dispaxis(axis)

    # FIT to a polynomial
    pfit = numpy.polyfit(trace[:,0], trace[:,1], deg)
    start = trace[0,0]
    stop = trace[-1,0],
    return PolyTrace(start, stop, axis, pfit)

def price_humanized(value, inst, currency=None):
    return (natural_number_with_currency(value, ugettext('CZK') if currency is None else currency) if value is not None
            else ugettext('(None)'))

def get_imgid(self, img):
        imgid = img.filename()

        # More heuristics here...
        # get FILENAME keyword, CHECKSUM, for example...
        hdr = self.get_header(img)
        if 'checksum' in hdr:
            return hdr['checksum']

        if 'filename' in hdr:
            return hdr['filename']

        if not imgid:
            imgid = repr(img)

        return imgid

def log_starting(self):
		delta = time.perf_counter() - self.start_time
		logger.log("Finished '", logger.cyan(self.name),
			"' after ", logger.magenta(time_to_text(delta)))

def call_task_fn(self):
		if not isinstance(stream, Stream):
			future.set_result(None)
		else:
			stream.pipe(TaskEndTransformer(future))

async def start_deps(self, deps):
        return cls.fromint(sum(map(cls._map.__getitem__, set(members))))

def frombools(cls, bools=()):
        if len(bits) > cls._len:
            raise ValueError('too many bits %r' % (bits,))
        return cls.fromint(bits[::-1], 2)

def atoms(self, reverse=False):
        if reverse:
            return filterfalse(self.__and__, reversed(self._atoms))
        return filterfalse(self.__and__, self._atoms)

def powerset(self, start=None, excludestart=False):
    Changes a given `changed_fields` on object and returns changed object.
    Changes a given `changed_fields` on object, saves it and returns changed object.
    Changes a given `changed_fields` on each object in a given `iterable`, saves objects
    and returns the changed objects.
    z = (x - mean) / stddev
    z2 = z + hpix / stddev
    z1 = z - hpix / stddev
    return amplitude * (norm.cdf(z2) - norm.cdf(z1))

def gauss_box_model_deriv(x, amplitude=1.0, mean=0.0, stddev=1.0, hpix=0.5):

    The algorithm imposes that the signal at both sides of the peak
    decreases monotonically.

    Parameters
    ----------
    sx : 1d numpy array, floats
        Input array.
    nwinwidth : int
        Width of the window where each peak must be found.
    threshold : float
        Minimum signal in the peaks.
    debugplot : int
        Determines whether intermediate computations and/or plots
        are displayed:
        00 : no debug, no plots
        01 : no debug, plots without pauses
        02 : no debug, plots with pauses
        10 : debug, no plots
        11 : debug, plots without pauses
        12 : debug, plots with pauses

    Returns
    -------
    ixpeaks : 1d numpy array, int
        Peak locations, in array coordinates (integers).

    home = os.path.expanduser("~")
    if path.startswith(home):
        path = "~" + path[len(home):]

    return path

def shortlex(start, other, excludestart=False):
    if not excludestart:
        yield start

    queue = collections.deque([(start, other)])

    while queue:
        current, other = queue.popleft()

        while other:
            first, other = other[0], other[1:]
            result = current | first

            yield result

            if other:
                queue.append((result, other))

def reverse_shortlex(end, other, excludeend=False):
    if not excludeend:
        yield end

    queue = collections.deque([(end, other)])

    while queue:
        current, other = queue.popleft()

        while other:
            first, other = other[0], other[1:]
            result = current & first

            yield result

            if other:
                queue.append((result, other))

def generate_filename(self, instance, filename):
        from unidecode import unidecode

        return super().generate_filename(instance, unidecode(force_text(filename)))

def next(self):
        if self.blocking >= 0:
            # returns queue name and item, we just need item
            res = self.redis.blpop([self.name], timeout=self.blocking)
            if res:
                res = res[1]
        else:
            res = self.redis.lpop(self.name)
        value = self.deserialize(res)
        logger.debug('Popped from "%s": %s', self.name, repr(value))
        return value

def send(self, *args):
        # this and the serializer could use some streamlining
        if None in args:
            raise TypeError('None is not a valid queue item.')
        serialized_values = [self.serialize(value) for value in args]
        logger.debug('Sending to "%s": %s', self.name, serialized_values)
        return self.redis.rpush(self.name, *serialized_values)

def clear(self):
    Converts a 32 bit unsigned number to signed.

    uint32:= an unsigned 32 bit number

    ...
    print(u2i(4294967272))
    -24
    print(u2i(37))
    37
    ...
    Converts a 32 bit unsigned number to signed.  If the number
    is negative it indicates an error.  On error a pigpio
    exception will be raised if exceptions is True.
        self.callbacks.append(cb.callb)
        self.monitor = self.monitor | cb.callb.bit

        yield from self.pi._pigpio_aio_command(_PI_CMD_NB, self.handle,
                                               self.monitor)

def remove(self, cb):
        Runs a pigpio socket command.

        sl:= command socket and lock.
        cmd:= the command to be executed.
        p1:= command parameter 1 (if applicable).
         p2:=  command parameter 2 (if applicable).
        Runs an extended pigpio socket command.

            sl:= command socket and lock.
           cmd:= the command to be executed.
            p1:= command parameter 1 (if applicable).
            p2:= command parameter 2 (if applicable).
            p3:= total size in bytes of following extents
        extents:= additional data blocks
        Store a script for later execution.

        script:= the script text as a series of bytes.

        Returns a >=0 script id if OK.

        ...
        sid = pi.store_script(
         b'tag 0 w 22 1 mils 100 w 22 0 mils 100 dcr p0 jp 0')
        ...
        Runs a stored script.

        script_id:= id of stored script.
         params:= up to 10 parameters required by the script.

        ...
        s = pi.run_script(sid, [par1, par2])

        s = pi.run_script(sid)

        s = pi.run_script(sid, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
        ...
        Returns the run status of a stored script as well as the
        current values of parameters 0 to 9.

        script_id:= id of stored script.

        The run status may be

        . .
        PI_SCRIPT_INITING
        PI_SCRIPT_HALTED
        PI_SCRIPT_RUNNING
        PI_SCRIPT_WAITING
        PI_SCRIPT_FAILED
        . .

        The return value is a tuple of run status and a list of
        the 10 parameters.  On error the run status will be negative
        and the parameter list will be empty.

        ...
        (s, pars) = pi.script_status(sid)
        ...
        Stops a running script.

        script_id:= id of stored script.

        ...
        status = pi.stop_script(sid)
        ...
        Deletes a stored script.

        script_id:= id of stored script.

        ...
        status = pi.delete_script(sid)
        ...
        Clears gpios 0-31 if the corresponding bit in bits is set.

        bits:= a 32 bit mask with 1 set if the corresponding gpio is
             to be cleared.

        A returned status of PI_SOME_PERMITTED indicates that the user
        is not allowed to write to one or more of the gpios.

        ...
        pi.clear_bank_1(int("111110010000",2))
        ...
        Sets gpios 0-31 if the corresponding bit in bits is set.

        bits:= a 32 bit mask with 1 set if the corresponding gpio is
             to be set.

        A returned status of PI_SOME_PERMITTED indicates that the user
        is not allowed to write to one or more of the gpios.

        ...
        pi.set_bank_1(int("111110010000",2))
        ...
        Sets the gpio mode.

        gpio:= 0-53.
        mode:= INPUT, OUTPUT, ALT0, ALT1, ALT2, ALT3, ALT4, ALT5.

        ...
        pi.set_mode( 4, apigpio.INPUT)  # gpio  4 as input
        pi.set_mode(17, apigpio.OUTPUT) # gpio 17 as output
        pi.set_mode(24, apigpio.ALT2)   # gpio 24 as ALT2
        ...
        Returns the gpio mode.

        gpio:= 0-53.

        Returns a value as follows

        . .
        0 = INPUT
        1 = OUTPUT
        2 = ALT5
        3 = ALT4
        4 = ALT0
        5 = ALT1
        6 = ALT2
        7 = ALT3
        . .

        ...
        print(pi.get_mode(0))
        4
        ...
        Sets the gpio level.

        gpio:= 0-53.
        level:= 0, 1.

        If PWM or servo pulses are active on the gpio they are
        switched off.

        ...
        pi.set_mode(17, pigpio.OUTPUT)

        pi.write(17,0)
        print(pi.read(17))
        0

        pi.write(17,1)
        print(pi.read(17))
        1
        ...
    s = re.findall(r'([A-Z][a-z0-9]+)', s)
    return [w.lower() for w in s] if lower else s

def get_route_param_names(endpoint):
        result = super(BaseStructuredCalibration, self).update_meta_info()

        result['instrument'] = self.instrument
        result['uuid'] = self.uuid
        result['tags'] = self.tags
        result['type'] = self.name()

        minfo = self.meta_info
        try:
            result['mode'] = minfo['mode_name']
            origin = minfo['origin']
            date_obs = origin['date_obs']
        except KeyError:
            origin = {}
            date_obs = "1970-01-01T00:00:00.00"

        result['observation_date'] = conv.convert_date(date_obs)
        result['origin'] = origin

        return result

def gnuplot(script_name, args_dict={}, data=[], silent=True):
    '''

    gnuplot_command = 'gnuplot'

    if data:
        assert 'data' not in args_dict, \
            'Can\'t use \'data\' variable twice.'
        data_temp = _GnuplotDataTemp(*data)
        args_dict['data'] = data_temp.name

    if args_dict:
        gnuplot_command += ' -e "'
        for arg in args_dict.items():
            gnuplot_command += arg[0] + '='
            if isinstance(arg[1], str):
                gnuplot_command += '\'' + arg[1] + '\''
            elif isinstance(arg[1], bool):
                if arg[1] is True:
                    gnuplot_command += '1'
                else:
                    gnuplot_command += '0'
            elif hasattr(arg[1], '__iter__'):
                gnuplot_command += '\'' + ' '.join([str(v) for v in arg[1]]) + '\''
            else:
                gnuplot_command += str(arg[1])
            gnuplot_command += '; '
        gnuplot_command  = gnuplot_command[:-1]
        gnuplot_command += '"'

    gnuplot_command += ' ' + script_name

    if silent:
        gnuplot_command += ' > /dev/null 2>&1'

    os.system(gnuplot_command)

    return gnuplot_command

def gnuplot_2d(x, y, filename, title='', x_label='', y_label=''):
    '''
    _, ext = os.path.splitext(filename)
    if ext != '.png':
        filename += '.png'

    gnuplot_cmds = \
    '''
    scr = _GnuplotScriptTemp(gnuplot_cmds)
    data = _GnuplotDataTemp(x, y)

    args_dict = {
        'filename': filename,
        'filename_data': data.name,
        'title': title,
        'x_label': x_label,
        'y_label': y_label
    }
    gnuplot(scr.name, args_dict)

def gnuplot_3d_matrix(z_matrix, filename, title='', x_label='', y_label=''):
    '''
    _, ext = os.path.splitext(filename)
    if ext != '.png':
        filename += '.png'

    gnuplot_cmds = \
    '''
    scr = _GnuplotScriptTemp(gnuplot_cmds)
    data = _GnuplotDataZMatrixTemp(z_matrix)

    args_dict = {
        'filename': filename,
        'filename_data': data.name,
        'title': title,
        'x_label': x_label,
        'y_label': y_label
    }
    gnuplot(scr.name, args_dict)

def read(self, skip=[], goto_metal=None, goto_reaction=None):
        if len(skip) > 0:
            for skip_f in skip:
                self.omit_folders.append(skip_f)

    '''
    call eagle and export sch or brd to partlist text file

    :param input: .sch or .brd file name
    :param output: text file name
    :param timeout: int
    :param showgui: Bool, True -> do not hide eagle GUI
    :rtype: None
    '''parse partlist text delivered by eagle.

    header is converted to lowercase

    :param str: input string
    :rtype: tuple of header list and dict list: (['part','value',..], [{'part':'C1', 'value':'1n'}, ..])
    '''export partlist by eagle, then return it

    :param input: .sch or .brd file name
    :param timeout: int
    :param showgui: Bool, True -> do not hide eagle GUI
    :rtype: string
    '''export partlist by eagle, then parse it

    :param input: .sch or .brd file name
    :param timeout: int
    :param showgui: Bool, True -> do not hide eagle GUI
    :rtype: tuple of header list and dict list: (['part','value',..], [{'part':'C1', 'value':'1n'}, ..])
    '''print partlist text delivered by eagle

    :param input: .sch or .brd file name
    :param timeout: int
    :param showgui: Bool, True -> do not hide eagle GUI
    :rtype: None

    Parameters
    ----------
    viblist : List of numbers or string of list of numbers.

    Returns
    -------
    ZPE : Zero point energy in eV.

    Parameters
    ----------
    df : Pandas DataFrame.

    Returns
    -------
    labels : list of system labels.

    Parameters
    ----------
    pH : pH of bulk solution
    temperature : numeric
        temperature in K
    pressure : numeric
       pressure in mbar

    Returns
    -------
    G_H, G_OH : Gibbs free energy of proton and hydroxide.

    Parameters
    ----------
    molecule_list : list of strings
    temperature : numeric
        temperature in K
    pressure : numeric
       pressure in mbar

    Returns
    -------
    G_H, G_OH : Gibbs free energy of proton and hydroxide.

    Parameters
    ----------
    db_file : Path to database
    slab : Which metal (slab) to select.
    facet : Which facets to select.

    Returns
    -------
    data : SQL cursor output.

    Parameters
    ----------
    filename : Filename including path.

    Returns
    -------
    df : pandas data frame

    Parameters
    ----------
    db_file : Path to database
    slabs : Which metals (slabs) to select.
    facet : Which facets to select.

    Returns
    -------
    df : Data frame.

    Parameters
    ----------
    df : Data frame.

    Returns
    -------
    reaction_list : List of unique elementary reactions.

        Parameters
        ----------
        temperature : numeric
            temperature in K
        electronic_energy : numeric
            energy in eV
        verbose : boolean
            whether to print ASE thermochemistry output

        Returns
        -------
        helmholtz_energy : numeric
            Helmholtz energy in eV

        Returns
        -------
        vibs : list of vibrations in eV
        Either provide individual contributions or net contributions. If both are given,
        only the net contributions are used.

        intermediate_list: list of basestrings
        transition_states: list of True and False
        electrochemical_steps: list of True and False
        betas = list of charge transfer coefficients
        net_corrections: A sum of all contributions per intermediate.
    sdb = _current[0]
    if sdb is None or not sdb.active:
        sdb = _current[0] = Sdb()
    return sdb

def global_matches(self, text):
        matches = []
        n = len(text)
        for word in self.namespace:
            if word[:n] == text and word != "__builtins__":
                matches.append(word)
        return matches

def dec2str(n):
    s = hex(int(n))[2:].rstrip('L')
    if len(s) % 2 != 0:
        s = '0' + s
    return hex2str(s)

def bin2str(b):
    ret = []
    for pos in range(0, len(b), 8):
        ret.append(chr(int(b[pos:pos + 8], 2)))
    return ''.join(ret)

def n2s(n):
    s = hex(n)[2:].rstrip("L")
    if len(s) % 2 != 0:
        s = "0" + s
    return s.decode("hex")

def s2b(s):
    ret = []
    for c in s:
        ret.append(bin(ord(c))[2:].zfill(8))
    return "".join(ret)

def long_to_bytes(n, blocksize=0):
    # after much testing, this algorithm was deemed to be the fastest
    s = b''
    n = int(n)
    pack = struct.pack
    while n > 0:
        s = pack('>I', n & 0xffffffff) + s
        n = n >> 32
    # strip off leading zeros
    for i in range(len(s)):
        if s[i] != b'\000'[0]:
            break
    else:
        # only happens when n == 0
        s = b'\000'
        i = 0
    s = s[i:]
    # add back some pad bytes.  this could be done more efficiently w.r.t. the
    # de-padding being done above, but sigh...
    if blocksize > 0 and len(s) % blocksize:
        s = (blocksize - len(s) % blocksize) * b'\000' + s
    return s

def make_request(endpoint, **kwargs):
        endpoint = os.path.join(self._config.get('napps', 'api'), 'napps', '')
        res = self.make_request(endpoint)

        if res.status_code != 200:
            msg = 'Error getting NApps from server (%s) - %s'
            LOG.error(msg, res.status_code, res.reason)
            sys.exit(1)

        return json.loads(res.content.decode('utf-8'))['napps']

def get_napp(self, username, name):

        Args:
            napp (list): NApp list to be reload.
        Raises:
            requests.HTTPError: When there's a server error.

        endpoint = os.path.join(self._config.get('napps', 'api'), 'napps', '')
        metadata['token'] = self._config.get('auth', 'token')
        request = self.make_request(endpoint, json=metadata, package=package,
                                    method="POST")
        if request.status_code != 201:
            KytosConfig().clear_token()
            LOG.error("%s: %s", request.status_code, request.reason)
            sys.exit(1)

        # WARNING: this will change in future versions, when 'author' will get
        # removed.
        username = metadata.get('username', metadata.get('author'))
        name = metadata.get('name')

        print("SUCCESS: NApp {}/{} uploaded.".format(username, name))

def register(self, user_dict):
        endpoint = os.path.join(self._config.get('napps', 'api'), 'users', '')
        res = self.make_request(endpoint, method='POST', json=user_dict)

        return res.content.decode('utf-8')

def on_message(self, message):
        if message.address != self._address:
            return
        if isinstance(message, velbus.ChannelNamePart1Message) or isinstance(message, velbus.ChannelNamePart1Message2):
            self._process_channel_name_message(1, message)
        elif isinstance(message, velbus.ChannelNamePart2Message) or isinstance(message, velbus.ChannelNamePart2Message2):
            self._process_channel_name_message(2, message)
        elif isinstance(message, velbus.ChannelNamePart3Message) or isinstance(message, velbus.ChannelNamePart3Message2):
            self._process_channel_name_message(3, message)
        elif isinstance(message, velbus.ModuleTypeMessage):
            self._process_module_type_message(message)
        else:
            self._on_message(message)

def load(self, callback):
        if callback is None:

            def callb():
        Check if all name messages have been received
        self._config['applyCss'] = self.applyCss
        self._json['config'] = self._config
        return self._json

def _delete_stale(self):
        for name, hash_ in self._stale_files.items():
            path = self.download_root.joinpath(name)
            if not path.exists():
                continue
            current_hash = self._path_hash(path)
            if current_hash == hash_:
                progress_logger.info('deleting: %s which is stale...', name)
                path.unlink()
                self._stale_deleted += 1
                while True:
                    path = path.parent
                    if path == self.download_root or list(path.iterdir()):
                        break
                    progress_logger.info('deleting: %s which is stale..', path.relative_to(self.download_root))
                    path.rmdir()
            else:

                progress_logger.error('Not deleting "%s" which is in the lock file but not the definition '
                                      'file, however appears to have been modified since it was downloaded. '
                                      'Please check and delete the file manually.', name)
                raise GrablibError('stale file modified')

def _file_path(self, src_path, dest, regex):
        m = re.search(regex, src_path)
        if dest.endswith('/') or dest == '':
            dest += '{filename}'
        names = m.groupdict()
        if not names and m.groups():
            names = {'filename': m.groups()[-1]}
        for name, value in names.items():
            dest = dest.replace('{%s}' % name, value)
        # remove starting slash so path can't be absolute
        dest = dest.strip(' /')
        if not dest:
            progress_logger.error('destination path must not resolve to be null')
            raise GrablibError('bad path')
        new_path = self.download_root.joinpath(dest)
        new_path.relative_to(self.download_root)
        return new_path

def _lock(self, url: str, name: str, hash_: str):
        self._new_lock.append({
            'url': url,
            'name': name,
            'hash': hash_,
        })
        self._stale_files.pop(name, None)

def setup_smtp(self, host, port, user, passwd, recipients, **kwargs):
        self._smtp = kwargs
        self._smtp.update({'host': host, 'port': port, 'user': user, 'passwd': passwd, 'recipients': recipients})
        try:
            self._smtp['timeout'] = int(kwargs.get('timeout', SMTP_DEFAULT_TIMEOUT))
        except Exception as e:
            logging.error(e)
            self._smtp['timeout'] = None
        self._smtp['from'] = kwargs.get('from', user)

def enable(self):
        if not CrashReporter.active:
            CrashReporter.active = True
            # Store this function so we can set it back if the CrashReporter is deactivated
            self._excepthook = sys.excepthook
            sys.excepthook = self.exception_handler
            self.logger.info('CrashReporter: Enabled')
            if self.report_dir:
                if os.path.exists(self.report_dir):
                    if self.get_offline_reports():
                        # First attempt to send the reports, if that fails then start the watcher
                        self.submit_offline_reports()
                        remaining_reports = len(self.get_offline_reports())
                        if remaining_reports and self.watcher_enabled:
                            self.start_watcher()
                else:
                    os.makedirs(self.report_dir)

def disable(self):
        if CrashReporter.active:
            CrashReporter.active = False
            # Restore the original excepthook
            sys.excepthook = self._excepthook
            self.stop_watcher()
            self.logger.info('CrashReporter: Disabled')

def start_watcher(self):
        if self._watcher and self._watcher.is_alive:
            self._watcher_running = True
        else:
            self.logger.info('CrashReporter: Starting watcher.')
            self._watcher = Thread(target=self._watcher_thread, name='offline_reporter')
            self._watcher.setDaemon(True)
            self._watcher_running = True
            self._watcher.start()

def stop_watcher(self):
        if self._watcher:
            self._watcher_running = False
            self.logger.info('CrashReporter: Stopping watcher.')

def subject(self):
        if self.application_name and self.application_version:
            return 'Crash Report - {name} (v{version})'.format(name=self.application_name,
                                                               version=self.application_version)
        else:
            return 'Crash Report'

def store_report(self, payload):
        offline_reports = self.get_offline_reports()
        if offline_reports:
            # Increment the name of all existing reports 1 --> 2, 2 --> 3 etc.
            for ii, report in enumerate(reversed(offline_reports)):
                rpath, ext = os.path.splitext(report)
                n = int(re.findall('(\d+)', rpath)[-1])
                new_name = os.path.join(self.report_dir, self._report_name % (n + 1)) + ext
                shutil.copy2(report, new_name)
            os.remove(report)
            # Delete the oldest report
            if len(offline_reports) >= self.offline_report_limit:
                oldest = glob.glob(os.path.join(self.report_dir, self._report_name % (self.offline_report_limit+1) + '*'))[0]
                os.remove(oldest)
        new_report_path = os.path.join(self.report_dir, self._report_name % 1 + '.json')
        # Write a new report
        with open(new_report_path, 'w') as _f:
            json.dump(payload, _f)

        return new_report_path

def _watcher_thread(self):
        while 1:
            time.sleep(self.check_interval)
            if not self._watcher_running:
                break
            self.logger.info('CrashReporter: Attempting to send offline reports.')
            self.submit_offline_reports()
            remaining_reports = len(self.get_offline_reports())
            if remaining_reports == 0:
                break
        self._watcher = None
        self.logger.info('CrashReporter: Watcher stopped.')

def colorize(style, msg, resp):
    code = resp.status.split(maxsplit=1)[0]
    if code[0] == '2':
        # Put 2XX first, since it should be the common case
        msg = style.HTTP_SUCCESS(msg)
    elif code[0] == '1':
        msg = style.HTTP_INFO(msg)
    elif code == '304':
        msg = style.HTTP_NOT_MODIFIED(msg)
    elif code[0] == '3':
        msg = style.HTTP_REDIRECT(msg)
    elif code == '404':
        msg = style.HTTP_NOT_FOUND(msg)
    elif code[0] == '4':
        msg = style.HTTP_BAD_REQUEST(msg)
    else:
        # Any 5XX, or any other response
        msg = style.HTTP_SERVER_ERROR(msg)
    return msg

def access(self, resp, req, environ, request_time):
        if not (self.cfg.accesslog or self.cfg.logconfig or self.cfg.syslog):
            return

        msg = self.make_access_message(resp, req, environ, request_time)
        try:
            self.access_log.info(msg)
        except:
            self.error(traceback.format_exc())

def update(cls, args):
        mgr = NAppsManager()

        if args['all']:
            napps = mgr.get_enabled()
        else:
            napps = args['<napp>']

        for napp in napps:
            mgr.set_napp(*napp)
            LOG.info('NApp %s:', mgr.napp_id)
            cls.disable_napp(mgr)

def disable_napp(mgr):
        mgr = NAppsManager()

        if args['all']:
            napps = mgr.get_disabled()
        else:
            napps = args['<napp>']

        cls.enable_napps(napps)

def enable_napp(cls, mgr):

        Args:
            napps (list): List of NApps.

        For local installations, do not delete code outside install_path and
        enabled_path.

        This method is recursive, it will install each napps and your
        dependencies.

        Raises:
            KytosException: If a NApp hasn't been found.

        safe_shell_pat = re.escape(args['<pattern>']).replace(r'\*', '.*')
        pat_str = '.*{}.*'.format(safe_shell_pat)
        pattern = re.compile(pat_str, re.IGNORECASE)
        remote_json = NAppsManager.search(pattern)
        remote = set()
        for napp in remote_json:
            # WARNING: This will be changed in future versions, when 'author'
            # will be removed.
            username = napp.get('username', napp.get('author'))
            remote.add(((username, napp.get('name')), napp.get('description')))

        cls._print_napps(remote)

def _print_napps(cls, napp_list):
        mgr = NAppsManager()

        # Add status
        napps = [napp + ('[ie]',) for napp in mgr.get_enabled()]
        napps += [napp + ('[i-]',) for napp in mgr.get_disabled()]

        # Sort, add description and reorder columns
        napps.sort()
        napps_ordered = []
        for user, name, status in napps:
            description = mgr.get_description(user, name)
            version = mgr.get_version(user, name)
            napp_id = f'{user}/{name}'
            if version:
                napp_id += f':{version}'

            napps_ordered.append((status, napp_id, description))

        cls.print_napps(napps_ordered)

def print_napps(napps):
        mgr = NAppsManager()
        for napp in args['<napp>']:
            mgr.set_napp(*napp)
            LOG.info('Deleting NApp %s from server...', mgr.napp_id)
            try:
                mgr.delete()
                LOG.info('  Deleted.')
            except requests.HTTPError as exception:
                if exception.response.status_code == 405:
                    LOG.error('Delete Napp is not allowed yet.')
                else:
                    msg = json.loads(exception.response.content)
                    LOG.error('  Server error: %s - ', msg['error'])

def reload(cls, args):

        This method makes 2 API calls: 1) to retrieve the referenced model, and 2) to retrieve the instances of
        that model.

        :return: the :class:`Part`'s that can be referenced as a :class:`~pykechain.model.PartSet`.
        :raises APIError: When unable to load and provide the choices

        Example
        -------

        >>> property = project.part('Bike').property('RefTest')
        >>> reference_part_choices = property.choices()

    Function that generates the OpenSignalsTools Notebooks File Hierarchy programatically.

    ----------
    Parameters
    ----------
    root : None or str
        The file path where the OpenSignalsTools Environment will be stored.

    update : bool
        If True the old files will be replaced by the new ones.

    clone : bool
        If True then all the available Notebooks will be stored in the users computer.
        If False only the folder hierarchy of OpenSignalsTools will be generated, giving to the
        user a blank template for creating his own Notebook Environment.

    Returns
    -------
    out : str
        The root file path of OpenSignalsTools Environment is returned.
    Internal function that is used for generation of the page where notebooks are organized by
    tag values.

    ----------
    Parameters
    ----------
    notebook_object : notebook object
        Object of "notebook" class where the body will be created.

    dict_by_tag : dict
        Dictionary where each key is a tag and the respective value will be a list containing the
        Notebooks (title and filename) that include this tag.

        Class method responsible for adding a markdown cell with content 'content' to the
        Notebook object.

        ----------
        Parameters
        ----------
        content : str
            Text/HTML code/... to include in the markdown cell (triple quote for multiline text).

        tags : list
            A list of tags to include in the markdown cell metadata.
        Class method responsible for adding a code cell with content 'content' to the
        Notebook object.

        ----------
        Parameters
        ----------
        content : str
            Code in a string format to include in the cell (triple quote for multiline
            text).

        tags : list
            A list of tags to include in the code cell metadata.

        Returns:
            bool: True or False if the Tracking Shield is displayed.

    and their magnitude values.

    Parameters
    ----------
    s: array-like
      the input signal.
    fmax: int
      the sampling frequency.
    doplot: boolean
      a variable to indicate whether the plot is done or not.

    Returns
    -------
    f: array-like
      the frequency values (xx axis)
    fs: array-like
      the amplitude of the frequency values (yy axis)

        By default this method will raise a `ValueError` if the model is not of
        expected type.

        Args:

            model (Model) : The instance to be type checked

            raise_error (bool) : Flag to specify whether to raise error on
                type check failure

        Raises:

            ValueError: If `model` is not an instance of the respective Model
                class
        Use this to filter the kwargs passed to `new`, `create`,
        `build` methods.

        Args:

            **kwargs: a dictionary of parameters

        Args:
            **kwargs  :  Arbitrary keyword arguments. Column names are
                keywords and their new values are the values.

        Examples:

            >>> customer.update(email="newemail@x.com", name="new")

        two special keyword arguments `limit` and `reverse` for limiting
        the results and reversing the order respectively.

        Args:

            **kwargs: filter parameters

        Examples:

            >>> user = User.filter_by(email="new@x.com")

            >>> shipments = Shipment.filter_by(country="India", limit=3, reverse=True)

        filter criterion and kwargs.

        Examples:

            >>> User.count()
            500

            >>> User.count(country="India")
            300

            >>> User.count(User.age > 50, country="India")
            39


        transaction.

        Args:

            model: The instance to add.

        Examples:

            >>> customer = Customer.new(name="hari", email="hari@gmail.com")

            >>> Customer.add(customer)
            hari@gmail.com
        to the db in one get_or_404.

        Args:

            models (list): A list of the instances to add.
            commit (bool, optional): Defaults to True. If False, the
                transaction won't get committed.
            check_type (bool, optional) :  If True, each instance
                is type checked and exception is thrown if it is

                not an instance of the model. By default, False.

        Returns:
            list: A list of `Model` instances

        for the attribute `key`.

        Args:

            keyval: The value of the attribute.


            key (str, optional):  The attribute to search by. By default,
                it is 'id'.

        Returns:

            A model instance if found. Else None.

        Examples:

            >>> User.get(35)
            user35@i.com

            >>> User.get('user35@i.com', key='email')
            user35@i.com



        Args:

            keyvals(list):  The list of values of the attribute.


            key (str, optional): The attribute to search by. By default, it is
                'id'.


        Returns:

            list: A list of model instances, in the same order as the list of
                keyvals.


        Examples:


            >>> User.get_all([2,5,7, 8000, 11])
            user2@i.com, user5@i.com, user7@i.com, None, user11@i.com

            >>> User.get_all(['user35@i.com', 'user5@i.com'], key='email')
            user35@i.com, user5@i.com

        the transaction.

        Args:

            **kwargs: The keyword arguments for the init constructor.

        Examples:

            >>> user = User.create(name="Vicky", email="vicky@h.com")
            >>> user.id
            35
        kwargs. If yes, returns that instance. If not, creates a new
        instance with kwargs and returns it

        Args:

            **kwargs: The keyword arguments which are used for filtering
                and initialization.
            keys(list, optional): A special keyword argument.
                If passed, only the set of keys mentioned here will be used
                for filtering. Useful when we want to 'find' based on a subset
                of the keys and create with all the keys

        Examples:

            >>> customer = Customer.find_or_create(
            ...     name="vicky", email="vicky@h.com", country="India")
            >>> customer.id
            45
            >>> customer1 = Customer.find_or_create(
            ...     name="vicky", email="vicky@h.com", country="India")
            >>> customer1==customer
            True
            >>> customer2 = Customer.find_or_create(
            ...     name="vicky", email="vicky@h.com", country="Russia")
            >>> customer2==customer
            False
            >>> customer3 = Customer.find_or_create(
            ...      name="vicky", email="vicky@h.com", country="Russia",
            ...      keys=['name', 'email'])
            >>> customer3==customer
            True
        kwargs. If yes, updates the instance with new kwargs and
        returns that instance. If not, creates a new
        instance with kwargs and returns it.

        Args:

            **kwargs: The keyword arguments which are used for filtering
                and initialization.

            keys (list, optional): A special keyword argument. If passed,
                only the set of keys mentioned here will be used for filtering.
                Useful when we want to 'filter' based on a subset of the keys
                and create with all the keys.

        Examples:

            >>> customer = Customer.update_or_create(
            ...     name="vicky", email="vicky@h.com", country="India")
            >>> customer.id
            45
            >>> customer1 = Customer.update_or_create(
            ...     name="vicky", email="vicky@h.com", country="India")
            >>> customer1==customer
            True
            >>> customer2 = Customer.update_or_create(
            ...     name="vicky", email="vicky@h.com", country="Russia")
            >>> customer2==customer
            False
            >>> customer3 = Customer.update_or_create(
            ...      name="vicky", email="vicky@h.com", country="Russia",
            ...      keys=['name', 'email'])
            >>> customer3==customer
            True

        Args:
            list_of_kwargs(list of dicts): hereA list of dicts where
                each dict denotes the keyword args that you would pass
                to the create method separately

        Examples:

            >>> Customer.create_all([
            ... {'name': 'Vicky', 'age': 34, 'user_id': 1},
            ... {'name': 'Ron', 'age': 40, 'user_id': 1, 'gender': 'Male'}])
        creating them if required

        Args:
            list_of_kwargs(list of dicts): A list of dicts where
                each dict denotes the keyword args that you would pass
                to the create method separately

            keys (list, optional): A list of keys to use for the 
                initial finding step. Matching is done only on these
                attributes.

        Examples:

            >>> Customer.find_or_create_all([
            ... {'name': 'Vicky', 'email': 'vicky@x.com', 'age': 34},
            ... {'name': 'Ron', 'age': 40, 'email': 'ron@x.com',
            ... 'gender': 'Male'}], keys=['name', 'email'])
        creating them if required

        Args:
            list_of_kwargs(list of dicts): A list of dicts where
                each dict denotes the keyword args that you would pass
                to the create method separately

            keys (list, optional): A list of keys to use for the 
                initial finding step. Matching is done only on these
                attributes.

        Examples:

            >>> Customer.update_or_create_all([
            ... {'name': 'Vicky', 'email': 'vicky@x.com', 'age': 34},
            ... {'name': 'Ron', 'age': 40, 'email': 'ron@x.com',
            ... 'gender': 'Male'}], keys=['name', 'email'])

        Args:

            **kwargs : The keyword arguments for the constructor

        Returns:

            A model instance which has been added to db session. But session
            transaction has not been committed yet.
        returns a new, saved instance of the service's model class.

        Args:
            **kwargs: instance parameters

        Args:
            *criterion: SQLAlchemy query criterion for filtering what
                instances to update
            **kwargs: The parameters to be updated

        Examples:

            >>> User.update_all(active=True)

            >>> Customer.update_all(Customer.country=='India', active=True)

        The second example sets active=True for all customers with
        country India.
    Returns two arrays

    function [maxtab, mintab]=peakdelta(v, delta, x)
    %PEAKDET Detect peaks in a vector
    %        [MAXTAB, MINTAB] = peakdelta(V, DELTA) finds the local
    %        maxima and minima ("peaks") in the vector V.
    %        MAXTAB and MINTAB consists of two columns. Column 1
    %        contains indices in V, and column 2 the found values.
    %      
    %        With [MAXTAB, MINTAB] = peakdelta(V, DELTA, X) the indices
    %        in MAXTAB and MINTAB are replaced with the corresponding
    %        X-values.
    %
    %        A point is considered a maximum peak if it has the maximal
    %        value, and was preceded (to the left) by a value lower by
    %        DELTA.
    %        Eli Billauer, 3.4.05 (Explicitly not copyrighted).

    %        This function is released to the public domain; Any use is allowed.
        Callback to execute on status of update of channel
    Create and return a temporary directory which you can use as a context manager.

    When you are out of the context the temprorary disk gets erased.

    .. versionadded:: 2.3

    :param cwd: path to change working directory back to path when out of context
    :type cwd: basestring or None
    :return: in context a temporary directory

    Example
    -------

    >>> with temp_chdir() as temp_dir:
    >>>     # do things here
    >>>     print(temp_dir)  # etc etc
    ...
    >>> # when moving out of the context the temp_dir is destroyed
    >>> pass

    Convert datetime string to datetime object.

    Helper function to convert a datetime string found in json responses to a datetime object with timezone information.
    The server is storing all datetime strings as UTC (ZULU time). This function supports time zone offsets. When
    the input contains one, the output uses a timezone with a fixed offset from UTC.

    Inspired on the Django project. From `django.utils.dateparse.parse_datetime`. The code is copyrighted and
    licences with an MIT license in the following fashion::

        Copyright (c) Django Software Foundation and individual contributors.
        All rights reserved.

    ..versionadded 2.5:

    :param value: datetime string
    :type value: str or None
    :return: datetime of the value is well formatted. Otherwise (including if value is None) returns None
    :rtype: datetime or None
    :raises ValueError: if the value is well formatted but not a valid datetime
        if isinstance(offset, timedelta):
            offset = offset.seconds // 60
        sign = '-' if offset < 0 else '+'
        hhmm = '%02d%02d' % divmod(abs(offset), 60)
        name = sign + hhmm
        return pytz.FixedOffset(offset, name)

    DATETIME_RE = re.compile(
        r'(?P<year>\d{4})-(?P<month>\d{1,2})-(?P<day>\d{1,2})'
        r'[T ](?P<hour>\d{1,2}):(?P<minute>\d{1,2})'
        r'(?::(?P<second>\d{1,2})(?:\.(?P<microsecond>\d{1,6})\d{0,6})?)?'
        r'(?P<tzinfo>Z|[+-]\d{2}(?::?\d{2})?)?$'
    )

    match = DATETIME_RE.match(value)
    if match:
        kw = match.groupdict()
        if kw['microsecond']:
            kw['microsecond'] = kw['microsecond'].ljust(6, '0')
        tzinfo = kw.pop('tzinfo')
        if tzinfo == 'Z':
            tzinfo = pytz.UTC
        elif tzinfo is not None:
            offset_mins = int(tzinfo[-2:]) if len(tzinfo) > 3 else 0
            offset = 60 * int(tzinfo[1:3]) + offset_mins
            if tzinfo[0] == '-':
                offset = -offset
            tzinfo = _get_fixed_timezone(offset)
        kw = {k: int(v) for k, v in six.iteritems(kw) if v is not None}
        kw['tzinfo'] = tzinfo
        return datetime(**kw)

def _save_customization(self, widgets):
        if len(widgets) > 0:
            # Get the current customization and only replace the 'ext' part of it
            customization = self.activity._json_data.get('customization', dict())
            if customization:
                customization['ext'] = dict(widgets=widgets)
            else:
                customization = dict(ext=dict(widgets=widgets))

        # Empty the customization if if the widgets list is empty
        else:
            customization = None

        # perform validation
        if customization:
            validate(customization, widgetconfig_json_schema)

        # Save to the activity and store the saved activity to self
        response = self._client._request("PUT",
                                         self._client._build_url("activity", activity_id=str(self.activity.id)),
                                         json=dict(customization=customization))
        if response.status_code != requests.codes.ok:  # pragma: no cover
            raise APIError("Could not save customization ({})".format(response))
        else:
            # refresh the activity json
            self.activity = self._client.activity(pk=self.activity.id)

def _add_widget(self, widget):
        widgets = self.widgets()
        widgets += [widget]
        self._save_customization(widgets)

def widgets(self):
        customization = self.activity._json_data.get('customization')

        if customization and "ext" in customization.keys():
            return customization['ext']['widgets']
        else:
            return []

def delete_widget(self, index):
        widgets = self.widgets()
        if len(widgets) == 0:
            raise ValueError("This customization has no widgets")
        widgets.pop(index)
        self._save_customization(widgets)

def add_json_widget(self, config):
        validate(config, component_jsonwidget_schema)
        self._add_widget(dict(config=config, name=WidgetNames.JSONWIDGET))

def add_property_grid_widget(self, part_instance, max_height=None, custom_title=False, show_headers=True,
                                 show_columns=None):
        height = max_height
        # Check whether the parent_part_instance is uuid type or class `Part`
        if isinstance(part_instance, Part):
            part_instance_id = part_instance.id
        elif isinstance(part_instance, text_type) and is_uuid(part_instance):
            part_instance_id = part_instance
            part_instance = self._client.part(id=part_instance_id)
        else:
            raise IllegalArgumentError("When using the add_property_grid_widget, part_instance must be a "
                                       "Part or Part id. Type is: {}".format(type(part_instance)))
        if not show_columns:
            show_columns = list()

        # Set the display_columns for the config
        possible_columns = [ShowColumnTypes.DESCRIPTION, ShowColumnTypes.UNIT]
        display_columns = dict()

        for possible_column in possible_columns:
            if possible_column in show_columns:
                display_columns[possible_column] = True
            else:
                display_columns[possible_column] = False

        # Declare property grid config
        config = {
            "xtype": ComponentXType.PROPERTYGRID,
            "category": Category.INSTANCE,
            "filter": {
                "activity_id": str(self.activity.id),
                "part": part_instance_id
            },
            "hideHeaders": not show_headers,
            "viewModel": {
                "data": {
                    "displayColumns": display_columns
                }
            },
        }

        # Add max height and custom title
        if height:
            config['height'] = height

        if custom_title is False:
            show_title_value = "Default"
            title = part_instance.name
        elif custom_title is None:
            show_title_value = "No title"
            title = str()
        else:
            show_title_value = "Custom title"
            title = str(custom_title)

        config["title"] = title
        config["showTitleValue"] = show_title_value

        # Declare the meta info for the property grid
        meta = {
            "activityId": str(self.activity.id),
            "customHeight": height if height else None,
            "customTitle": title,
            "partInstanceId": part_instance_id,
            "showColumns": show_columns,
            "showHeaders": show_headers,
            "showHeightValue": "Set height" if height else "Automatic height",
            "showTitleValue": show_title_value
        }

        self._add_widget(dict(config=config, meta=meta, name=WidgetNames.PROPERTYGRIDWIDGET))

def add_text_widget(self, text=None, custom_title=None, collapsible=True, collapsed=False):
        # Declare text widget config
        config = {
            "xtype": ComponentXType.HTMLPANEL,
            "filter": {
                "activity_id": str(self.activity.id),
            }
        }

        # Add text and custom title
        if text:
            config['html'] = text
        if custom_title:
            show_title_value = "Custom title"
            title = custom_title
        else:
            show_title_value = "No title"
            title = None
        config['collapsible'] = collapsible
        # A widget can only be collapsed if it is collapsible in the first place
        if collapsible:
            config['collapsed'] = collapsed
        else:
            config['collapsed'] = False
        config['title'] = title
        # Declare the meta info for the property grid
        meta = {
            "activityId": str(self.activity.id),
            "customTitle": title,
            "collapsible": collapsible,
            "collapsed": collapsed,
            "html": text,
            "showTitleValue": show_title_value
        }

        self._add_widget(dict(config=config, meta=meta, name=WidgetNames.HTMLWIDGET))

def enable_mp_crash_reporting():
    global mp_crash_reporting_enabled
    multiprocessing.Process = multiprocessing.process.Process = CrashReportingProcess
    mp_crash_reporting_enabled = True

def feed(self, data):
        self.buffer += data
        while len(self.buffer) >= 6:
            self.next_packet()

def valid_header_waiting(self):
        if len(self.buffer) < 4:
            self.logger.debug("Buffer does not yet contain full header")
            result = False
        else:
            result = True
            result = result and self.buffer[0] == velbus.START_BYTE
            if not result:
                self.logger.warning("Start byte not recognized")
            result = result and (self.buffer[1] in velbus.PRIORITY)
            if not result:
                self.logger.warning("Priority not recognized")
            result = result and (self.buffer[3] & 0x0F <= 8)
            if not result:
                self.logger.warning("Message size not recognized")
        self.logger.debug("Valid Header Waiting: %s(%s)", result, str(self.buffer))
        return result

def valid_body_waiting(self):
        # 0f f8 be 04 00 08 00 00 2f 04
        packet_size = velbus.MINIMUM_MESSAGE_SIZE + \
            (self.buffer[3] & 0x0F)
        if len(self.buffer) < packet_size:
            self.logger.debug("Buffer does not yet contain full message")
            result = False
        else:
            result = True
            result = result and self.buffer[packet_size - 1] == velbus.END_BYTE
            if not result:
                self.logger.warning("End byte not recognized")
            result = result and velbus.checksum(
                self.buffer[0:packet_size - 2])[0] == self.buffer[packet_size - 2]
            if not result:
                self.logger.warning("Checksum not recognized")
        self.logger.debug("Valid Body Waiting: %s (%s)", result, str(self.buffer))
        return result

def next_packet(self):
        try:
            start_byte_index = self.buffer.index(velbus.START_BYTE)
        except ValueError:
            self.buffer = bytes([])
            return
        if start_byte_index >= 0:
            self.buffer = self.buffer[start_byte_index:]
        if self.valid_header_waiting() and self.valid_body_waiting():
            next_packet = self.extract_packet()
            self.buffer = self.buffer[len(next_packet):]
            message = self.parse(next_packet)
            if isinstance(message, velbus.Message):
                self.controller.new_message(message)

def extract_packet(self):
        packet_size = velbus.MINIMUM_MESSAGE_SIZE + \
            (self.buffer[3] & 0x0F)
        packet = self.buffer[0:packet_size]
        return packet

def _get_number_from_fmt(fmt):
    if '%' in fmt:
        # its datetime
        return len(("{0:" + fmt + "}").format(dt.datetime.now()))
    else:
        # its something else
        fmt = fmt.lstrip('0')
        return int(re.search('[0-9]+', fmt).group(0))

def get_convert_dict(fmt):
    # finally try some data, create some random data for the fmt.
    data = {}
    # keep track of how many "free_size" (wildcard) parameters we have
    # if we get two in a row then we know the pattern is invalid, meaning
    # we'll never be able to match the second wildcard field
    free_size_start = False
    for literal_text, field_name, format_spec, conversion in formatter.parse(fmt):
        if literal_text:
            free_size_start = False

        if not field_name:
            free_size_start = False
            continue

        # encapsulating free size keys,
        # e.g. {:s}{:s} or {:s}{:4s}{:d}
        if not format_spec or format_spec == "s" or format_spec == "d":
            if free_size_start:
                return None
            else:
                free_size_start = True

        # make some data for this key and format
        if format_spec and '%' in format_spec:
            # some datetime
            t = dt.datetime.now()
            # run once through format to limit precision
            t = parse(
                "{t:" + format_spec + "}", compose("{t:" + format_spec + "}", {'t': t}))['t']
            data[field_name] = t
        elif format_spec and 'd' in format_spec:
            # random number (with n sign. figures)
            if not format_spec.isalpha():
                n = _get_number_from_fmt(format_spec)
            else:
                # clearly bad
                return None
            data[field_name] = random.randint(0, 99999999999999999) % (10 ** n)
        else:
            # string type
            if format_spec is None:
                n = 4
            elif format_spec.isalnum():
                n = _get_number_from_fmt(format_spec)
            else:
                n = 4
            randstri = ''
            for x in range(n):
                randstri += random.choice(string.ascii_letters)
            data[field_name] = randstri
    return data

def is_one2one(fmt):
    data = _generate_data_for_format(fmt)
    if data is None:
        return False

    # run data forward once and back to data
    stri = compose(fmt, data)
    data2 = parse(fmt, stri)
    # check if data2 equal to original data
    if len(data) != len(data2):
        return False
    for key in data:
        if key not in data2:
            return False
        if data2[key] != data[key]:
            return False
    # all checks passed, so just return True
    return True

def convert_field(self, value, conversion):

        Similar to `re.escape` but allows '%' to pass through.

        # NOTE: remove escaped backslashes so regex matches
        regex_match = fmt_spec_regex.match(format_spec.replace('\\', ''))
        if regex_match is None:
            raise ValueError("Invalid format specification: '{}'".format(format_spec))
        regex_dict = regex_match.groupdict()
        fill = regex_dict['fill']
        ftype = regex_dict['type']
        width = regex_dict['width']
        align = regex_dict['align']
        # NOTE: does not properly handle `=` alignment
        if fill is None:
            if width is not None and width[0] == '0':
                fill = '0'
            elif ftype in ['s', 'd']:
                fill = ' '

        char_type = spec_regexes[ftype]
        if ftype == 's' and align and align.endswith('='):
            raise ValueError("Invalid format specification: '{}'".format(format_spec))
        final_regex = char_type
        if ftype in allow_multiple and (not width or width == '0'):
            final_regex += r'*'
        elif width and width != '0':
            if not fill:
                # we know we have exactly this many characters
                final_regex += r'{{{}}}'.format(int(width))
            elif fill:
                # we don't know how many fill characters we have compared to
                # field characters so just match all characters and sort it out
                # later during type conversion.
                final_regex = r'.{{{}}}'.format(int(width))
            elif ftype in allow_multiple:
                final_regex += r'*'

        return r'(?P<{}>{})'.format(field_name, final_regex)

def feed_parser(self, data):
        assert isinstance(data, bytes)
        self.parser.feed(data)

def scan(self, callback=None):

        def scan_finished():
            time.sleep(3)
            logging.info('Scan finished')
            self._nb_of_modules_loaded = 0


            def module_loaded():
                self._nb_of_modules_loaded += 1
                if self._nb_of_modules_loaded >= len(self._modules):
                    callback()
            for module in self._modules:
                self._modules[module].load(module_loaded)
        for address in range(0, 256):
            message = velbus.ModuleTypeRequestMessage(address)
            if address == 255:
                self.send(message, scan_finished)
            else:
                self.send(message)

def sync_clock(self):
        self.send(velbus.SetRealtimeClock())
        self.send(velbus.SetDate())
        self.send(velbus.SetDaylightSaving())

def string_variable_lookup(tb, s):

    refs = []
    dot_refs = s.split('.')
    DOT_LOOKUP = 0
    DICT_LOOKUP = 1
    for ii, ref in enumerate(dot_refs):
        dict_refs = dict_lookup_regex.findall(ref)
        if dict_refs:
            bracket = ref.index('[')
            refs.append((DOT_LOOKUP, ref[:bracket]))
            refs.extend([(DICT_LOOKUP, t) for t in dict_refs])
        else:
            refs.append((DOT_LOOKUP, ref))

    scope = tb.tb_frame.f_locals.get(refs[0][1], ValueError)
    if scope is ValueError:
        return scope
    for lookup, ref in refs[1:]:
        try:
            if lookup == DOT_LOOKUP:
                scope = getattr(scope, ref, ValueError)
            else:
                scope = scope.get(ref, ValueError)
        except Exception as e:
            logging.error(e)
            scope = ValueError

        if scope is ValueError:
            return scope
        elif isinstance(scope, (FunctionType, MethodType, ModuleType, BuiltinMethodType, BuiltinFunctionType)):
            return ValueError
    return scope

def get_object_references(tb, source, max_string_length=1000):
    global obj_ref_regex
    referenced_attr = set()
    for line in source.split('\n'):
        referenced_attr.update(set(re.findall(obj_ref_regex, line)))
    referenced_attr = sorted(referenced_attr)
    info = []
    for attr in referenced_attr:
        v = string_variable_lookup(tb, attr)
        if v is not ValueError:
            ref_string = format_reference(v, max_string_length=max_string_length)
            info.append((attr, ref_string))
    return info

def get_local_references(tb, max_string_length=1000):
    if 'self' in tb.tb_frame.f_locals:
        _locals = [('self', repr(tb.tb_frame.f_locals['self']))]
    else:
        _locals = []
    for k, v in tb.tb_frame.f_locals.iteritems():
        if k == 'self':
            continue
        try:
            vstr = format_reference(v, max_string_length=max_string_length)
            _locals.append((k, vstr))
        except TypeError:
            pass
    return _locals

def analyze_traceback(tb, inspection_level=None, limit=None):
    info = []
    tb_level = tb
    extracted_tb = traceback.extract_tb(tb, limit=limit)
    for ii, (filepath, line, module, code) in enumerate(extracted_tb):
        func_source, func_lineno = inspect.getsourcelines(tb_level.tb_frame)

        d = {"File": filepath,
             "Error Line Number": line,
             "Module": module,
             "Error Line": code,
             "Module Line Number": func_lineno,
             "Custom Inspection": {},
             "Source Code": ''}
        if inspection_level is None or len(extracted_tb) - ii <= inspection_level:
            # Perform advanced inspection on the last `inspection_level` tracebacks.
            d['Source Code'] = ''.join(func_source)
            d['Local Variables'] = get_local_references(tb_level)
            d['Object Variables'] = get_object_references(tb_level, d['Source Code'])
        tb_level = getattr(tb_level, 'tb_next', None)
        info.append(d)

    return info

def log_configs(self):

        If no environment variable is found and the config section/key is

        empty, then set some default values.

        default_sections = ['global', 'auth', 'napps', 'kytos']

        for section in default_sections:
            if not config.has_section(section):
                config.add_section(section)

def save_token(self, user, token):
        # allow_no_value=True is used to keep the comments on the config file.
        new_config = ConfigParser(allow_no_value=True)

        # Parse the config file. If no config file was found, then create some

        # default sections on the config variable.
        new_config.read(self.config_file)
        self.check_sections(new_config)

        new_config.remove_option('auth', 'user')
        new_config.remove_option('auth', 'token')
        filename = os.path.expanduser(self.config_file)
        with open(filename, 'w') as out_file:
            os.chmod(filename, 0o0600)
            new_config.write(out_file)

def relocate_model(part, target_parent, name=None, include_children=True):
    if target_parent.id in get_illegal_targets(part, include={part.id}):
        raise IllegalArgumentError('cannot relocate part "{}" under target parent "{}", because the target is part of '
                                   'its descendants'.format(part.name, target_parent.name))


    # First, if the user doesn't provide the name, then just use the default "Clone - ..." name
    if not name:
        name = "CLONE - {}".format(part.name)

    # The description cannot be added when creating a model, so edit the model after creation.
    part_desc = part._json_data['description']
    moved_part_model = target_parent.add_model(name=name, multiplicity=part.multiplicity)
    if part_desc:
        moved_part_model.edit(description=str(part_desc))

    # Map the current part model id with newly created part model Object
    get_mapping_dictionary().update({part.id: moved_part_model})

    # Loop through properties and retrieve their type, description and unit
    list_of_properties_sorted_by_order = part.properties
    list_of_properties_sorted_by_order.sort(key=lambda x: x._json_data['order'])
    for prop in list_of_properties_sorted_by_order:
        prop_type = prop._json_data.get('property_type')
        desc = prop._json_data.get('description')
        unit = prop._json_data.get('unit')
        options = prop._json_data.get('options')

        # On "Part references" properties, the models referenced also need to be added
        if prop_type == PropertyType.REFERENCES_VALUE:
            referenced_part_ids = [referenced_part.id for referenced_part in prop.value]
            moved_prop = moved_part_model.add_property(name=prop.name, description=desc, property_type=prop_type,

                                                       default_value=referenced_part_ids)

        # On "Attachment" properties, attachments needs to be downloaded and re-uploaded to the new property.
        elif prop_type == PropertyType.ATTACHMENT_VALUE:
            moved_prop = moved_part_model.add_property(name=prop.name, description=desc, property_type=prop_type)
            if prop.value:
                attachment_name = prop._json_data['value'].split('/')[-1]
                with temp_chdir() as target_dir:
                    full_path = os.path.join(target_dir or os.getcwd(), attachment_name)
                    prop.save_as(filename=full_path)
                    moved_prop.upload(full_path)

        # Other properties are quite straightforward
        else:
            moved_prop = moved_part_model.add_property(name=prop.name, description=desc, property_type=prop_type,

                                                       default_value=prop.value, unit=unit, options=options)

        # Map the current property model id with newly created property model Object
        get_mapping_dictionary()[prop.id] = moved_prop

    # Now copy the sub-tree of the part
    if include_children:
        # Populate the part so multiple children retrieval is not needed
        part.populate_descendants()
        # For each part, recursively run this function
        for sub_part in part._cached_children:
            relocate_model(part=sub_part, target_parent=moved_part_model, name=sub_part.name,
                           include_children=include_children)
    return moved_part_model

def relocate_instance(part, target_parent, name=None, include_children=True):

    # First, if the user doesn't provide the name, then just use the default "Clone - ..." name
    if not name:
        name = "CLONE - {}".format(part.name)
    # Initially the model of the part needs to be recreated under the model of the target_parent. Retrieve them.
    part_model = part.model()
    target_parent_model = target_parent.model()

    # Call the move_part() function for those models.
    relocate_model(part=part_model, target_parent=target_parent_model, name=part_model.name,
                   include_children=include_children)

    # Populate the descendants of the Part (category=Instance), in order to avoid to retrieve children for every
    # level and save time. Only need it the children should be included.
    if include_children:
        part.populate_descendants()

    # This function will move the part instance under the target_parent instance, and its children if required.
    moved_instance = move_part_instance(part_instance=part, target_parent=target_parent, part_model=part_model,
                                        name=name, include_children=include_children)
    return moved_instance

def move_part_instance(part_instance, target_parent, part_model, name=None, include_children=True):
    # If no specific name has been required, then call in as Clone of the part_instance.
    if not name:
        name = part_instance.name

    # Retrieve the model of the future part to be created
    moved_model = get_mapping_dictionary()[part_model.id]

    # Now act based on multiplicity
    if moved_model.multiplicity == Multiplicity.ONE:
        # If multiplicity is 'Exactly 1', that means the instance was automatically created with the model, so just
        # retrieve it, map the original instance with the moved one and update the name and property values.
        moved_instance = moved_model.instances(parent_id=target_parent.id)[0]
        map_property_instances(part_instance, moved_instance)
        moved_instance = update_part_with_properties(part_instance, moved_instance, name=str(name))
    elif moved_model.multiplicity == Multiplicity.ONE_MANY:
        # If multiplicity is '1 or more', that means one instance has automatically been created with the model, so
        # retrieve it, map the original instance with the moved one and update the name and property values. Store
        # the model in a list, in case there are multiple instance those need to be recreated.
        if target_parent.id not in get_edited_one_many():
            moved_instance = moved_model.instances(parent_id=target_parent.id)[0]
            map_property_instances(part_instance, moved_instance)
            moved_instance = update_part_with_properties(part_instance, moved_instance, name=str(name))
            get_edited_one_many().append(target_parent.id)
        else:
            moved_instance = target_parent.add(name=part_instance.name, model=moved_model, suppress_kevents=True)
            map_property_instances(part_instance, moved_instance)
            moved_instance = update_part_with_properties(part_instance, moved_instance, name=str(name))
    else:
        # If multiplicity is '0 or more' or '0 or 1', it means no instance has been created automatically with the
        # model, so then everything must be created and then updated.
        moved_instance = target_parent.add(name=name, model=moved_model, suppress_kevents=True)
        map_property_instances(part_instance, moved_instance)
        moved_instance = update_part_with_properties(part_instance, moved_instance, name=str(name))

    # If include_children is True, then recursively call this function for every descendant. Keep the name of the
    # original sub-instance.
    if include_children:
        for sub_instance in part_instance._cached_children:
            move_part_instance(part_instance=sub_instance, target_parent=moved_instance,
                               part_model=sub_instance.model(),
                               name=sub_instance.name, include_children=True)

    return moved_instance

def update_part_with_properties(part_instance, moved_instance, name=None):
    # Instantiate and empty dictionary later used to map {property.id: property.value} in order to update the part
    # in one go
    properties_id_dict = dict()
    for prop_instance in part_instance.properties:
        # Do different magic if there is an attachment property and it has a value
        if prop_instance._json_data['property_type'] == PropertyType.ATTACHMENT_VALUE:
            moved_prop = get_mapping_dictionary()[prop_instance.id]
            if prop_instance.value:
                attachment_name = prop_instance._json_data['value'].split('/')[-1]
                with temp_chdir() as target_dir:
                    full_path = os.path.join(target_dir or os.getcwd(), attachment_name)
                    prop_instance.save_as(filename=full_path)
                    moved_prop.upload(full_path)
            else:
                moved_prop.clear()
        # For a reference value property, add the id's of the part referenced {property.id: [part1.id, part2.id, ...]},
        # if there is part referenced at all.
        elif prop_instance._json_data['property_type'] == PropertyType.REFERENCES_VALUE:
            if prop_instance.value:
                moved_prop_instance = get_mapping_dictionary()[prop_instance.id]
                properties_id_dict[moved_prop_instance.id] = [ref_part.id for ref_part in prop_instance.value]
        else:
            moved_prop_instance = get_mapping_dictionary()[prop_instance.id]
            properties_id_dict[moved_prop_instance.id] = prop_instance.value
    # Update the name and property values in one go.
    moved_instance.update(name=str(name), update_dict=properties_id_dict, bulk=True, suppress_kevents=True)
    return moved_instance

def map_property_instances(original_part, new_part):
    # Map the original part with the new one
    get_mapping_dictionary()[original_part.id] = new_part

    # Do the same for each Property of original part instance, using the 'model' id and the get_mapping_dictionary
    for prop_original in original_part.properties:
        get_mapping_dictionary()[prop_original.id] = [prop_new for prop_new in new_part.properties if
                                                      get_mapping_dictionary()[prop_original._json_data['model']].id ==
                                                      prop_new._json_data['model']][0]

def ask_question(self, field_name, pattern=NAME_PATTERN, is_required=False,
                     password=False):
        input_value = ""
        question = ("Insert the field using the pattern below:"
                    "\n{}\n{}: ".format(pattern[0], field_name))

        while not input_value:
            input_value = getpass(question) if password else input(question)

            if not (input_value or is_required):
                break

            if password:
                confirm_password = getpass('Confirm your password: ')
                if confirm_password != input_value:
                    print("Password does not match")
                    input_value = ""

            if not self.valid_attribute(input_value, pattern[1]):
                error_message = "The content must fit the pattern: {}\n"
                print(error_message.format(pattern[0]))
                input_value = ""

        return input_value

def cli(action, config_file, debug, verbose):
    if verbose is True:
        log_level = 'DEBUG'
    elif verbose is False:
        log_level = 'WARNING'
    else:
        assert verbose is None
        log_level = 'INFO'

    setup_logging(log_level)
    try:
        grab = Grab(config_file, debug=debug)
        if action in {'download', None}:
            grab.download()
        if action in {'build', None}:
            grab.build()
    except GrablibError as e:
        click.secho('Error: %s' % e, fg='red')
        sys.exit(2)

def part(self):
        part_id = self._json_data['part']

        return self._client.part(pk=part_id, category=self._json_data['category'])

def delete(self):
        # type () -> ()
        r = self._client._request('DELETE', self._client._build_url('property', property_id=self.id))

        if r.status_code != requests.codes.no_content:  # pragma: no cover
            raise APIError("Could not delete property: {} with id {}".format(self.name, self.id))

def create(cls, json, **kwargs):
        # type: (dict, **Any) -> Property
        property_type = json.get('property_type')

        if property_type == PropertyType.ATTACHMENT_VALUE:
            from .property_attachment import AttachmentProperty
            return AttachmentProperty(json, **kwargs)
        elif property_type == PropertyType.SINGLE_SELECT_VALUE:
            from .property_selectlist import SelectListProperty
            return SelectListProperty(json, **kwargs)
        elif property_type == PropertyType.REFERENCE_VALUE:
            from .property_reference import ReferenceProperty
            return ReferenceProperty(json, **kwargs)
        elif property_type == PropertyType.REFERENCES_VALUE:
            from .property_multi_reference import MultiReferenceProperty
            return MultiReferenceProperty(json, **kwargs)
        else:
            return Property(json, **kwargs)

def __parse_validators(self):
        if hasattr(self, '_validators'):
            validators_json = []
            for validator in self._validators:
                if isinstance(validator, PropertyValidator):
                    validators_json.append(validator.as_json())
                else:
                    raise APIError("validator is not a PropertyValidator: '{}'".format(validator))
            if self._options.get('validators', list()) == validators_json:
                # no change
                pass
            else:
                new_options = self._options.copy()  # make a copy
                new_options.update({'validators': validators_json})
                validate(new_options, options_json_schema)
                self._options = new_options

def is_valid(self):
        # type: () -> Union[bool, None]
        if not hasattr(self, '_validators'):
            return None
        else:
            self.validate(reason=False)
            if all([vr is None for vr in self._validation_results]):
                return None
            else:
                return all(self._validation_results)

def get_ini_config(config=os.path.join(os.path.expanduser('~'), '.zdeskcfg'),

        default_section=None, section=None):

    plac_ini.call(__placeholder__, config=config, default_section=default_section)
    return __placeholder__.getconfig(section)

def _ecg_band_pass_filter(data, sample_rate):
    nyquist_sample_rate = sample_rate / 2.
    normalized_cut_offs = [5/nyquist_sample_rate, 15/nyquist_sample_rate]
    b_coeff, a_coeff = butter(2, normalized_cut_offs, btype='bandpass')[:2]
    return filtfilt(b_coeff, a_coeff, data, padlen=150)

def _integration(data, sample_rate):
    wind_size = int(0.080 * sample_rate)
    int_ecg = numpy.zeros_like(data)
    cum_sum = data.cumsum()
    int_ecg[wind_size:] = (cum_sum[wind_size:] - cum_sum[:-wind_size]) / wind_size
    int_ecg[:wind_size] = cum_sum[:wind_size] / numpy.arange(1, wind_size + 1)

    return int_ecg

def _buffer_ini(data, sample_rate):
    rr_buffer = [1] * 8
    spk1 = max(data[sample_rate:2*sample_rate])
    npk1 = 0
    threshold = _buffer_update(npk1, spk1)

    return rr_buffer, spk1, npk1, threshold

def _detects_peaks(ecg_integrated, sample_rate):

    # Minimum RR interval = 200 ms
    min_rr = (sample_rate / 1000) * 200

    # Computes all possible peaks and their amplitudes
    possible_peaks = [i for i in range(0, len(ecg_integrated)-1)
                      if ecg_integrated[i-1] < ecg_integrated[i] and
                      ecg_integrated[i] > ecg_integrated[i+1]]

    possible_amplitudes = [ecg_integrated[k] for k in possible_peaks]
    chosen_peaks = []

    # Starts with first peak
    if not possible_peaks:
        raise Exception("No Peaks Detected.")
    peak_candidate_i = possible_peaks[0]
    peak_candidate_amp = possible_amplitudes[0]
    for peak_i, peak_amp in zip(possible_peaks, possible_amplitudes):
        if peak_i - peak_candidate_i <= min_rr and peak_amp > peak_candidate_amp:
            peak_candidate_i = peak_i
            peak_candidate_amp = peak_amp
        elif peak_i - peak_candidate_i > min_rr:
            chosen_peaks += [peak_candidate_i - 6]  # Delay of 6 samples
            peak_candidate_i = peak_i
            peak_candidate_amp = peak_amp
        else:
            pass

    return chosen_peaks, possible_peaks

def _checkup(peaks, ecg_integrated, sample_rate, rr_buffer, spk1, npk1, threshold):
    peaks_amp = [ecg_integrated[peak] for peak in peaks]

    definitive_peaks = []
    for i, peak in enumerate(peaks):
        amp = peaks_amp[i]

        # accept if larger than threshold and slope in raw signal
        # is +-30% of previous slopes
        if amp > threshold:

            definitive_peaks, spk1, rr_buffer = _acceptpeak(peak, amp, definitive_peaks, spk1,
                                                            rr_buffer)

        # accept as qrs if higher than half threshold,
        # but is 360 ms after last qrs and next peak
        # is more than 1.5 rr intervals away
        # just abandon it if there is no peak before
        # or after

        elif amp > threshold / 2 and list(definitive_peaks) and len(peaks) > i + 1:
            mean_rr = numpy.mean(rr_buffer)

            last_qrs_ms = (peak - definitive_peaks[-1]) * (1000 / sample_rate)

            last_qrs_to_next_peak = peaks[i+1] - definitive_peaks[-1]

            if last_qrs_ms > 360 and last_qrs_to_next_peak > 1.5 * mean_rr:

                definitive_peaks, spk1, rr_buffer = _acceptpeak(peak, amp, definitive_peaks, spk1,
                                                                rr_buffer)
            else:
                npk1 = _noisepeak(amp, npk1)
        # if not either of these it is noise
        else:
            npk1 = _noisepeak(amp, npk1)
        threshold = _buffer_update(npk1, spk1)


    definitive_peaks = numpy.array(definitive_peaks)


    return definitive_peaks

def _acceptpeak(peak, amp, definitive_peaks, spk1, rr_buffer):


    definitive_peaks_out = definitive_peaks

    definitive_peaks_out = numpy.append(definitive_peaks_out, peak)
    spk1 = 0.125 * amp + 0.875 * spk1  # spk1 is the running estimate of the signal peak

    if len(definitive_peaks_out) > 1:
        rr_buffer.pop(0)

        rr_buffer += [definitive_peaks_out[-1] - definitive_peaks_out[-2]]


    return numpy.array(definitive_peaks_out), spk1, rr_buffer

def tachogram(data, sample_rate, signal=False, in_seconds=False, out_seconds=False):

    if signal is False:  # data is a list of R peaks position.
        data_copy = data
        time_axis = numpy.array(data)#.cumsum()
        if out_seconds is True and in_seconds is False:
            time_axis = time_axis / sample_rate
    else:  # data is a ECG signal.
        # Detection of R peaks.
        data_copy = detect_r_peaks(data, sample_rate, time_units=out_seconds, volts=False,
                                   resolution=None, plot_result=False)[0]
        time_axis = data_copy

    # Generation of Tachogram.
    tachogram_data = numpy.diff(time_axis)
    tachogram_time = time_axis[1:]

    return tachogram_data, tachogram_time

def stop(self):
        assert isinstance(data, bytes)
        self.controller.feed_parser(data)

def send(self, message, callback=None):
        while True:
            (message, callback) = self._write_queue.get(block=True)
            self.logger.info("Sending message on USB bus: %s", str(message))
            self.logger.debug("Sending binary message:  %s", str(message.to_binary()))
            self._reader.write(message.to_binary())
            time.sleep(self.SLEEP_TIME)
            if callback:
                callback()

def __require_kytos_config(self):
        if self.__enabled is None:
            uri = self._kytos_api + 'api/kytos/core/config/'
            try:
                options = json.loads(urllib.request.urlopen(uri).read())
            except urllib.error.URLError:
                print('Kytos is not running.')
                sys.exit()
            self.__enabled = Path(options.get('napps'))
            self.__installed = Path(options.get('installed_napps'))

def set_napp(self, user, napp, version=None):
        self.user = user
        self.napp = napp
        self.version = version or 'latest'

def dependencies(self, user=None, napp=None):
        napps = self._get_napp_key('napp_dependencies', user, napp)
        return [tuple(napp.split('/')) for napp in napps]

def _get_napp_key(self, key, user=None, napp=None):
        if user is None:
            user = self.user
        if napp is None:
            napp = self.napp
        kytos_json = self._installed / user / napp / 'kytos.json'
        try:
            with kytos_json.open() as file_descriptor:
                meta = json.load(file_descriptor)
                return meta[key]
        except (FileNotFoundError, json.JSONDecodeError, KeyError):
            return ''

def disable(self):

        Raises:
            FileNotFoundError: If NApp is not installed.
            PermissionError: No filesystem permission to enable NApp.

        if self.is_installed():
            installed = self.installed_dir()
            if installed.is_symlink():
                installed.unlink()
            else:
                shutil.rmtree(str(installed))

def render_template(templates_path, template_filename, context):

        Args:
            pattern (str): Python regular expression.
            # WARNING: This will change for future versions, when 'author' will
            # be removed.
            username = napp.get('username', napp.get('author'))

            strings = ['{}/{}'.format(username, napp.get('name')),
                       napp.get('description')] + napp.get('tags')
            return any(pattern.match(string) for string in strings)

        napps = NAppsClient().get_napps()
        return [napp for napp in napps if match(napp)]

def install_local(self):
        folder = self._get_local_folder()
        installed = self.installed_dir()
        self._check_module(installed.parent)
        installed.symlink_to(folder.resolve())

def _get_local_folder(self, root=None):
        if root is None:
            root = Path()
        for folders in ['.'], [self.user, self.napp]:
            kytos_json = root / Path(*folders) / 'kytos.json'
            if kytos_json.exists():
                with kytos_json.open() as file_descriptor:
                    meta = json.load(file_descriptor)
                    # WARNING: This will change in future versions, when
                    # 'author' will be removed.
                    username = meta.get('username', meta.get('author'))
                    if username == self.user and meta.get('name') == self.napp:
                        return kytos_json.parent
        raise FileNotFoundError('kytos.json not found.')

def install_remote(self):

        Return:
            str: Downloaded temp filename.

        Raises:
            urllib.error.HTTPError: If download is not successful.


        Return:
            pathlib.Path: Temp dir with package contents.


        This will create, on the current folder, a clean structure of a NAPP,
        filling some contents on this structure.

        os.makedirs(os.path.join(username, napp_name))

        #: Creating the other files based on the templates
        templates = os.listdir(templates_path)
        templates.remove('ui')
        templates.remove('openapi.yml.template')

        if meta_package:
            templates.remove('main.py.template')
            templates.remove('settings.py.template')

        for tmp in templates:
            fname = os.path.join(username, napp_name,
                                 tmp.rsplit('.template')[0])
            with open(fname, 'w') as file:
                content = cls.render_template(templates_path, tmp, context)
                file.write(content)

        if not meta_package:
            NAppsManager.create_ui_structure(username, napp_name,
                                             ui_templates_path, context)

        print()
        print(f'Congratulations! Your NApp has been bootstrapped!\nNow you '
              'can go to the directory {username}/{napp_name} and begin to '
              'code your NApp.')
        print('Have fun!')

def create_ui_structure(cls, username, napp_name, ui_templates_path,
                            context):

        Args:
            napp_identifier (str): Identifier formatted as
                <username>/<napp_name>

        Return:
            file_payload (binary): The binary representation of the napp
                package that will be POSTed to the napp server.

        json_filename = kwargs.get('json_filename', 'kytos.json')
        readme_filename = kwargs.get('readme_filename', 'README.rst')
        ignore_json = kwargs.get('ignore_json', False)
        metadata = {}

        if not ignore_json:
            try:
                with open(json_filename) as json_file:
                    metadata = json.load(json_file)
            except FileNotFoundError:
                print("ERROR: Could not access kytos.json file.")
                sys.exit(1)

        try:
            with open(readme_filename) as readme_file:
                metadata['readme'] = readme_file.read()
        except FileNotFoundError:
            metadata['readme'] = ''

        try:
            yaml = YAML(typ='safe')
            openapi_dict = yaml.load(Path('openapi.yml').open())
            openapi = json.dumps(openapi_dict)
        except FileNotFoundError:
            openapi = ''
        metadata['OpenAPI_Spec'] = openapi

        return metadata

def upload(self, *args, **kwargs):
        self.prepare()
        metadata = self.create_metadata(*args, **kwargs)
        package = self.build_napp_package(metadata.get('name'))

        NAppsClient().upload_napp(metadata, package)

def prepare(cls):

        Args:
            napps (list): NApp list to be reloaded.
        Raises:
            requests.HTTPError: When there's a server error.

    lhs, rhs = splitext(image.url)
    lhs += THUMB_EXT
    thumb_url = f'{lhs}{rhs}'
    return thumb_url

def from_env(cls, env_filename=None):
        # type: (Optional[str]) -> Client
        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            env.read_envfile(env_filename)
        client = cls(url=env(KechainEnv.KECHAIN_URL))

        if env(KechainEnv.KECHAIN_TOKEN, None):
            client.login(token=env(KechainEnv.KECHAIN_TOKEN))
        elif env(KechainEnv.KECHAIN_USERNAME, None) and env(KechainEnv.KECHAIN_PASSWORD, None):
            client.login(username=env(KechainEnv.KECHAIN_USERNAME), password=env(KechainEnv.KECHAIN_PASSWORD))

        return client

def _build_url(self, resource, **kwargs):
        # type: (str, **str) -> str
        Retrieve user objects of the entire administration.

        :return: list of dictionary with users information
        :rtype: list(dict)
        -------

        self.last_request = None
        self.last_response = self.session.request(method, url, auth=self.auth, headers=self.headers, **kwargs)
        self.last_request = self.last_response.request
        self.last_url = self.last_response.url

        if self.last_response.status_code == requests.codes.forbidden:
            raise ForbiddenError(self.last_response.json()['results'][0]['detail'])

        return self.last_response

def app_versions(self):

        Checks if a KE-chain app matches a version comparison. Uses the `semver` matcher to check.

        `match("2.0.0", ">=1.0.0")` => `True`
        `match("1.0.0", ">1.0.0")` => `False`

        Examples
        --------
        >>> client.match_app_version(label='wim', version=">=1.99")
        >>> True

        >>> client.match_app_version(app='kechain2.core.pim', version=">=1.0.0")
        >>> True

        :param app: (optional) appname eg. 'kechain.core.wim'
        :type app: basestring or None
        :param label: (optional) app label (last part of the app name) eb 'wim'
        :type label: basestring or None
        :param version: semantic version string to match appname version against eg '2.0.0' or '>=2.0.0'
        :type version: basestring

        :param default: (optional) boolean to return if the version of the app is not set but the app found.
                        Set to None to return a NotFoundError when a version if not found in the app.

        :type default: bool or None
        :return: True if the version of the app matches against the match_version, otherwise False
        :raises IllegalArgumentError: if no app nor a label is provided
        :raises NotFoundError: if the app is not found
        :raises ValueError: if the version provided is not parseable by semver,
                            should contain (<operand><major>.<minor>.<patch) where <operand> is '>,<,>=,<=,=='


        :param obj: object to reload
        :type obj: :py:obj:`obj`
        :param extra_params: additional object specific extra query string params (eg for activity)
        :type extra_params: dict
        :return: a new object
        :raises NotFoundError: if original object is not found or deleted in the mean time

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :return: a single :class:`models.Scope`
        :raises NotFoundError: When no `Scope` is found
        :raises MultipleFoundError: When more than a single `Scope` is found

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param pk: id (primary key) of the activity to retrieve
        :type pk: basestring or None
        :param name: filter the activities by name
        :type name: basestring or None
        :param scope: filter by scope id
        :type scope: basestring or None
        :return: list of :class:`models.Activity`
        :raises NotFoundError: If no `Activities` are found

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param pk: id (primary key) of the activity to retrieve
        :type pk: basestring or None
        :param name: filter the activities by name
        :type name: basestring or None
        :param scope: filter by scope id
        :type scope: basestring or None
        :return: a single :class:`models.Activity`
        :raises NotFoundError: When no `Activity` is found
        :raises MultipleFoundError: When more than a single `Activity` is found

        If no parameters are provided, all parts are retrieved.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param name: filter on name
        :type name: basestring or None
        :param pk: filter on primary key
        :type pk: basestring or None
        :param model: filter on model_id
        :type model: basestring or None
        :param category: filter on category (INSTANCE, MODEL, None)
        :type category: basestring or None
        :param bucket: filter on bucket_id
        :type bucket: basestring or None
        :param parent: filter on the parent_id, returns all childrent of the parent_id
        :type parent: basestring or None
        :param activity: filter on activity_id
        :type activity: basestring or None

        :param limit: limit the return to # items (default unlimited, so return all results)
        :type limit: int or None

        :param batch: limit the batch size to # items (defaults to 100 items per batch)
        :type batch: int or None
        :param kwargs: additional `keyword=value` arguments for the api
        :type kwargs: dict or None
        :return: :class:`models.PartSet` which is an iterator of :class:`models.Part`
        :raises NotFoundError: If no `Part` is found

        Examples
        --------

        Return all parts (defaults to instances) with exact name 'Gears'.


        >>> client = Client(url='https://default.localhost:9443', verify=False)
        >>> client.login('admin','pass')
        >>> client.parts(name='Gears')  # doctest:Ellipsis
        ...

        Return all parts with category is MODEL or category is INSTANCE.

        >>> client.parts(name='Gears', category=None)  # doctest:Ellipsis
        ...

        Return a maximum of 5 parts

        >>> client.parts(limit=5)  # doctest:Ellipsis
        ...


        Uses the same interface as the :func:`parts` method but returns only a single pykechain :class:`models.Part`
        instance.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :return: a single :class:`models.Part`
        :raises NotFoundError: When no `Part` is found
        :raises MultipleFoundError: When more than a single `Part` is found

        Uses the same interface as the :func:`part` method but returns only a single pykechain
        :class:`models.Part` instance of category `MODEL`.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :return: a single :class:`models.Part`
        :raises NotFoundError: When no `Part` is found
        :raises MultipleFoundError: When more than a single `Part` is found

        Uses the same interface as the :func:`properties` method but returns only a single pykechain :class:
        `models.Property` instance.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :return: a single :class:`models.Property`
        :raises NotFoundError: When no `Property` is found
        :raises MultipleFoundError: When more than a single `Property` is found

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param name: name to limit the search for.
        :type name: basestring or None
        :param pk: primary key or id (UUID) of the property to search for
        :type pk: basestring or None
        :param category: filter the properties by category. Defaults to INSTANCE. Other options MODEL or None
        :type category: basestring or None
        :param kwargs: (optional) additional search keyword arguments
        :type kwargs: dict or None
        :return: list of :class:`models.Property`
        :raises NotFoundError: When no `Property` is found
        Retrieve Services.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param name: (optional) name to limit the search for
        :type name: basestring or None
        :param pk: (optional) primary key or id (UUID) of the service to search for
        :type pk: basestring or None
        :param scope: (optional) id (UUID) of the scope to search in
        :type scope: basestring or None
        :param kwargs: (optional) additional search keyword arguments
        :type kwargs: dict or None
        :return: list of :class:`models.Service` objects
        :raises NotFoundError: When no `Service` objects are found
        Retrieve single KE-chain Service.

        Uses the same interface as the :func:`services` method but returns only a single pykechain
        :class:`models.Service` instance.

        :param name: (optional) name to limit the search for
        :type name: basestring or None
        :param pk: (optional) primary key or id (UUID) of the service to search for
        :type pk: basestring or None
        :param scope: (optional) id (UUID) of the scope to search in
        :type scope: basestring or None
        :param kwargs: (optional) additional search keyword arguments
        :type kwargs: dict or None
        :return: a single :class:`models.Service` object
        :raises NotFoundError: When no `Service` object is found
        :raises MultipleFoundError: When more than a single `Service` object is found
        Retrieve Service Executions.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param name: (optional) name to limit the search for
        :type name: basestring or None
        :param pk: (optional) primary key or id (UUID) of the service to search for
        :type pk: basestring or None
        :param scope: (optional) id (UUID) of the scope to search in
        :type scope: basestring or None
        :param service: (optional) service UUID to filter on
        :type service: basestring or None
        :param kwargs: (optional) additional search keyword arguments
        :type kwargs: dict or None
        :return: a single :class:`models.ServiceExecution` object
        :raises NotFoundError: When no `ServiceExecution` object is found
        Retrieve single KE-chain ServiceExecution.

        Uses the same interface as the :func:`service_executions` method but returns only a single
        pykechain :class:`models.ServiceExecution` instance.

        If additional `keyword=value` arguments are provided, these are added to the request parameters. Please
        refer to the documentation of the KE-chain API for additional query parameters.

        :param name: (optional) name to limit the search for
        :type name: basestring or None
        :param pk: (optional) primary key or id (UUID) of the service to search for
        :type pk: basestring or None
        :param scope: (optional) id (UUID) of the scope to search in
        :type scope: basestring or None
        :param kwargs: (optional) additional search keyword arguments
        :type kwargs: dict or None
        :return: a single :class:`models.ServiceExecution` object
        :raises NotFoundError: When no `ServiceExecution` object is found
        :raises MultipleFoundError: When more than a single `ServiceExecution` object is found
        Users of KE-chain.

        Provide a list of :class:`User`s of KE-chain. You can filter on username or id or any other advanced filter.

        :param username: (optional) username to filter
        :type username: basestring or None
        :param pk: (optional) id of the user to filter
        :type pk: basestring or None
        :param kwargs: Additional filtering keyword=value arguments
        :type kwargs: dict or None
        :return: List of :class:`Users`
        :raises NotFoundError: when a user could not be found
        User of KE-chain.

        Provides single user of :class:`User` of KE-chain. You can filter on username or id or an advanced filter.

        :param username: (optional) username to filter
        :type username: basestring or None
        :param pk: (optional) id of the user to filter
        :type pk: basestring or None
        :param kwargs: Additional filtering keyword=value arguments
        :type kwargs: dict or None
        :return: List of :class:`User`
        :raises NotFoundError: when a user could not be found
        :raises MultipleFoundError: when more than a single user can be found
        Team of KE-chain.

        Provides a team of :class:`Team` of KE-chain. You can filter on team name or provide id.

        :param name: (optional) team name to filter
        :type name: basestring or None
        :param id: (optional) id of the user to filter
        :type id: basestring or None

        :param is_hidden: (optional) boolean to show non-hidden or hidden teams or both (None) (default is non-hidden)
        :type is_hidden: bool or None
        :param kwargs: Additional filtering keyword=value arguments
        :type kwargs: dict or None
        :return: List of :class:`Team`
        :raises NotFoundError: when a user could not be found
        :raises MultipleFoundError: when more than a single user can be found
        Teams of KE-chain.

        Provide a list of :class:`Team`s of KE-chain. You can filter on teamname or id or any other advanced filter.

        :param name: (optional) teamname to filter
        :type name: basestring or None
        :param id: (optional) id of the team to filter
        :type id: basestring or None

        :param is_hidden: (optional) boolean to show non-hidden or hidden teams or both (None) (default is non-hidden)
        :type is_hidden: bool or None
        :param kwargs: Additional filtering keyword=value arguments
        :type kwargs: dict or None
        :return: List of :class:`Teams`
        :raises NotFoundError: when a team could not be found
        # suppress_kevents should be in the data (not the query_params)
        if 'suppress_kevents' in kwargs:
            data['suppress_kevents'] = kwargs.pop('suppress_kevents')

        # prepare url query parameters
        query_params = kwargs
        query_params['select_action'] = action

        response = self._request('POST', self._build_url('parts'),
                                 params=query_params,  # {"select_action": action},
                                 data=data)

        if response.status_code != requests.codes.created:
            raise APIError("Could not create part, {}: {}".format(str(response), response.content))

        return Part(response.json()['results'][0], client=self)

def create_part(self, parent, model, name=None, **kwargs):
        if parent.category != Category.INSTANCE:
            raise IllegalArgumentError("The parent should be an category 'INSTANCE'")
        if model.category != Category.MODEL:
            raise IllegalArgumentError("The models should be of category 'MODEL'")

        if not name:
            name = model.name

        data = {
            "name": name,
            "parent": parent.id,
            "model": model.id
        }

        return self._create_part(action="new_instance", data=data, **kwargs)

def create_model(self, parent, name, multiplicity='ZERO_MANY', **kwargs):
        if parent.category != Category.MODEL:
            raise IllegalArgumentError("The parent should be of category 'MODEL'")

        data = {
            "name": name,
            "parent": parent.id,
            "multiplicity": multiplicity
        }

        return self._create_part(action="create_child_model", data=data, **kwargs)

def _create_clone(self, parent, part, **kwargs):
        if part.category == Category.MODEL:
            select_action = 'clone_model'
        else:
            select_action = 'clone_instance'

        data = {
            "part": part.id,
            "parent": parent.id,
            "suppress_kevents": kwargs.pop('suppress_kevents', None)
        }

        # prepare url query parameters
        query_params = kwargs
        query_params['select_action'] = select_action

        response = self._request('POST', self._build_url('parts'),
                                 params=query_params,
                                 data=data)

        if response.status_code != requests.codes.created:
            raise APIError("Could not clone part, {}: {}".format(str(response), response.content))

        return Part(response.json()['results'][0], client=self)

def create_property(self, model, name, description=None, property_type=PropertyType.CHAR_VALUE, default_value=None,
                        unit=None, options=None):
        if model.category != Category.MODEL:
            raise IllegalArgumentError("The model should be of category MODEL")

        if not property_type.endswith('_VALUE'):
            warnings.warn("Please use the `PropertyType` enumeration to ensure providing correct "
                          "values to the backend.", UserWarning)
            property_type = '{}_VALUE'.format(property_type.upper())

        if property_type not in PropertyType.values():
            raise IllegalArgumentError("Please provide a valid propertytype, please use one of `enums.PropertyType`. "
                                       "Got: '{}'".format(property_type))


        # because the references value only accepts a single 'model_id' in the default value, we need to convert this
        # to a single value from the list of values.
        if property_type in (PropertyType.REFERENCE_VALUE, PropertyType.REFERENCES_VALUE) and \

                isinstance(default_value, (list, tuple)) and default_value:

            default_value = default_value[0]

        data = {
            "name": name,
            "part": model.id,
            "description": description or '',
            "property_type": property_type.upper(),

            "value": default_value,
            "unit": unit or '',
            "options": options or {}
        }

        # # We add options after the fact only if they are available, otherwise the options will be set to null in the
        # # request and that can't be handled by KE-chain.
        # if options:
        #     data['options'] = options

        response = self._request('POST', self._build_url('properties'),
                                 json=data)

        if response.status_code != requests.codes.created:
            raise APIError("Could not create property")

        prop = Property.create(response.json()['results'][0], client=self)

        model.properties.append(prop)

        return prop

def create_service(self, name, scope, description=None, version=None,
                       service_type=ServiceType.PYTHON_SCRIPT,
                       environment_version=ServiceEnvironmentVersion.PYTHON_3_5,
                       pkg_path=None):
        if service_type not in ServiceType.values():
            raise IllegalArgumentError("The type should be of one of {}".format(ServiceType.values()))

        if environment_version not in ServiceEnvironmentVersion.values():
            raise IllegalArgumentError("The environment version should be of one of {}".
                                       format(ServiceEnvironmentVersion.values()))

        data = {
            "name": name,
            "scope": scope,
            "description": description,
            "script_type": service_type,
            "script_version": version,
            "env_version": environment_version,
        }

        response = self._request('POST', self._build_url('services'),
                                 data=data)

        if response.status_code != requests.codes.created:  # pragma: no cover
            raise APIError("Could not create service ({})".format((response, response.json())))

        service = Service(response.json().get('results')[0], client=self)

        if pkg_path:
            # upload the package
            service.upload(pkg_path)

        # refresh service contents in place
        service.refresh()

        return service

def delete_scope(self, scope):
        assert isinstance(scope, Scope), 'Scope "{}" is not a scope!'.format(scope.name)

        response = self._request('DELETE', self._build_url('scope', scope_id=str(scope.id)))

        if response.status_code != requests.codes.no_content:  # pragma: no cover
            raise APIError("Could not delete scope, {}: {}".format(str(response), response.content))

def set_sort(self, request):

        # Look for 'sort' in get request. If not available use default.

        sort_request = request.GET.get(self.sort_parameter, self.default_sort)
        if sort_request.startswith('-'):
            sort_order = '-'
            sort_field = sort_request.split('-')[1]
        else:
            sort_order = ''
            sort_field = sort_request
        # Invalid sort requests fail silently
        if not sort_field in self._allowed_sort_fields:

            sort_order = self.default_sort_order

            sort_field = self.default_sort_field
        return (sort_order, sort_field)

def get_next_sort_string(self, field):
        # self.sort_field is the currect sort field
        if field == self.sort_field:
            next_sort = self.toggle_sort_order() + field
        else:

            default_order_for_field = \

                self._allowed_sort_fields[field]['default_direction']

            next_sort = default_order_for_field + field
        return self.get_sort_string(next_sort)

def get_sort_indicator(self, field):
        indicator = ''
        if field == self.sort_field:
            indicator = 'sort-asc'
            if self.sort_order == '-':
                indicator = 'sort-desc'
        return indicator

def get_basic_sort_link(self, request, field):
        query_string = self.get_querystring()
        sort_string = self.get_next_sort_string(field)
        if sort_string:
            sort_link = request.path + '?' + sort_string
            if query_string:
                sort_link += '&' + query_string
        else:
            sort_link = request.path
            if query_string:
                sort_link += '?' + query_string
        return sort_link

def build_thumb_path(self, image):
        image_file = image.file
        image_name_w_ext = split(image.name)[-1]
        image_name, ext = splitext(image_name_w_ext)
        if not self.in_memory(image_file):
            # `image_file` is already in disk (not in memory).
            # `image_name` is the full path, not just the name
            image_name = image_name.split('/')[-1]
        upload_to = image.field.upload_to
        if not upload_to.endswith('/'):
            upload_to = f'{upload_to}/'
        path_upload_to = f'{upload_to}{image_name}'
        return f'{self.storage.location}/{path_upload_to}{THUMB_EXT}{ext}'

def run(self, **options):
        shutdown_message = options.get('shutdown_message', '')

        self.stdout.write("Performing system checks...\n\n")
        self.check(display_num_errors=True)
        self.check_migrations()
        now = datetime.datetime.now().strftime(r'%B %d, %Y - %X')
        if six.PY2:
            now = now.decode(get_system_encoding())
        self.stdout.write(now)

        addr, port = self.addr, self.port
        addr = '[{}]'.format(addr) if self._raw_ipv6 else addr

        runner = GunicornRunner(addr, port, options)
        try:
            runner.run()
        except KeyboardInterrupt:
            runner.shutdown()
            if shutdown_message:
                self.stdout.write(shutdown_message)
            sys.exit(0)
        except:
            runner.shutdown()
            raise

def _plot(x, mph, mpd, threshold, edge, valley, ax, ind):
        if 'assignees' in self._json_data and self._json_data.get('assignees_ids') == list():
            return []
        elif 'assignees' in self._json_data and self._json_data.get('assignees_ids'):
            assignees_ids_str = ','.join([str(id) for id in self._json_data.get('assignees_ids')])
            return self._client.users(id__in=assignees_ids_str, is_hidden=False)
        return None

def is_rootlevel(self):
        # when the activity itself is a root, than return False immediately
        if self.is_root():
            return False

        parent_name = None
        parent_dict = self._json_data.get('parent_id_name')

        if parent_dict and 'name' in parent_dict:
            parent_name = parent_dict.get('name')
        if not parent_dict:
            parent_name = self._client.activity(id=self._json_data.get('parent_id')).name
        if parent_name in ActivityRootNames.values():
            return True
        return False

def parent(self):
        parent_id = self._json_data.get('parent_id')
        if parent_id is None:
            raise NotFoundError("Cannot find subprocess for this task '{}', "
                                "as this task exist on top level.".format(self.name))
        return self._client.activity(pk=parent_id, scope=self.scope_id)

def siblings(self, **kwargs):
        parent_id = self._json_data.get('parent_id')
        if parent_id is None:
            raise NotFoundError("Cannot find subprocess for this task '{}', "
                                "as this task exist on top level.".format(self.name))
        return self._client.activities(parent_id=parent_id, scope=self.scope_id, **kwargs)

def download_as_pdf(self, target_dir=None, pdf_filename=None, paper_size=PaperSize.A4,
                        paper_orientation=PaperOrientation.PORTRAIT, include_appendices=False):
        if not pdf_filename:
            pdf_filename = self.name + '.pdf'
        if not pdf_filename.endswith('.pdf'):
            pdf_filename += '.pdf'

        full_path = os.path.join(target_dir or os.getcwd(), pdf_filename)

        request_params = {
            'papersize': paper_size,
            'orientation': paper_orientation,
            'appendices': include_appendices
        }

        url = self._client._build_url('activity_export', activity_id=self.id)
        response = self._client._request('GET', url, params=request_params)
        if response.status_code != requests.codes.ok:  # pragma: no cover
            raise APIError("Could not download PDF of activity {}".format(self.name))

        # If appendices are included, the request becomes asynchronous

        if include_appendices:
            data = response.json()

            # Download the pdf async
            url = urljoin(self._client.api_root, data['download_url'])

            count = 0

            while count <= ASYNC_TIMEOUT_LIMIT:
                response = self._client._request('GET', url=url)

                if response.status_code == requests.codes.ok:  # pragma: no cover
                    with open(full_path, 'wb') as f:
                        for chunk in response.iter_content(1024):
                            f.write(chunk)
                    return

                count += ASYNC_REFRESH_INTERVAL
                time.sleep(ASYNC_REFRESH_INTERVAL)

            raise APIError("Could not download PDF of activity {} within the time-out limit of {} "
                           "seconds".format(self.name, ASYNC_TIMEOUT_LIMIT))

        with open(full_path, 'wb') as f:
            for chunk in response.iter_content(1024):
                f.write(chunk)

def parse(argv):
        super().run()
        call('rm -vrf ./build ./dist ./*.egg-info', shell=True)
        call('find . -name __pycache__ -type d | xargs rm -rf', shell=True)
        call('test -d docs && make -C docs/ clean', shell=True)

def run(self):
        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            self.find_primary_button().click()

def addon_name(self):
        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            el = self.find_description()
            return el.find_element(By.CSS_SELECTOR, "b").text

def cancel(self):
        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            self.find_primary_button().click()

def _load_txt(file, devices, channels, header, **kwargs):

    # %%%%%%%%%%%%%%%%%%%%%%%%%%% Exclusion of invalid keywords %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    kwargs_txt = _filter_keywords(numpy.loadtxt, kwargs)

    # %%%%%%%%%%%%%%%%%%%%%%%%%% Columns of the selected channels %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    out_dict = {}
    for dev_nbr, device in enumerate(devices):
        out_dict[device] = {}
        columns = []
        for chn in channels[dev_nbr]:
            columns.append(header[device]["column labels"][chn])
            # header[device]["column labels"] contains the column of .txt file where the data of
            # channel "chn" is located.
            out_dict[device]["CH" + str(chn)] = numpy.loadtxt(fname=file, usecols=header[device]["column labels"][chn],
                                                              **kwargs_txt)

    return out_dict

def _load_h5(file, devices, channels):

    # %%%%%%%%%%%%%%%%%%%%%%%%%%%% Creation of h5py object %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    h5_object = h5py.File(file)

    # %%%%%%%%%%%%%%%%%%%%%%%%% Data of the selected channels %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    out_dict = {}
    for dev_nbr, device in enumerate(devices):
        out_dict[device] = {}
        for chn in channels[dev_nbr]:
            data_temp = list(h5_object.get(device).get("raw").get("channel_" + str(chn)))

            # Conversion of a nested list to a flatten list by list-comprehension
            # The following line is equivalent to:
            # for sublist in h5_data:
            #    for item in sublist:
            #        flat_list.append(item)
            #out_dict[device]["CH" + str(chn)] = [item for sublist in data_temp for item in sublist]
            out_dict[device]["CH" + str(chn)] = numpy.concatenate(data_temp)

    return out_dict

def _check_chn_type(channels, available_channels):

    # ------------------------ Definition of constants and variables -------------------------------
    chn_list_standardized = []

    # %%%%%%%%%%%%%%%%%%%%%%%%%%% Fill of "chn_list_standardized" %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    devices = list(available_channels.keys())
    for dev_nbr, device in enumerate(devices):
        if channels is not None:
            sub_unit = channels[dev_nbr]
            for channel in sub_unit:  # Each sublist must be composed by integers.
                if channel in available_channels[devices[dev_nbr]]:
                    continue
                else:
                    raise RuntimeError("At least one of the specified channels is not available in "
                                       "the acquisition file.")
            chn_list_standardized.append(sub_unit)

        else:  # By omission all the channels were selected.
            chn_list_standardized.append(available_channels[device])

    return chn_list_standardized

def _available_channels(devices, header):

    # ------------------------ Definition of constants and variables ------------------------------
    chn_dict = {}

    # %%%%%%%%%%%%%%%%%%%%%% Access to the relevant data in the header %%%%%%%%%%%%%%%%%%%%%%%%%%%%
    for dev in devices:
        chn_dict[dev] = header[dev]["column labels"].keys()

    return chn_dict

def _check_dev_type(devices, dev_list):

    if devices is not None:
        for device in devices:
            if device in dev_list:  # List element is one of the available devices.
                continue
            else:
                raise RuntimeError("At least one of the specified devices is not available in the "
                                   "acquisition file.")
        out = devices

    else:
        out = dev_list

    return out

def _file_type(file):
    # %%%%%%%%%%%%%%%%%%%%%%%%%%%%% Verification of file type %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    if "." in file:  # File with known extension.
        file_type = file.split(".")[-1]
    else:  # File without known extension.
        file_type = magic.from_file(file, mime=True).split("/")[-1]

    return file_type

def team(self):

        See :class:`pykechain.Client.parts` for available parameters.

        See :class:`pykechain.Client.part` for available parameters.

        See :class:`pykechain.Client.create_model` for available parameters.

        See :class:`pykechain.Client.model` for available parameters.

        See :class:`pykechain.Client.activities` for available parameters.

        See :class:`pykechain.Client.create_activity` for available parameters.

        See :class:`pykechain.Client.create_service` for available parameters.

        .. versionadded:: 1.13

        See :class:`pykechain.Client.service` for available parameters.

        .. versionadded:: 1.13

        See :class:`pykechain.Client.service_execution` for available parameters.

        .. versionadded:: 1.13
        Retrieve members of the scope.

        :param is_manager: (optional) set to True to return only Scope members that are also managers.
        :type is_manager: bool
        :return: List of members (usernames)

        Examples
        --------
        >>> members = project.members()
        >>> managers = project.members(is_manager=True)

        Add a single member to the scope.

        You may only edit the list of members if the pykechain credentials allow this.

        :param member: single username to be added to the scope list of members
        :type member: basestring
        :raises APIError: when unable to update the scope member
        Remove a single member to the scope.

        :param member: single username to be removed from the scope list of members
        :type member: basestring
        :raises APIError: when unable to update the scope member
        Add a single manager to the scope.

        :param manager: single username to be added to the scope list of managers
        :type manager: basestring
        :raises APIError: when unable to update the scope manager
        Remove a single manager to the scope.

        :param manager: single username to be added to the scope list of managers
        :type manager: basestring
        :raises APIError: when unable to update the scope manager
        Update the Project Team of the Scope. Updates include addition or removing of managers or members.

        :param select_action: type of action to be applied
        :type select_action: basestring
        :param user: the username of the user to which the action applies to
        :type user: basestring
        :param user_type: the type of the user (member or manager)
        :type user_type: basestring
        :raises APIError: When unable to update the scope project team.
        Clone current scope.

        See :class:`pykechain.Client.clone_scope` for available parameters.

        .. versionadded:: 2.6.0
        if self.is_platform:
            if self._data["publicCode"]:
                return self._data['name'] + " Platform " + \
                       self._data["publicCode"]
            else:
                return self._data['name'] + " Platform " + \
                       self.place_id.split(':')[-1]
        else:
            return self._data['name']

def remove(self, value, _sa_initiator=None):
    Used as a Jinja2 filter, this function returns a safe HTML chunk.

    Usage (in the HTML template):

        {{ obj.image|progressive }}

    :param django.db.models.fields.files.ImageFieldFile image_field: image
    :param str alt_text: str
    :return: a safe HTML template ready to be rendered
        form = super().get_form(form_class)
        if self._save:
            make_form_or_formset_fields_not_required(form)
        return form

def save_task(self):
        if self._save:
            self.save_task()
        else:
            super().activation_done(*args, **kwargs)

def niplot():
    fig = gcf()
    cid = fig.canvas.mpl_connect('key_press_event',  # @UnusedVariable
                                 on_key_press)
    cid = fig.canvas.mpl_connect('key_release_event',  # @UnusedVariable
                                 on_key_release)
    cid = fig.canvas.mpl_connect('scroll_event', zoom)

def acquire_subsamples_gp1(input_data, file_name=None):

    # Generation of the HTML file where the plot will be stored.
    #file_name = _generate_bokeh_file(file_name)

    # Number of acquired samples (Original sample_rate = 4000 Hz)
    fs_orig = 4000
    nbr_samples_orig = len(input_data)
    data_interp = {"4000": {}}
    data_interp["4000"]["data"] = input_data
    data_interp["4000"]["time"] = numpy.linspace(0, nbr_samples_orig / fs_orig, nbr_samples_orig)

    # Constants
    time_orig = data_interp["4000"]["time"]
    data_orig = data_interp["4000"]["data"]

    # ============ Interpolation of data accordingly to the desired sampling frequency ============
    # sample_rate in [3000, 1000, 500, 200, 100] - Some of the available sample frequencies at Plux
    # acquisition systems
    # sample_rate in [50, 20] - Non-functional sampling frequencies (Not available at Plux devices
    # because of their limited application)
    for sample_rate in [3000, 1000, 500, 200, 100, 50, 20]:
        fs_str = str(sample_rate)
        nbr_samples_interp = int((nbr_samples_orig * sample_rate) / fs_orig)
        data_interp[fs_str] = {}
        data_interp[fs_str]["time"] = numpy.linspace(0, nbr_samples_orig / fs_orig,
                                                     nbr_samples_interp)
        data_interp[fs_str]["data"] = numpy.interp(data_interp[fs_str]["time"], time_orig,
                                                   data_orig)

    # List that store the figure handler.
    list_figures = []

    # Generation of Bokeh Figures.
    for iter_nbr, sample_rate in enumerate(["4000", "3000", "1000", "500", "200", "100"]):
        # If figure number is a multiple of 3 or if we are generating the first figure...
        if iter_nbr == 0 or iter_nbr % 2 == 0:
            list_figures.append([])

        # Plotting phase.
        list_figures[-1].append(figure(x_axis_label='Time (s)', y_axis_label='Raw Data',
                                       title="Sampling Frequency: " + sample_rate + " Hz",
                                       **opensignals_kwargs("figure")))
        list_figures[-1][-1].line(data_interp[sample_rate]["time"][:int(sample_rate)],
                                  data_interp[sample_rate]["data"][:int(sample_rate)],
                                  **opensignals_kwargs("line"))

def download(link, out):

    # [Source: https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3]
    r = requests.get(link)
    with open(out, 'wb') as outfile:
        outfile.write(r.content)

def argrelmin(data, axis=0, order=1, mode='clip'):
    return argrelextrema(data, np.less, axis, order, mode)

def argrelmax(data, axis=0, order=1, mode='clip'):
    return argrelextrema(data, np.greater, axis, order, mode)

def peaks(signal, tol=None):

    if (tol is None):
        tol = min(signal)
    pks = argrelmax(clip(signal, tol, signal.max()))
    return pks[0]

def get_project(url=None, username=None, password=None, token=None, scope=None, scope_id=None,
                env_filename=None, status=ScopeStatus.ACTIVE):

    if env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False):
        if not os.getenv(kecenv.KECHAIN_URL):
            raise ClientError(
                "Error: KECHAIN_URL should be provided as environment variable (use of env vars is enforced)")
        if not (os.getenv(kecenv.KECHAIN_TOKEN) or
                (os.getenv(kecenv.KECHAIN_PASSWORD) and os.getenv(kecenv.KECHAIN_PASSWORD))):
            raise ClientError("Error: KECHAIN_TOKEN or KECHAIN_USERNAME and KECHAIN_PASSWORD should be provided as "
                              "environment variable(s) (use of env vars is enforced)")
        if not (os.getenv(kecenv.KECHAIN_SCOPE) or os.getenv(kecenv.KECHAIN_SCOPE_ID)):
            raise ClientError("Error: KECHAIN_SCOPE or KECHAIN_SCOPE_ID should be provided as environment variable "
                              "(use of env vars is enforced)")


    if env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False) or \
            not any((url, username, password, token, scope, scope_id)):
        client = Client.from_env(env_filename=env_filename)

        scope_id = env(kecenv.KECHAIN_SCOPE_ID, default=None)

        scope = env(kecenv.KECHAIN_SCOPE, default=None)

        status = env(kecenv.KECHAIN_SCOPE_STATUS, default=None)
    elif (url and ((username and password) or (token)) and (scope or scope_id)) and \

            not env.bool(kecenv.KECHAIN_FORCE_ENV_USE, default=False):
        client = Client(url=url)
        client.login(username=username, password=password, token=token)
    else:
        raise ClientError("Error: insufficient arguments to connect to KE-chain. "
                          "See documentation of `pykechain.get_project()`")

    if scope_id:
        return client.scope(pk=scope_id, status=status)
    else:
        return client.scope(name=scope, status=status)

def _rebuild_key_ids(self):
        
        Only yields the first key of duplicates.
        
        url = self._client._build_url(resource, **kwargs)
        response = self._client._request('PUT', url, json=update_dict, params=params)

        if response.status_code != requests.codes.ok:  # pragma: no cover
            raise APIError("Could not update {} ({})".format(self.__class__.__name__, response.json().get('results')))
        else:
            self.refresh()

def members(self, role=None):
        if role and role not in TeamRoles.values():
            raise IllegalArgumentError("role should be one of `TeamRoles` {}, got '{}'".format(TeamRoles.values(),
                                                                                               role))

        member_list = self._json_data.get('members')
        if role:
            return [teammember for teammember in member_list if teammember.get('role') == role]
        else:
            return member_list

def scopes(self, **kwargs):
    Insert a hash based on the content into the path after the first dot.

    hash_length 7 matches git commit short references
        return sorted((value, name) for (name, value) in cls.__dict__.items() if not name.startswith('__'))

def navbar(self):
        window = BaseWindow(self.selenium, self.selenium.current_window_handle)
        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            el = self.selenium.find_element(*self._nav_bar_locator)
            return NavBar(window, el)

def notification(self):
        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            try:
                root = self.selenium.find_element(*self._notification_locator)
                return BaseNotification.create(self, root)
            except NoSuchElementException:
                pass
            try:
                notifications = self.selenium.find_elements(
                    *self._app_menu_notification_locator
                )
                root = next(n for n in notifications if n.is_displayed())
                return BaseNotification.create(self, root)
            except StopIteration:
                pass
        return None

def wait_for_notification(self, notification_class=BaseNotification):
        if notification_class:
            if notification_class is BaseNotification:
                message = "No notification was shown."
            else:
                message = "{0} was not shown.".format(notification_class.__name__)
            self.wait.until(
                lambda _: isinstance(self.notification, notification_class),
                message=message,
            )
            return self.notification
        else:
            self.wait.until(
                lambda _: self.notification is None,
                message="Unexpected notification shown.",
            )

def open_window(self, private=False):
        handles_before = self.selenium.window_handles
        self.switch_to()

        with self.selenium.context(self.selenium.CONTEXT_CHROME):
            # Opens private or non-private window
            self.selenium.find_element(*self._file_menu_button_locator).click()
            if private:
                self.selenium.find_element(
                    *self._file_menu_private_window_locator
                ).click()
            else:
                self.selenium.find_element(
                    *self._file_menu_new_window_button_locator
                ).click()

        return self.wait.until(
            expected.new_browser_window_is_opened(self.selenium, handles_before),
            message="No new browser window opened",
        )

def to_serializable_dict(self, attrs_to_serialize=None,
                             rels_to_expand=None,
                             rels_to_serialize=None,
                             key_modifications=None):
        return self.todict(
            attrs_to_serialize=attrs_to_serialize,
            rels_to_expand=rels_to_expand, rels_to_serialize=rels_to_serialize,
            key_modifications=key_modifications)

def serialize_attrs(self, *args):
        # return dict([(a, getattr(self, a)) for a in args])
        cls = type(self)
        result = {}
        # result = {
        #     a: getattr(self, a)
        #     for a in args
        #     if hasattr(cls, a) and
        #     a not in cls.attrs_forbidden_for_serialization()
        # }
        for a in args:
            if hasattr(cls, a) and a not in cls.attrs_forbidden_for_serialization():
                val = getattr(self, a)
                if is_list_like(val):
                    result[a] = list(val)
                else:
                    result[a] = val
        return result

def fundamental_frequency(s,FS):
    # TODO: review fundamental frequency to guarantee that f0 exists 
    # suggestion peak level should be bigger 
    # TODO: explain code
    
    s = s - mean(s)
    f, fs = plotfft(s, FS, doplot=False)
    
    #fs = smooth(fs, 50.0)
  
    fs = fs[1:int(len(fs) / 2)]
    f = f[1:int(len(f) / 2)]
    
    cond = find(f > 0.5)[0]
    
    bp = bigPeaks(fs[cond:], 0)
    
    if bp==[]:
        f0=0
    else:
        
        bp = bp + cond
        
        f0 = f[min(bp)]
    
    return f0

def max_frequency (sig,FS):
    
    f, fs = plotfft(sig, FS, doplot=False)    
    t = cumsum(fs)
    
    ind_mag = find (t>t[-1]*0.95)[0]
    f_max=f[ind_mag]
    return f_max

def median_frequency(sig,FS):
    
    f, fs = plotfft(sig, FS, doplot=False)    
    t = cumsum(fs)
    
    ind_mag = find (t>t[-1]*0.50)[0]
    f_median=f[ind_mag]
    return f_median

def call(subcommand, args):

    Args:
        napp_id: String with the form 'username/napp[:version]' (version is
                  optional). If no version is found, it will be None.

    Returns:
        tuple: A tuple with (username, napp, version)

    Raises:
        KytosException: If a NApp has not the form _username/name_.

    Internal function that is used for generation of the generic notebooks header.

    ----------
    Parameters
    ----------
    notebook_object : notebook object
        Object of "notebook" class where the header will be created.

    notebook_type : str
        Notebook type: - "Main_Files_Signal_Samples"
                       - "Main_Files_By_Category"
                       - "Main_Files_By_Difficulty"
                       - "Main_Files_By_Tag"
                       - "Acquire"
                       - "Open"
                       - "Visualise"
                       - "Process"
                       - "Detect"
                       - "Extract"
                       - "Train_and_Classify"
                       - "Explain"

    notebook_title : None or str

        The Notebook title should only be defined when 'notebook_type' is:
       - "Acquire"
       - "Open"
       - "Visualise"
       - "Process"
       - "Detect"
       - "Extract"
       - "Train_and_Classify"
       - "Explain"

    tags : str
        Sequence of tags that characterize the Notebook.

    difficulty_stars : int

        This input defines the difficulty level of the Notebook instructions.

    notebook_description : str
        An introductory text to present the Notebook and involve the reader.

        url = self._base_url + path

        try:
            if method == 'GET':
                response = requests.get(url, timeout=TIMEOUT)
            elif method == "POST":
                response = requests.post(url, params, timeout=TIMEOUT)
            elif method == "PUT":
                response = requests.put(url, params, timeout=TIMEOUT)
            elif method == "DELETE":
                response = requests.delete(url, timeout=TIMEOUT)

            if response:
                return response.json()
            else:
                return {'status': 'error'}
        except requests.exceptions.HTTPError:
            return {'status': 'error'}
        except requests.exceptions.Timeout:
            return {'status': 'offline'}
        except requests.exceptions.RequestException:
            return {'status': 'offline'}

def post_worker_init(worker):
    quit_command = 'CTRL-BREAK' if sys.platform == 'win32' else 'CONTROL-C'
    sys.stdout.write(
        "Django version {djangover}, Gunicorn version {gunicornver}, "
        "using settings {settings!r}\n"
        "Starting development server at {urls}\n"
        "Quit the server with {quit_command}.\n".format(
            djangover=django.get_version(),
            gunicornver=gunicorn.__version__,
            settings=os.environ.get('DJANGO_SETTINGS_MODULE'),
            urls=', '.join('http://{0}/'.format(b) for b in worker.cfg.bind),
            quit_command=quit_command,
        ),
    )

def value(self):
        if 'value' in self._json_data and self._json_data['value']:
            return "[Attachment: {}]".format(self._json_data['value'].split('/')[-1])
        else:
            return None

def filename(self):

        When providing a :class:`matplotlib.figure.Figure` object as data, the figure is uploaded as PNG.
        For this, `matplotlib`_ should be installed.

        :param filename: File path
        :type filename: basestring
        :raises APIError: When unable to upload the file to KE-chain
        :raises OSError: When the path to the file is incorrect or file could not be found

        .. _matplotlib: https://matplotlib.org/

        :param filename: File path
        :type filename: basestring
        :raises APIError: When unable to download the data
        :raises OSError: When unable to save the data to disk
        Check if a switch is turned on

        :return: bool
        Turn on switch.

        :return: None
                pass
            callback = callb
        message = velbus.SwitchRelayOnMessage(self._address)
        message.relay_channels = [channel]
        self._controller.send(message, callback)

def turn_off(self, channel, callback=None):
        if callback is None:

            def callb():

        Makes a single API call.

        :param options_list: list of options to set.
        :raises APIError: when unable to update the options
    if isinstance(form_or_formset, BaseFormSet):
        for single_form in form_or_formset:
            make_form_fields_not_required(single_form)
    else:
        make_form_fields_not_required(form_or_formset)

def scope_id(self):
        if self.scope:
            scope_id = self.scope and self.scope.get('id')
        else:
            pseudo_self = self._client.activity(pk=self.id, fields="id,scope")
            if pseudo_self.scope and pseudo_self.scope.get('id'):
                self.scope = pseudo_self.scope
                scope_id = self.scope.get('id')
            else:
                raise NotFoundError("This activity '{}'({}) does not belong to a scope, something is weird!".
                                    format(self.name, self.id))
        return scope_id

def is_rootlevel(self):
        container_id = self._json_data.get('container')
        if container_id:
            return container_id == self._json_data.get('root_container')
        else:
            return False

def is_configured(self):
        # check configured based on if we get at least 1 part back
        associated_models = self.parts(category=Category.MODEL, limit=1)
        if associated_models:
            return True
        else:
            return False

def parts(self, *args, **kwargs):
        return self._client.parts(*args, activity=self.id, **kwargs)

def associated_parts(self, *args, **kwargs):
        return (
            self.parts(category=Category.MODEL, *args, **kwargs),
            self.parts(category=Category.INSTANCE, *args, **kwargs)
        )

def subprocess(self):
        subprocess_id = self._json_data.get('container')
        if subprocess_id == self._json_data.get('root_container'):
            raise NotFoundError("Cannot find subprocess for this task '{}', "
                                "as this task exist on top level.".format(self.name))
        return self._client.activity(pk=subprocess_id, scope=self.scope_id)

def siblings(self, **kwargs):
        container_id = self._json_data.get('container')
        return self._client.activities(container=container_id, scope=self.scope_id, **kwargs)

def create(self, *args, **kwargs):
        if self.activity_type != ActivityType.SUBPROCESS:
            raise IllegalArgumentError("One can only create a task under a subprocess.")
        return self._client.create_activity(self.id, *args, **kwargs)

def customization(self):
        from .customization import ExtCustomization

        # For now, we only allow customization in an Ext JS context
        return ExtCustomization(activity=self, client=self._client)

def all_stop_places_quays(self) -> list:
        if not self.stops:
            return

        headers = {'ET-Client-Name': self._client_name}
        request = {
            'query': GRAPHQL_STOP_TO_QUAY_TEMPLATE,
            'variables': {
                'stops': self.stops,
                'omitNonBoarding': self.omit_non_boarding
            }
        }

        with async_timeout.timeout(10):
            resp = await self.web_session.post(RESOURCE,
                                               json=request,
                                               headers=headers)

        if resp.status != 200:
            _LOGGER.error(
                "Error connecting to Entur, response http status code: %s",
                resp.status)
            return None
        result = await resp.json()

        if 'errors' in result:
            return

        for stop_place in result['data']['stopPlaces']:
            if len(stop_place['quays']) > 1:
                for quay in stop_place['quays']:
                    if quay['estimatedCalls']:
                        self.quays.append(quay['id'])

async def update(self) -> None:
        place_id = place['id']
        self.info[place_id] = Place(place, is_platform)

def serializable_list(
        olist, attrs_to_serialize=None, rels_to_expand=None,
        group_listrels_by=None, rels_to_serialize=None,
        key_modifications=None, groupby=None, keyvals_to_merge=None,
        preserve_order=False, dict_struct=None, dict_post_processors=None):
    if groupby:
        if preserve_order:
            result = json_encoder(deep_group(
                olist, keys=groupby, serializer='todict',
                preserve_order=preserve_order,
                serializer_kwargs={
                    'rels_to_serialize': rels_to_serialize,
                    'rels_to_expand': rels_to_expand,
                    'attrs_to_serialize': attrs_to_serialize,
                    'group_listrels_by': group_listrels_by,
                    'key_modifications': key_modifications,
                    'dict_struct': dict_struct,
                    'dict_post_processors': dict_post_processors
                }))
        else:
            result = deep_group(
                olist, keys=groupby, serializer='todict',
                preserve_order=preserve_order,
                serializer_kwargs={
                    'rels_to_serialize': rels_to_serialize,
                    'rels_to_expand': rels_to_expand,
                    'attrs_to_serialize': attrs_to_serialize,
                    'group_listrels_by': group_listrels_by,
                    'key_modifications': key_modifications,
                    'dict_struct': dict_struct,
                    'dict_post_processors': dict_post_processors
                })
        return result
    else:
        result_list = map(
            lambda o: serialized_obj(
                o, attrs_to_serialize=attrs_to_serialize,
                rels_to_expand=rels_to_expand,
                group_listrels_by=group_listrels_by,
                rels_to_serialize=rels_to_serialize,
                key_modifications=key_modifications,
                dict_struct=dict_struct,
                dict_post_processors=dict_post_processors),
            olist)
        if keyvals_to_merge:
            result_list = [merge(obj_dict, kvdict)
                           for obj_dict, kvdict in
                           zip(result_list, keyvals_to_merge)]
        return result_list

def jsoned(struct, wrap=True, meta=None, struct_key='result', pre_render_callback=None):
    return _json.dumps(
        structured(
            struct, wrap=wrap, meta=meta, struct_key=struct_key,
            pre_render_callback=pre_render_callback),

        default=json_encoder)

def as_list(func):
    @wraps(func)

    def wrapper(*args, **kwargs):
        response = func(*args, **kwargs)
        if isinstance(response, Response):
            return response
        return as_json_list(
            response,
            **_serializable_params(request.args, check_groupby=True))
    return wrapper

def as_processed_list(func):
    @wraps(func)

    def wrapper(*args, **kwargs):
        func_argspec = inspect.getargspec(func)
        func_args = func_argspec.args
        for kw in request.args:
            if (kw in func_args and kw not in RESTRICTED and
                    not any(request.args.get(kw).startswith(op)
                            for op in OPERATORS)
                    and not any(kw.endswith(op) for op in OPERATORS)):
                kwargs[kw] = request.args.get(kw)
        func_output = func(*args, **kwargs)

        return process_args_and_render_json_list(func_output)

    return wrapper

def as_obj(func):
    @wraps(func)

    def wrapper(*args, **kwargs):
        response = func(*args, **kwargs)
        return render_json_obj_with_requested_structure(response)
    return wrapper

def execute(self, interactive=False):
        url = self._client._build_url('service_execute', service_id=self.id)
        response = self._client._request('GET', url, params=dict(interactive=interactive, format='json'))

        if response.status_code != requests.codes.accepted:  # pragma: no cover
            raise APIError("Could not execute service '{}': {}".format(self, (response.status_code, response.json())))

        data = response.json()
        return ServiceExecution(json=data.get('results')[0], client=self._client)

def edit(self, name=None, description=None, version=None, **kwargs):
        update_dict = {'id': self.id}
        if name:
            if not isinstance(name, str):
                raise IllegalArgumentError("name should be provided as a string")
            update_dict.update({'name': name})
        if description:
            if not isinstance(description, str):
                raise IllegalArgumentError("description should be provided as a string")
            update_dict.update({'description': description})
        if version:
            if not isinstance(version, str):
                raise IllegalArgumentError("description should be provided as a string")
            update_dict.update({'script_version': version})

        if kwargs:  # pragma: no cover
            update_dict.update(**kwargs)
        response = self._client._request('PUT',
                                         self._client._build_url('service', service_id=self.id), json=update_dict)

        if response.status_code != requests.codes.ok:  # pragma: no cover
            raise APIError("Could not update Service ({})".format(response))

        if name:
            self.name = name
        if version:
            self.version = version

def delete(self):
        # type: () -> None
        response = self._client._request('DELETE', self._client._build_url('service', service_id=self.id))

        if response.status_code != requests.codes.no_content:  # pragma: no cover
            raise APIError("Could not delete service: {} with id {}".format(self.name, self.id))

def get_executions(self, **kwargs):
        return self._client.service_executions(service=self.id, scope=self.scope_id, **kwargs)

def service(self):
        Terminate the Service execution.

        .. versionadded:: 1.13

        :return: None if the termination request was successful
        :raises APIError: When the service execution could not be terminated.
        Retrieve the log of the service execution.

        .. versionadded:: 1.13

        :param target_dir: (optional) directory path name where the store the log.txt to.
        :type target_dir: basestring or None

        :param log_filename: (optional) log filename to write the log to, defaults to `log.txt`.
        :type log_filename: basestring or None
        :raises APIError: if the logfile could not be found.
        :raises OSError: if the file could not be written.
        Get the url of the notebook, if the notebook is executed in interactive mode.

        .. versionadded:: 1.13

        :return: full url to the interactive running notebook as `basestring`
        :raises APIError: when the url cannot be retrieved.

          Send websocket data frame to the client.



          If data is a unicode object then the frame is sent as Text.

          If the data is a bytearray object then the frame is sent as Binary.

    Function that equalises the input arrays by zero-padding the shortest one.

    ----------
    Parameters
    ----------
    array1: list or numpy.array
        Array
    array2: list or numpy.array
        Array

    Return
    ------
    arrays: numpy.array
        Array containing the equal-length arrays.
    This function allows to generate a text file with synchronised signals from the input file.

    ----------
    Parameters
    ----------
    in_path : str
        Path to the file containing the two signals that will be synchronised.
    channels : list
        List with the strings identifying the channels of each signal.
    new_path : str
        The path to create the new file.

        Args:
            bindings (dict): A dictionary of var names to binding strings.

        Returns:
            str: The rendered instantiation of this path template.

        Raises:
            ValidationError: If a key isn't provided or if a sub-template can't
                be parsed.

        Args:
            path (str): A fully qualified path template string.

        Returns:
            dict: Var names to matched binding values.

        Raises:
            ValidationException: If path can't be matched to the template.

        Args:
            data: A path template string.
        Returns:
            A list of _Segment.

        Args:
            window (:py:class:`BrowserWindow`): Window object this region
                appears in.
            root
                (:py:class:`~selenium.webdriver.remote.webelement.WebElement`):
                WebDriver element object that serves as the root for the
                notification.

        Returns:
            :py:class:`BaseNotification`: Firefox notification.


        Returns:
            str: The notification label


        Returns:
            str: The notification origin.

        if self.window.firefox_version >= 67:
            return self.root.find_element(
                By.CLASS_NAME, "popup-notification-primary-button")
        return self.root.find_anonymous_element_by_attribute(
            "anonid", "button")

def windows(self):
        from foxpuppet.windows import BrowserWindow

        return [
            BrowserWindow(self.selenium, handle)
            for handle in self.selenium.window_handles
        ]

def read_daemon(self):

        The validation results are returned as tuple (boolean (true/false), reasontext)
        Query the darwin webservice to obtain a board for a particular station
        and return a StationBoard instance

        Positional arguments:
        crs -- the three letter CRS code of a UK station

        Keyword arguments:

        rows -- the number of rows to retrieve (default 10)
        include_departures -- include departing services in the departure board

        (default True)
        include_arrivals -- include arriving services in the departure board

        (default False)
        destination_crs -- filter results so they only include services

        calling at a particular destination (default None)
        origin_crs -- filter results so they only include services

        originating from a particular station (default None)
        Get the details of an individual service and return a ServiceDetails
        instance.

        Positional arguments:
        service_id: A Darwin LDB service id
        self._parse_paths()
        context = dict(napp=self._napp.__dict__, paths=self._paths)
        self._save(context)

def _parse_decorated_functions(self, code):
                # @rest decorators
                (?P<decorators>
                    (?:@rest\(.+?\)\n)+  # one or more @rest decorators inside
                )
                # docstring delimited by 3 double quotes
                .+?"{3}(?P<docstring>.+?)"{3}

        summary = 'TODO write the summary.'
        description = 'TODO write/remove the description'
        if match:
            m_dict = match.groupdict()
            summary = m_dict['summary']
            if m_dict['description']:
                description = re.sub(r'(\s|\n){2,}', ' ',
                                     m_dict['description'])
        self._summary = summary
        self._description = description

def _parse_methods(cls, list_string):
        typeless = re.sub(r'<\w+?:', '<', rule)  # remove Flask types
        return typeless.replace('<', '{').replace('>', '}')

def property(self, name):
        found = None
        if is_uuid(name):
            found = find(self.properties, lambda p: name == p.id)
        else:
            found = find(self.properties, lambda p: name == p.name)

        if not found:
            raise NotFoundError("Could not find property with name or id {}".format(name))

        return found

def parent(self):
        # type: () -> Any
        if self.parent_id:
            return self._client.part(pk=self.parent_id, category=self.category)
        else:
            return None

def children(self, **kwargs):
        if not kwargs:

            # no kwargs provided is the default, we aim to cache it.
            if not self._cached_children:
                self._cached_children = list(self._client.parts(parent=self.id, category=self.category))
            return self._cached_children
        else:
            # if kwargs are provided, we assume no use of cache as specific filtering on the children is performed.
            return self._client.parts(parent=self.id, category=self.category, **kwargs)

def siblings(self, **kwargs):
        # type: (Any) -> Any
        if self.parent_id:
            return self._client.parts(parent=self.parent_id, category=self.category, **kwargs)
        else:
            from pykechain.models.partset import PartSet
            return PartSet(parts=[])

def model(self):
        if self.category == Category.INSTANCE:
            model_id = self._json_data['model'].get('id')
            return self._client.model(pk=model_id)
        else:
            raise NotFoundError("Part {} has no model".format(self.name))

def instances(self, **kwargs):
        if self.category == Category.MODEL:
            return self._client.parts(model=self, category=Category.INSTANCE, **kwargs)
        else:
            raise NotFoundError("Part {} is not a model".format(self.name))

def proxy_model(self):
        if self.category != Category.MODEL:
            raise IllegalArgumentError("Part {} is not a model, therefore it cannot have a proxy model".format(self))
        if 'proxy' in self._json_data and self._json_data.get('proxy'):
            catalog_model_id = self._json_data['proxy'].get('id')
            return self._client.model(pk=catalog_model_id)
        else:
            raise NotFoundError("Part {} is not a proxy".format(self.name))

def add(self, model, **kwargs):
        # type: (Part, **Any) -> Part
        if self.category != Category.INSTANCE:
            raise APIError("Part should be of category INSTANCE")

        return self._client.create_part(self, model, **kwargs)

def add_to(self, parent, **kwargs):
        # type: (Part, **Any) -> Part
        if self.category != Category.MODEL:
            raise APIError("Part should be of category MODEL")

        return self._client.create_part(parent, self, **kwargs)

def add_model(self, *args, **kwargs):
        # type: (*Any, **Any) -> Part
        if self.category != Category.MODEL:
            raise APIError("Part should be of category MODEL")

        return self._client.create_model(self, *args, **kwargs)

def add_property(self, *args, **kwargs):
        # type: (*Any, **Any) -> Property
        if self.category != Category.MODEL:
            raise APIError("Part should be of category MODEL")

        return self._client.create_property(self, *args, **kwargs)

def update(self, name=None, update_dict=None, bulk=True, **kwargs):
        # dict(name=name, properties=json.dumps(update_dict))) with property ids:value
        action = 'bulk_update_properties'

        request_body = dict()
        for prop_name_or_id, property_value in update_dict.items():
            if is_uuid(prop_name_or_id):
                request_body[prop_name_or_id] = property_value
            else:
                request_body[self.property(prop_name_or_id).id] = property_value

        if bulk and len(update_dict.keys()) > 1:
            if name:
                if not isinstance(name, str):
                    raise IllegalArgumentError("Name of the part should be provided as a string")
            r = self._client._request('PUT', self._client._build_url('part', part_id=self.id),
                                      data=dict(name=name, properties=json.dumps(request_body), **kwargs),
                                      params=dict(select_action=action))
            if r.status_code != requests.codes.ok:  # pragma: no cover
                raise APIError('{}: {}'.format(str(r), r.content))
        else:
            for property_name, property_value in update_dict.items():
                self.property(property_name).value = property_value

def add_with_properties(self, model, name=None, update_dict=None, bulk=True, **kwargs):
        if self.category != Category.INSTANCE:
            raise APIError("Part should be of category INSTANCE")
        name = name or model.name
        action = 'new_instance_with_properties'

        properties_update_dict = dict()
        for prop_name_or_id, property_value in update_dict.items():
            if is_uuid(prop_name_or_id):
                properties_update_dict[prop_name_or_id] = property_value
            else:
                properties_update_dict[model.property(prop_name_or_id).id] = property_value

        if bulk:
            r = self._client._request('POST', self._client._build_url('parts'),
                                      data=dict(
                                          name=name,
                                          model=model.id,
                                          parent=self.id,
                                          properties=json.dumps(properties_update_dict),
                                          **kwargs
                                      ),
                                      params=dict(select_action=action))

            if r.status_code != requests.codes.created:  # pragma: no cover
                raise APIError('{}: {}'.format(str(r), r.content))
            return Part(r.json()['results'][0], client=self._client)
        else:  # do the old way
            new_part = self.add(model, name=name)  # type: Part
            new_part.update(update_dict=update_dict, bulk=bulk)
            return new_part

def order_properties(self, property_list=None):
        if self.category != Category.MODEL:
            raise APIError("Part should be of category MODEL")
        if not isinstance(property_list, list):
            raise IllegalArgumentError('Expected a list of strings or Property() objects, got a {} object'.
                                       format(type(property_list)))

        order_dict = dict()

        for prop in property_list:
            if isinstance(prop, (str, text_type)):
                order_dict[self.property(name=prop).id] = property_list.index(prop)
            else:
                order_dict[prop.id] = property_list.index(prop)

        r = self._client._request('PUT', self._client._build_url('part', part_id=self.id),
                                  data=dict(
                                      property_order=json.dumps(order_dict)
                                  ))
        if r.status_code != requests.codes.ok:  # pragma: no cover
            raise APIError("Could not reorder properties")

def clone(self, **kwargs):
        parent = self.parent()
        return self._client._create_clone(parent, self, **kwargs)

def copy(self, target_parent, name=None, include_children=True, include_instances=True):
        if self.category == Category.MODEL and target_parent.category == Category.MODEL:
            # Cannot add a model under an instance or vice versa
            copied_model = relocate_model(part=self, target_parent=target_parent, name=name,
                                          include_children=include_children)
            if include_instances:
                instances_to_be_copied = list(self.instances())
                parent_instances = list(target_parent.instances())
                for parent_instance in parent_instances:
                    for instance in instances_to_be_copied:
                        instance.populate_descendants()
                        move_part_instance(part_instance=instance, target_parent=parent_instance,
                                           part_model=self, name=instance.name, include_children=include_children)
            return copied_model

        elif self.category == Category.INSTANCE and target_parent.category == Category.INSTANCE:
            copied_instance = relocate_instance(part=self, target_parent=target_parent, name=name,
                                                include_children=include_children)
            return copied_instance
        else:
            raise IllegalArgumentError('part "{}" and target parent "{}" must have the same category')

def move(self, target_parent, name=None, include_children=True, include_instances=True):
        if not name:
            name = self.name
        if self.category == Category.MODEL and target_parent.category == Category.MODEL:
            moved_model = relocate_model(part=self, target_parent=target_parent, name=name,
                                         include_children=include_children)
            if include_instances:
                retrieve_instances_to_copied = list(self.instances())
                retrieve_parent_instances = list(target_parent.instances())
                for parent_instance in retrieve_parent_instances:
                    for instance in retrieve_instances_to_copied:
                        instance.populate_descendants()
                        move_part_instance(part_instance=instance, target_parent=parent_instance,
                                           part_model=self, name=instance.name, include_children=include_children)
            self.delete()
            return moved_model
        elif self.category == Category.INSTANCE and target_parent.category == Category.INSTANCE:
            moved_instance = relocate_instance(part=self, target_parent=target_parent, name=name,
                                               include_children=include_children)
            try:
                self.delete()
            except APIError:
                model_of_instance = self.model()
                model_of_instance.delete()
            return moved_instance
        else:
            raise IllegalArgumentError('part "{}" and target parent "{}" must have the same category')

def _generate_notebook_by_difficulty_body(notebook_object, dict_by_difficulty):

    difficulty_keys = list(dict_by_difficulty.keys())
    difficulty_keys.sort()
    for difficulty in difficulty_keys:
        markdown_cell = STAR_TABLE_HEADER
        markdown_cell = _set_star_value(markdown_cell, int(difficulty))
        for notebook_file in dict_by_difficulty[str(difficulty)]:
            split_path = notebook_file.split("/")
            notebook_type = split_path[-2]
            notebook_name = split_path[-1].split("&")[0]
            notebook_title = split_path[-1].split("&")[1]
            markdown_cell += "\n\t<tr>\n\t\t<td width='20%' class='header_image_color_" + \
                             str(NOTEBOOK_KEYS[notebook_type]) + "'><img " \
                             "src='../../images/icons/" + notebook_type.title() +\
                             ".png' width='15%'>\n\t\t</td>"
            markdown_cell += "\n\t\t<td width='60%' class='center_cell open_cell_light'>" + \
                             notebook_title + "\n\t\t</td>"
            markdown_cell += "\n\t\t<td width='20%' class='center_cell'>\n\t\t\t<a href='" \
                             "../" + notebook_type.title() + "/" + notebook_name + \
                             "'><div class='file_icon'></div></a>\n\t\t</td>\n\t</tr>"

        markdown_cell += "</table>"

        # ==================== Insertion of HTML table in a new Notebook cell ======================
        notebook_object["cells"].append(nb.v4.new_markdown_cell(markdown_cell))

def _generate_dir_structure(path):

    # ============================ Creation of the main directory ==================================
    current_dir = (path + "\\opensignalsfactory_environment").replace("\\", "/")
    if not os.path.isdir(current_dir):
        os.makedirs(current_dir)

    # ================== Copy of 'images' 'styles' and 'signal_samples' folders ====================
    path_cloned_files = (os.path.abspath(__file__).split(os.path.basename(__file__))[0] + \
                        "\\notebook_files\\osf_files\\").replace("\\", "/")
    for var in ["images", "styles", "signal_samples"]:
        if os.path.isdir((current_dir + "\\" + var).replace("\\", "/")):
            shutil.rmtree((current_dir + "\\" + var).replace("\\", "/"))
        src = (path_cloned_files + "\\" + var).replace("\\", "/")
        destination = (current_dir + "\\" + var).replace("\\", "/")
        shutil.copytree(src, destination)

    # =========================== Generation of 'Categories' folder ================================
    current_dir += "/Categories"
    if not os.path.isdir(current_dir):
        os.makedirs(current_dir)

    categories = list(NOTEBOOK_KEYS.keys())
    for category in categories:
        if not os.path.isdir(current_dir + "/" + category):
            os.makedirs(current_dir + "/" + category)

    return current_dir

def in_lamp_reach(p):
    '''
    Find the closest point on a line. This point will be reproducible by a Hue
    lamp.
    '''
    Used to find the closest point to an unreproducible Color is unreproducible
    on each line in the CIE 1931 'triangle'.
    '''
    Returns X, Y coordinates containing the closest avilable CIE 1931
    based on the hex_value provided.
    '''
    Stops the Host passed by parameter or all of them if none is
    specified, stopping at the same time all its actors.
    Should be called at the end of its usage, to finish correctly
    all the connections and threads.
        '''
        For remote communication. Sets the communication dispatcher of the host
        at the address and port specified.

        The scheme must be http if using a XMLRPC dispatcher.
        amqp for RabbitMQ communications.

        This methos is internal. Automatically called when creating the host.

        :param str. url: URL where to bind the host. Must be provided in
            the tipical form: 'scheme://address:port/hierarchical_path'
        '''
        Checks if the given id is used in the host by some actor.

        :param str. aid: identifier of the actor to check.
        :return: True if the id is used within the host.
        '''
        This method removes one actor from the Host, stoping it and deleting
        all its references.

        :param str. aid: identifier of the actor you want to stop.
        '''
        Gets a proxy reference to the actor indicated by the URL in the
        parameters. It can be a local reference or a remote direction to
        another host.

        This method can be called remotely synchronously.

        :param srt. url: address that identifies an actor.
        :param class klass: the class of the actor.
        :param srt. module: if the actor class is not in the calling module,
            you need to specify the module where it is here. Also, the *klass*
            parameter change to be a string.
        :return: :class:`~.Proxy` of the actor requested.
        :raises: :class:`NotFoundError`, if the URL specified do not
            correspond to any actor in the host.
        :raises: :class:`HostDownError`  if the host is down.
        :raises: :class:`HostError`  if there is an error looking for
            the actor in another server.
        '''
        Checks the parameters generating new proxy instances to avoid
        query concurrences from shared proxies and creating proxies for
        actors from another host.
        '''
        Checks the return parameters generating new proxy instances to
        avoid query concurrences from shared proxies and creating
        proxies for actors from another host.
        '''
        Register a new thread executing a parallel method.
        # Handle special cases.
        if not self._eta.started or self._eta.stalled or not self.rate:
            return '--.-KiB/s'

        unit_rate, unit = UnitByte(self._eta.rate_overall if self.done else self.rate).auto
        if unit_rate >= 100:
            formatter = '%d'
        elif unit_rate >= 10:
            formatter = '%.1f'
        else:
            formatter = '%.2f'
        return '{0}{1}/s'.format(locale.format(formatter, unit_rate, grouping=False), unit)

def str_rate(self):
    Initialize database with gsshapy tables
    Create session with database to work in
    Load project manager and in memory sqlite db sessionmaker for GSSHA project
    Load the settings from a named section.

    .. code-block:: python

        settings = plaster.get_settings(...)
        print(settings['foo'])

    :param config_uri: Anything that can be parsed by
        :func:`plaster.parse_uri`.

    :param section: The name of the section in the config file. If this is

        ``None`` then it is up to the loader to determine a sensible default
        usually derived from the fragment in the ``path#name`` syntax of the
        ``config_uri``.


    :param defaults: A ``dict`` of default values used to populate the

        settings and support variable interpolation. Any values in ``defaults``
        may be overridden by the loader prior to returning the final
        configuration dictionary.

    :returns: A ``dict`` of settings. This should return a dictionary object
        even if no data is available.

    Find all loaders that match the requested scheme and protocols.

    :param scheme: Any valid scheme. Examples would be something like ``ini``
        or ``ini+pastedeploy``.

    :param protocols: Zero or more :term:`loader protocol` identifiers that
        the loader must implement. If ``None`` then only generic loaders will
        be returned.

    :returns: A list containing zero or more :class:`plaster.ILoaderInfo`
        objects.

    Combines multiple dicts in one.

    :param dicts:
        A sequence of dicts.
    :type dicts: dict

    :param copy:
        If True, it returns a deepcopy of input values.
    :type copy: bool, optional

    :param base:
        Base dict where combine multiple dicts in one.
    :type base: dict, optional

    :return:
        A unique dict.
    :rtype: dict

    Example::

        >>> sorted(combine_dicts({'a': 3, 'c': 3}, {'a': 1, 'b': 2}).items())
        [('a', 1), ('b', 2), ('c', 3)]

    Merges and defines dictionaries with values identical to keys.

    :param kk:
        A sequence of keys and/or dictionaries.
    :type kk: object | dict, optional

    :param adict:
        A dictionary.
    :type adict: dict, optional

    :return:
        Merged dictionary.
    :rtype: dict

    Example::

        >>> sorted(kk_dict('a', 'b', 'c').items())
        [('a', 'a'), ('b', 'b'), ('c', 'c')]
        
        >>> sorted(kk_dict('a', 'b', **{'a-c': 'c'}).items())
        [('a', 'a'), ('a-c', 'c'), ('b', 'b')]
        
        >>> sorted(kk_dict('a', {'b': 'c'}, 'c').items())
        [('a', 'a'), ('b', 'c'), ('c', 'c')]
        
        >>> sorted(kk_dict('a', 'b', **{'b': 'c'}).items())
        Traceback (most recent call last):
         ...
        ValueError: keyword argument repeated
        >>> sorted(kk_dict('a', {'b': 'd'}, **{'b': 'c'}).items())
        Traceback (most recent call last):
         ...
        ValueError: keyword argument repeated
    Returns the same arguments.

    :param inputs:
        Inputs values.
    :type inputs: T

    :param copy:
        If True, it returns a deepcopy of input values.
    :type copy: bool, optional

    :return:
        Same input values.
    :rtype: (T, ...), T

    Example::

        >>> bypass('a', 'b', 'c')
        ('a', 'b', 'c')
        >>> bypass('a')
        'a'
    Returns a dict with new key values.

    :param key_map:
        A dictionary that maps the dict keys ({old key: new key}
    :type key_map: dict

    :param dicts:
        A sequence of dicts.
    :type dicts: dict

    :param copy:
        If True, it returns a deepcopy of input values.
    :type copy: bool, optional

    :param base:
        Base dict where combine multiple dicts in one.
    :type base: dict, optional

    :return:
        A unique dict with new key values.
    :rtype: dict

    Example::

        >>> d = map_dict({'a': 'c', 'b': 'd'}, {'a': 1, 'b': 1}, {'b': 2})
        >>> sorted(d.items())
        [('c', 1), ('d', 2)]
    Returns a new dict.

    :param key_map:
        A list that maps the dict keys ({old key: new key}
    :type key_map: list[str | dict | list]

    :param inputs:
        A sequence of data.
    :type inputs: iterable | dict | int | float | list | tuple

    :param copy:
        If True, it returns a deepcopy of input values.
    :type copy: bool, optional

    :param base:
        Base dict where combine multiple dicts in one.
    :type base: dict, optional

    :return:
        A unique dict with new values.
    :rtype: dict

    Example::

        >>> key_map = [
        ...     'a',
        ...     {'a': 'c'},
        ...     [
        ...         'a',
        ...         {'a': 'd'}
        ...     ]
        ... ]
        >>> inputs = (
        ...     2,
        ...     {'a': 3, 'b': 2},
        ...     [
        ...         1,
        ...         {'a': 4}
        ...     ]
        ... )
        >>> d = map_list(key_map, *inputs)
        >>> sorted(d.items())
        [('a', 1), ('b', 2), ('c', 3), ('d', 4)]
    Selects the chosen dictionary keys from the given dictionary.

    :param keys:
        Keys to select.
    :type keys: list, tuple, set

    :param dictionary:
        A dictionary.
    :type dictionary: dict

    :param copy:
        If True the output contains deep-copies of the values.
    :type copy: bool

    :param output_type:
        Type of function output:

            + 'list': a list with all values listed in `keys`.
            + 'dict': a dictionary with any outputs listed in `keys`.
            + 'values': if output length == 1 return a single value otherwise a
                        tuple with all values listed in `keys`.

        :type output_type: str, optional

    :param allow_miss:
        If True it does not raise when some key is missing in the dictionary.
    :type allow_miss: bool

    :return:
        A dictionary with chosen dictionary keys if present in the sequence of
        dictionaries. These are combined with :func:`combine_dicts`.
    :rtype: dict

    Example::

        >>> from functools import partial
        >>> fun = partial(selector, ['a', 'b'])
        >>> sorted(fun({'a': 1, 'b': 2, 'c': 3}).items())
        [('a', 1), ('b', 2)]
    Replicates `n` times the input value.

    :param n:
        Number of replications.
    :type n: int

    :param value:
        Value to be replicated.
    :type value: T

    :param copy:
        If True the list contains deep-copies of the value.
    :type copy: bool

    :return:
        A list with the value replicated `n` times.
    :rtype: list

    Example::

        >>> from functools import partial
        >>> fun = partial(replicate_value, n=5)
        >>> fun({'a': 3})
        ({'a': 3}, {'a': 3}, {'a': 3}, {'a': 3}, {'a': 3})
    Stacks the keys of nested-dictionaries into tuples and yields a list of
    k-v pairs.

    :param nested_dict:
        Nested dictionary.
    :type nested_dict: dict

    :param key:
        Initial keys.
    :type key: tuple, optional

    :param depth:
        Maximum keys depth.
    :type depth: int, optional

    :return:
        List of k-v pairs.
    :rtype: generator
    Nested keys are inside of nested-dictionaries.

    :param nested_dict:
        Nested dictionary.
    :type nested_dict: dict

    :param keys:
        Nested keys.
    :type keys: object

    :return:
        True if nested keys are inside of nested-dictionaries, otherwise False.
    :rtype: bool
    Merge nested-dictionaries.

    :param nested_dicts:
        Nested dictionaries.
    :type nested_dicts: dict

    :param depth:
        Maximum keys depth.
    :type depth: int, optional

    :param base:
        Base dict where combine multiple dicts in one.
    :type base: dict, optional

    :return:
        Combined nested-dictionary.
    :rtype: dict
    Decorator to add a function to a dispatcher.

    :param dsp:
        A dispatcher.
    :type dsp: schedula.Dispatcher

    :param inputs_kwargs:
        Do you want to include kwargs as inputs? 
    :type inputs_kwargs: bool
    

    :param inputs_defaults:

        Do you want to set default values? 

    :type inputs_defaults: bool
    
    :param kw:
        See :func:~`schedula.dispatcher.Dispatcher.add_function`.
    :type kw: dict

    :return:
        Decorator.
    :rtype: callable

    **------------------------------------------------------------------------**
    
    **Example**:
    
    .. dispatcher:: sol
       :opt: graph_attr={'ratio': '1'}
       :code:

        >>> import schedula as sh
        >>> dsp = sh.Dispatcher(name='Dispatcher')
        >>> @sh.add_function(dsp, outputs=['e'])
        ... @sh.add_function(dsp, False, True, outputs=['i'], inputs='ecah')
        ... @sh.add_function(dsp, True, outputs=['l'])

        ... def f(a, b, c, d=1):
        ...     return (a + b) - c + d
        >>> @sh.add_function(dsp, True, outputs=['d'])

        ... def g(e, i, *args, d=0):
        ...     return e + i + d
        >>> sol = dsp({'a': 1, 'b': 2, 'c': 3}); sol
        Solution([('a', 1), ('b', 2), ('c', 3), ('h', 1), ('e', 1), ('i', 4),
                  ('d', 5), ('l', 5)])
        Constructs a Blueprint out of the current object.

        :param memo:
            A dictionary to cache Blueprints.
        :type memo: dict[T,schedula.utils.blue.Blueprint]

        :return:
            A Blueprint of the current object.
        :rtype: schedula.utils.blue.Blueprint
        Given a dictionary of data and this widget's name, returns the value
        of this widget. Returns None if it's not provided.
    "ssh_cb" callback.
    session are ready, invoke the sftp_cb callback.


    def auth_cb(ssh):
        key = ssh_pki_import_privkey_file(key_filepath)
        ssh.userauth_publickey(key)

    return auth_cb

def add_edge_fun(graph):

    # Namespace shortcut for speed.
    succ, pred, node = graph._succ, graph._pred, graph._node


    def add_edge(u, v, **attr):
        if v not in succ:  # Add nodes.
            succ[v], pred[v], node[v] = {}, {}, {}

        succ[u][v] = pred[v][u] = attr  # Add the edge.

    return add_edge

def remove_edge_fun(graph):

    # Namespace shortcut for speed.
    rm_edge, rm_node = graph.remove_edge, graph.remove_node
    from networkx import is_isolate


    def remove_edge(u, v):
        rm_edge(u, v)  # Remove the edge.
        if is_isolate(graph, v):  # Check if v is isolate.
            rm_node(v)  # Remove the isolate out node.

    return remove_edge

def get_unused_node_id(graph, initial_guess='unknown', _format='{}<%d>'):

    has_node = graph.has_node  # Namespace shortcut for speed.

    n = counter()  # Counter.
    node_id_format = _format.format(initial_guess)  # Node id format.

    node_id = initial_guess  # Initial guess.
    while has_node(node_id):  # Check if node id is used.
        node_id = node_id_format % n()  # Guess.

    return node_id

def add_func_edges(dsp, fun_id, nodes_bunch, edge_weights=None, input=True,
                   data_nodes=None):

    # Namespace shortcut for speed.
    add_edge = _add_edge_dmap_fun(dsp.dmap, edge_weights)
    node, add_data = dsp.dmap.nodes, dsp.add_data
    remove_nodes = dsp.dmap.remove_nodes_from

    # Define an error message.
    msg = 'Invalid %sput id: {} is not a data node' % ['out', 'in'][input]
    i, j = ('i', 'o') if input else ('o', 'i')

    data_nodes = data_nodes or []  # Update data nodes.

    for u in nodes_bunch:  # Iterate nodes.
        try:
            if node[u]['type'] != 'data':  # The node is not a data node.
                data_nodes.append(fun_id)  # Add function id to be removed.

                remove_nodes(data_nodes)  # Remove function and new data nodes.

                raise ValueError(msg.format(u))  # Raise error.
        except KeyError:
            data_nodes.append(add_data(data_id=u))  # Add new data node.

        add_edge(**{i: u, j: fun_id, 'w': u})  # Add edge.

    return data_nodes

def _add_edge_dmap_fun(graph, edges_weights=None):

    add = graph.add_edge  # Namespace shortcut for speed.

    if edges_weights is not None:

        def add_edge(i, o, w):
            if w in edges_weights:
                add(i, o, weight=edges_weights[w])  # Weighted edge.
            else:
                add(i, o)  # Normal edge.
    else:
        # noinspection PyUnusedLocal

        def add_edge(i, o, w):
            add(i, o)  # Normal edge.

    return add_edge

def _get_node(nodes, node_id, fuzzy=True):

    try:
        return node_id, nodes[node_id]  # Return dispatcher node and its id.
    except KeyError as ex:
        if fuzzy:
            it = sorted(nodes.items())
            n = next(((k, v) for k, v in it if node_id in k), EMPTY)
            if n is not EMPTY:
                return n
        raise ex

def get_full_pipe(sol, base=()):

    pipe, i = DspPipe(), len(base)

    for p in sol._pipe:
        n, s = p[-1]
        d = s.dsp
        p = {'task': p}

        if n in s._errors:
            p['error'] = s._errors[n]

        node_id = s.full_name + (n,)

        assert base == node_id[:i], '%s != %s' % (node_id[:i], base)

        n_id = node_id[i:]

        n, path = d.get_node(n, node_attr=None)
        if n['type'] == 'function' and 'function' in n:
            try:
                sub_sol = s.workflow.node[path[-1]]['solution']
                sp = get_full_pipe(sub_sol, base=node_id)
                if sp:
                    p['sub_pipe'] = sp
            except KeyError:
                pass

        pipe[bypass(*n_id)] = p

    return pipe

def connectChunk(key, chunk):
    schunk = chunk[0].strip().split()

    result = {'slinkNumber': schunk[1],
              'upSjunc': schunk[2],
              'downSjunc': schunk[3]}

    return result

def get_items(self, page=1, order_by=None, filters=None):
        start = (page-1)*self.per_page
        query = self.get_query()
        if order_by is not None:
            query = query.order_by(self._get_field(order_by))
        if filters is not None:
            query = self._filter(query, filters)
        return query.offset(start).limit(self.per_page), self.count(query)

def _read(self, directory, filename, session, path, name, extension, spatial, spatialReferenceID, replaceParamFile):
        # Persist name and extension of file
        self.name = name
        self.fileExtension = extension

        # Open file and parse into a data structure
        with open(path, 'r') as f:
            self.text = f.read()

def isdisjoint(self, other):
        if isinstance(other, _sequence_types + (BaseMultiset, )):
            pass
        elif not isinstance(other, Container):
            other = self._as_multiset(other)
        return all(element not in other for element in self._elements.keys())

def difference(self, *others):
        result = self.__copy__()
        _elements = result._elements
        _total = result._total
        for other in map(self._as_multiset, others):
            for element, multiplicity in other.items():
                if element in _elements:
                    old_multiplicity = _elements[element]
                    new_multiplicity = old_multiplicity - multiplicity
                    if new_multiplicity > 0:
                        _elements[element] = new_multiplicity
                        _total -= multiplicity
                    else:
                        del _elements[element]
                        _total -= old_multiplicity
        result._total = _total
        return result

def union(self, *others):
        result = self.__copy__()
        _elements = result._elements
        _total = result._total
        for other in map(self._as_mapping, others):
            for element, multiplicity in other.items():
                old_multiplicity = _elements.get(element, 0)
                if multiplicity > old_multiplicity:
                    _elements[element] = multiplicity
                    _total += multiplicity - old_multiplicity
        result._total = _total
        return result

def intersection(self, *others):
        result = self.__copy__()
        _elements = result._elements
        _total = result._total
        for other in map(self._as_mapping, others):
            for element, multiplicity in list(_elements.items()):
                new_multiplicity = other.get(element, 0)
                if new_multiplicity < multiplicity:
                    if new_multiplicity > 0:
                        _elements[element] = new_multiplicity
                        _total -= multiplicity - new_multiplicity
                    else:
                        del _elements[element]
                        _total -= multiplicity
        result._total = _total
        return result

def symmetric_difference(self, other):
        other = self._as_multiset(other)
        result = self.__class__()
        _total = 0
        _elements = result._elements
        self_elements = self._elements
        other_elements = other._elements
        dist_elements = set(self_elements.keys()) | set(other_elements.keys())
        for element in dist_elements:
            multiplicity = self_elements.get(element, 0)
            other_multiplicity = other_elements.get(element, 0)
            new_multiplicity = (multiplicity - other_multiplicity
                                if multiplicity > other_multiplicity else other_multiplicity - multiplicity)
            _total += new_multiplicity
            if new_multiplicity > 0:
                _elements[element] = new_multiplicity
        result._total = _total
        return result

def times(self, factor):
        if factor == 0:
            return self.__class__()
        if factor < 0:
            raise ValueError('The factor must no be negative.')
        result = self.__copy__()
        _elements = result._elements
        for element in _elements:
            _elements[element] *= factor
        result._total *= factor
        return result

def union_update(self, *others):
        _elements = self._elements
        _total = self._total
        for other in map(self._as_mapping, others):
            for element, multiplicity in other.items():
                old_multiplicity = _elements.get(element, 0)
                if multiplicity > old_multiplicity:
                    _elements[element] = multiplicity
                    _total += multiplicity - old_multiplicity
        self._total = _total

def intersection_update(self, *others):
        for other in map(self._as_mapping, others):
            for element, current_count in list(self.items()):
                multiplicity = other.get(element, 0)
                if multiplicity < current_count:
                    self[element] = multiplicity

def difference_update(self, *others):
        for other in map(self._as_multiset, others):
            for element, multiplicity in other.items():
                self.discard(element, multiplicity)

def symmetric_difference_update(self, other):
        other = self._as_multiset(other)
        elements = set(self.distinct_elements()) | set(other.distinct_elements())
        for element in elements:
            multiplicity = self[element]
            other_count = other[element]
            self[element] = (multiplicity - other_count if multiplicity > other_count else other_count - multiplicity)

def times_update(self, factor):
        if factor < 0:
            raise ValueError("The factor must not be negative.")
        elif factor == 0:
            self.clear()
        else:
            _elements = self._elements
            for element in _elements:
                _elements[element] *= factor
            self._total *= factor

def add(self, element, multiplicity=1):
        if multiplicity < 1:
            raise ValueError("Multiplicity must be positive")
        self._elements[element] += multiplicity
        self._total += multiplicity

def remove(self, element, multiplicity=None):
        _elements = self._elements
        if element not in _elements:
            raise KeyError
        old_multiplicity = _elements.get(element, 0)
        if multiplicity is None or multiplicity >= old_multiplicity:
            del _elements[element]
            self._total -= old_multiplicity
        elif multiplicity < 0:
            raise ValueError("Multiplicity must be not be negative")
        elif multiplicity > 0:
            _elements[element] -= multiplicity
            self._total -= multiplicity
        return old_multiplicity

def discard(self, element, multiplicity=None):
        _elements = self._elements
        if element in _elements:
            old_multiplicity = _elements[element]
            if multiplicity is None or multiplicity >= old_multiplicity:
                del _elements[element]
                self._total -= old_multiplicity
            elif multiplicity < 0:
                raise ValueError("Multiplicity must not be negative")
            elif multiplicity > 0:
                _elements[element] -= multiplicity
                self._total -= multiplicity
            return old_multiplicity
        else:
            return 0

def shutdown_executors(wait=True):
    return {k: shutdown_executor(k, wait) for k in list(_EXECUTORS.keys())}

def async_thread(sol, args, node_attr, node_id, *a, **kw):
    executor = _get_executor(_executor_name(kw.get('executor', False), sol.dsp))
    if not executor:
        return sol._evaluate_node(args, node_attr, node_id, *a, **kw)

    futures = args
    if node_attr['type'] == 'data' and (
            node_attr['wait_inputs'] or 'function' in node_attr):
        futures = args[0].values()
    from concurrent.futures import Future
    futures = {v for v in futures if isinstance(v, Future)}


    def _submit():
        return executor.thread(
            _async_eval, sol, args, node_attr, node_id, *a, **kw
        )

    if futures:  # Chain results.
        result = Future()


        def _set_res(fut):
            try:
                result.set_result(fut.result())
            except BaseException as ex:
                result.set_exception(ex)


        def _submit_task(fut=None):
            futures.discard(fut)
            not futures and _submit().add_done_callback(_set_res)

        for f in list(futures):
            f.add_done_callback(_submit_task)
    else:
        result = _submit()

    timeout = node_attr.get('await_result', False)
    if timeout is not False:
        return _await_result(result, timeout, sol, node_id)

    n = len(node_attr.get('outputs', []))
    return AsyncList(future=result, n=n) if n > 1 else result

def await_result(obj, timeout=None):
    from concurrent.futures import Future
    return obj.result(timeout) if isinstance(obj, Future) else obj

